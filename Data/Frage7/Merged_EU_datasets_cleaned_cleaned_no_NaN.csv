ORCID,Author,Institution,Title,PubDate,Abstract,DOI,IDlist,flag,found_words
0000-0001-6252-7658,Matthew Baugh,"Imperial College London, Imperial College London Department of Computing",Trade-offs in Fine-tuned Diffusion Models Between Accuracy and   Interpretability,1970,"  Recent advancements in diffusion models have significantly impacted the trajectory of generative machine learning research, with many adopting the strategy of fine-tuning pre-trained models using domain-specific text-to-image datasets. Notably, this method has been readily employed for medical applications, such as X-ray image synthesis, leveraging the plethora of associated radiology reports. Yet, a prevailing concern is the lack of assurance on whether these models genuinely comprehend their generated content. With the evolution of text-conditional image generation, these models have grown potent enough to facilitate object localization scrutiny. Our research underscores this advancement in the critical realm of medical imaging, emphasizing the crucial role of interpretability. We further unravel a consequential trade-off between image fidelity as gauged by conventional metrics and model interpretability in generative diffusion models. Specifically, the adoption of learnable text encoders when fine-tuning results in diminished interpretability. Our in-depth exploration uncovers the underlying factors responsible for this divergence. Consequently, we present a set of design principles for the development of truly interpretable generative models. Code is available at https://github.com/MischaD/chest-distillation. ",Kein DOI-Link verfügbar,2303.17908v2,Yes,potent(1)
0000-0001-6252-7658,Matthew Baugh,"Imperial College London, Imperial College London Department of Computing",DISYRE: Diffusion-Inspired SYnthetic REstoration for Unsupervised   Anomaly Detection,1970,"  Unsupervised Anomaly Detection (UAD) techniques aim to identify and localize anomalies without relying on annotations, only leveraging a model trained on a dataset known to be free of anomalies. Diffusion models learn to modify inputs $x$ to increase the probability of it belonging to a desired distribution, i.e., they model the score function $\nabla_x \log p(x)$. Such a score function is potentially relevant for UAD, since $\nabla_x \log p(x)$ is itself a pixel-wise anomaly score. However, diffusion models are trained to invert a corruption process based on Gaussian noise and the learned score function is unlikely to generalize to medical anomalies. This work addresses the problem of how to learn a score function relevant for UAD and proposes DISYRE: Diffusion-Inspired SYnthetic REstoration. We retain the diffusion-like pipeline but replace the Gaussian noise corruption with a gradual, synthetic anomaly corruption so the learned score function generalizes to medical, naturally occurring anomalies. We evaluate DISYRE on three common Brain MRI UAD benchmarks and substantially outperform other methods in two out of the three tasks. ",Kein DOI-Link verfügbar,2311.15453v2,Yes,potent(1)
0000-0002-0201-4874,Wenjun Guo,Imperial College London,Different Regular Black Holes: Geodesic Structures of Test Particles,1970,"  This paper investigates the metric of previously proposed regular black holes, calculates their effective potentials, and plots the curves of the effective potentials. By determining the conserved quantities, the dynamical equations for particles and photons near the black hole are derived. The analysis encompasses timelike and null geodesics in different spacetimes, including bound geodesics, unstable circular geodesics, stable circular geodesics, and escape geodesics. The findings are presented through figures and tables. Furthermore, the bound geodesics of the four regular black hole spacetimes are analyzed, examining the average distance of particle orbits from the center of the event horizon, the precession behavior of the perihelion, and the probability of particles appearing inside the outer event horizon during motion. Based on these analyses, a general formula is proposed, which yields the existing metrics when specific parameter values are chosen. The impact of parameter variations on the effective potential and geodesics is then computed using this new formula. ",Kein DOI-Link verfügbar,2309.12932v1,Yes,potent(3)
0000-0002-0201-4874,Wenjun Guo,Imperial College London,Scalar field quasinormal modes of noncommutative high dimensional   Schwarzschild-Tangherlini black hole spacetime with smeared matter sources,1970,"  We investigate the massless scalar quasinormal modes (QNMs) of the noncommutative $D$-dimensional Schwarzschild-Tangherlini black hole spacetime in this paper. By using the Wentzel-Kramers-Brillouin (WKB) approximation method, the asymptotic iterative method (AIM) and the inverted potential method (IPM) method, we made a detail analysis of the massless scalar QNM frequencies by varying the general smeared matter distribution and the allowable characteristic parameters ($k$ and $\theta$) corresponding to different dimensions. It is found that the nonconvergence of the high order WKB approximation exists in the QNMs frequencies of scalar perturbation around the noncommutative $D$-dimensional Schwarzschild black holes. We conclude that the 3rd WKB result should be more reliable than those of the high order WKB method since our numerical results are also verified by the AIM method and the IPM method. In the dimensional range of $4\leq D \leq7$, the scalar QNMs as a function of the different papameters (the noncommutative parameter $\theta$, the smeared matter distribution parameter $k$, the multipole number $l$ and the main node number $n$) are obtained. Moreover, we study the dynamical evolution of a scalar field in the background of the noncommutative high dimensional Schwarzschild-Tangherlini black hole. ",https://doi.org/10.1016/j.nuclphysb.2020.115217,2012.00320v1,Yes,potent(1)
0000-0002-0201-4874,Wenjun Guo,Imperial College London,Quasinormal modes of scalar field coupled to Einstein's tensor in the   non-commutative geometry inspired black hole,1970,"  We investigate the quasinormal modes (QNMs) of the scalar field coupled to the Einstein's tensor in the non-commutative geometry inspired black hole spacetime. It is found that the lapse function of the non-commutative black hole metric can be represented by a Kummer's confluent hypergeometric function, which can effectively solve the problem that the numerical results of the QNMs are sensitive to the model parameters and make the QNMs values more reliable. We make a careful analysis of the scalar QNM frequencies by using several numerical methods, and find that the numerical results obtained by the new WKB method (the Pad\'e approximants) and the Mashhoon method (P$\ddot{\text{o}}$schl-Teller potential method) are quite different from those obtained by the asymptotic iterative method (AIM) and time-domain integration method when the non-commutative parameter $\theta$ and coupling parameter $\eta$ are large. The most obvious difference is that the numerical results obtained by the AIM and the time-domain integration method appear a critical value $\eta_c$ with an increase of $\eta$, which leads to the dynamical instability. After carefully analyzing the numeral results, we conclude that the numerical results obtained by the AIM and the time-domain integration method are closer to the theoretical values than those obtained by the WKB method and the Mashhoon method, when the $\theta$ and $\eta$ are large. Moreover, through a numerical fitting, we obtain that the functional relationship between the threshold $\eta_c$ and the non-commutative parameter $\theta$ satisfies $\eta_{c}=a\theta^{b}+c$ for a fixed $l$ approximately. We find that the stability of dynamics can be ensured in the $\eta<\eta_c(\theta, l)$ region. ",https://doi.org/10.1016/j.nuclphysb.2021.115595,2012.03004v2,Yes,potent(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",CromSS: Cross-modal pre-training with noisy labels for remote sensing   image segmentation,1970,"  We study the potential of noisy labels y to pretrain semantic segmentation models in a multi-modal learning framework for geospatial applications. Specifically, we propose a novel Cross-modal Sample Selection method (CromSS) that utilizes the class distributions P^{(d)}(x,c) over pixels x and classes c modelled by multiple sensors/modalities d of a given geospatial scene. Consistency of predictions across sensors $d$ is jointly informed by the entropy of P^{(d)}(x,c). Noisy label sampling we determine by the confidence of each sensor d in the noisy class label, P^{(d)}(x,c=y(x)). To verify the performance of our approach, we conduct experiments with Sentinel-1 (radar) and Sentinel-2 (optical) satellite imagery from the globally-sampled SSL4EO-S12 dataset. We pair those scenes with 9-class noisy labels sourced from the Google Dynamic World project for pretraining. Transfer learning evaluations (downstream task) on the DFC2020 dataset confirm the effectiveness of the proposed method for remote sensing image segmentation. ",Kein DOI-Link verfügbar,2405.01217v1,Yes,potent(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",AutoLCZ: Towards Automatized Local Climate Zone Mapping from Rule-Based   Remote Sensing,1970,"  Local climate zones (LCZs) established a standard classification system to categorize the landscape universe for improved urban climate studies. Existing LCZ mapping is guided by human interaction with geographic information systems (GIS) or modelled from remote sensing (RS) data. GIS-based methods do not scale to large areas. However, RS-based methods leverage machine learning techniques to automatize LCZ classification from RS. Yet, RS-based methods require huge amounts of manual labels for training.   We propose a novel LCZ mapping framework, termed AutoLCZ, to extract the LCZ classification features from high-resolution RS modalities. We study the definition of numerical rules designed to mimic the LCZ definitions. Those rules model geometric and surface cover properties from LiDAR data. Correspondingly, we enable LCZ classification from RS data in a GIS-based scheme. The proposed AutoLCZ method has potential to reduce the human labor to acquire accurate metadata. At the same time, AutoLCZ sheds light on the physical interpretability of RS-based methods. In a proof-of-concept for New York City (NYC) we leverage airborne LiDAR surveys to model 4 LCZ features to distinguish 10 LCZ types. The results indicate the potential of AutoLCZ as promising avenue for large-scale LCZ mapping from RS data. ",Kein DOI-Link verfügbar,2405.13993v1,Yes,potent(2)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",Naive Gabor Networks for Hyperspectral Image Classification,1970,"  Recently, many convolutional neural network (CNN) methods have been designed for hyperspectral image (HSI) classification since CNNs are able to produce good representations of data, which greatly benefits from a huge number of parameters. However, solving such a high-dimensional optimization problem often requires a large amount of training samples in order to avoid overfitting. Additionally, it is a typical non-convex problem affected by many local minima and flat regions. To address these problems, in this paper, we introduce naive Gabor Networks or Gabor-Nets which, for the first time in the literature, design and learn CNN kernels strictly in the form of Gabor filters, aiming to reduce the number of involved parameters and constrain the solution space, and hence improve the performances of CNNs. Specifically, we develop an innovative phase-induced Gabor kernel, which is trickily designed to perform the Gabor feature learning via a linear combination of local low-frequency and high-frequency components of data controlled by the kernel phase. With the phase-induced Gabor kernel, the proposed Gabor-Nets gains the ability to automatically adapt to the local harmonic characteristics of the HSI data and thus yields more representative harmonic features. Also, this kernel can fulfill the traditional complex-valued Gabor filtering in a real-valued manner, hence making Gabor-Nets easily perform in a usual CNN thread. We evaluated our newly developed Gabor-Nets on three well-known HSIs, suggesting that our proposed Gabor-Nets can significantly improve the performance of CNNs, particularly with a small training set. ",Kein DOI-Link verfügbar,1912.03991v2,Yes,innovative(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford","SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for   Self-Supervised Learning in Earth Observation",1970,"  Self-supervised pre-training bears potential to generate expressive representations without human annotation. Most pre-training in Earth observation (EO) are based on ImageNet or medium-size, labeled remote sensing (RS) datasets. We share an unlabeled RS dataset SSL4EO-S12 (Self-Supervised Learning for Earth Observation - Sentinel-1/2) to assemble a large-scale, global, multimodal, and multi-seasonal corpus of satellite imagery from the ESA Sentinel-1 \& -2 satellite missions. For EO applications we demonstrate SSL4EO-S12 to succeed in self-supervised pre-training for a set of methods: MoCo-v2, DINO, MAE, and data2vec. Resulting models yield downstream performance close to, or surpassing accuracy measures of supervised learning. In addition, pre-training on SSL4EO-S12 excels compared to existing datasets. We make openly available the dataset, related source code, and pre-trained models at https://github.com/zhu-xlab/SSL4EO-S12. ",Kein DOI-Link verfügbar,2211.07044v2,Yes,potent(1)
0000-0002-6946-727X,Chenying Liu,"New College, University of Oxford, University of Oxford",Deep Semantic Model Fusion for Ancient Agricultural Terrace Detection,1970,"  Discovering ancient agricultural terraces in desert regions is important for the monitoring of long-term climate changes on the Earth's surface. However, traditional ground surveys are both costly and limited in scale. With the increasing accessibility of aerial and satellite data, machine learning techniques bear large potential for the automatic detection and recognition of archaeological landscapes. In this paper, we propose a deep semantic model fusion method for ancient agricultural terrace detection. The input data includes aerial images and LiDAR generated terrain features in the Negev desert. Two deep semantic segmentation models, namely DeepLabv3+ and UNet, with EfficientNet backbone, are trained and fused to provide segmentation maps of ancient terraces and walls. The proposed method won the first prize in the International AI Archaeology Challenge. Codes are available at https://github.com/wangyi111/international-archaeology-ai-challenge. ",Kein DOI-Link verfügbar,2308.02225v1,Yes,potent(1)
0000-0002-3083-6613,Adam Baker,"University of Oxford, University of Oxford Worcester College",A comparative study of artificial intelligence and human doctors for the   purpose of triage and diagnosis,1970,"  Online symptom checkers have significant potential to improve patient care, however their reliability and accuracy remain variable. We hypothesised that an artificial intelligence (AI) powered triage and diagnostic system would compare favourably with human doctors with respect to triage and diagnostic accuracy. We performed a prospective validation study of the accuracy and safety of an AI powered triage and diagnostic system. Identical cases were evaluated by both an AI system and human doctors. Differential diagnoses and triage outcomes were evaluated by an independent judge, who was blinded from knowing the source (AI system or human doctor) of the outcomes. Independently of these cases, vignettes from publicly available resources were also assessed to provide a benchmark to previous studies and the diagnostic component of the MRCGP exam. Overall we found that the Babylon AI powered Triage and Diagnostic System was able to identify the condition modelled by a clinical vignette with accuracy comparable to human doctors (in terms of precision and recall). In addition, we found that the triage advice recommended by the AI System was, on average, safer than that of human doctors, when compared to the ranges of acceptable triage provided by independent expert judges, with only a minimal reduction in appropriateness. ",Kein DOI-Link verfügbar,1806.10698v1,Yes,potent(1)
0000-0003-2167-1623,Zhuocong Xiao,University of Cambridge,Ferrotronics for the creation of band gaps in Graphene,1970,"  We experimentally demonstrate a simple graphene/ ferrolectric device, termed Ferrotronic (electronic effect from ferroelectric) device in which the band-structure of single-layer graphene is modified. The device architecture consists of graphene deposited on a ferroelectric substrate which encodes a periodic surface potential achieved through domain engineering. This structure takes advantage of the nature of conduction through graphene to modulate the Fermi velocity of the charge carriers by the variations in surface potential, leading to the emergence of energy mini-bands and a band gap at the superlattice Brillouin zone boundary. Our work represents a simple route to building circuits whose functionality is controlled by the underlying substrate. ",Kein DOI-Link verfügbar,2112.07444v1,Yes,potent(2)
0000-0002-5552-4536,Eiko Yoneki,University of Cambridge,IA2: Leveraging Instance-Aware Index Advisor with Reinforcement Learning   for Diverse Workloads,1970,"  This study introduces the Instance-Aware Index Advisor (IA2), a novel deep reinforcement learning (DRL)-based approach for optimizing index selection in databases facing large action spaces of potential candidates. IA2 introduces the Twin Delayed Deep Deterministic Policy Gradient - Temporal Difference State-Wise Action Refinery (TD3-TD-SWAR) model, enabling efficient index selection by understanding workload-index dependencies and employing adaptive action masking. This method includes a comprehensive workload model, enhancing its ability to adapt to unseen workloads and ensuring robust performance across diverse database environments. Evaluation on benchmarks such as TPC-H reveals IA2's suggested indexes' performance in enhancing runtime, securing a 40% reduction in runtime for complex TPC-H workloads compared to scenarios without indexes, and delivering a 20% improvement over existing state-of-the-art DRL-based index advisors. ",https://doi.org/10.1145/3642970.3655839,2404.05777v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",PMFL: Partial Meta-Federated Learning for heterogeneous tasks and its   applications on real-world medical records,1970,"  Federated machine learning is a versatile and flexible tool to utilize distributed data from different sources, especially when communication technology develops rapidly and an unprecedented amount of data could be collected on mobile devices nowadays. Federated learning method exploits not only the data but the computational power of all devices in the network to achieve more efficient model training. Nevertheless, while most traditional federated learning methods work well for homogeneous data and tasks, adapting the method to a different heterogeneous data and task distribution is challenging. This limitation has constrained the applications of federated learning in real-world contexts, especially in healthcare settings. Inspired by the fundamental idea of meta-learning, in this study we propose a new algorithm, which is an integration of federated learning and meta-learning, to tackle this issue. In addition, owing to the advantage of transfer learning for model generalization, we further improve our algorithm by introducing partial parameter sharing. We name this method partial meta-federated learning (PMFL). Finally, we apply the algorithms to two medical datasets. We show that our algorithm could obtain the fastest training speed and achieve the best performance when dealing with heterogeneous medical datasets. ",Kein DOI-Link verfügbar,2112.05321v2,Yes,versatile(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Pancreatic Cancer ROSE Image Classification Based on Multiple Instance   Learning with Shuffle Instances,1970,"  The rapid on-site evaluation (ROSE) technique can significantly ac-celerate the diagnostic workflow of pancreatic cancer by immediately analyzing the fast-stained cytopathological images with on-site pathologists. Computer-aided diagnosis (CAD) using the deep learning method has the potential to solve the problem of insufficient pathology staffing. However, the cancerous patterns of ROSE images vary greatly between different samples, making the CAD task extremely challenging. Besides, due to different staining qualities and various types of acquisition devices, the ROSE images also have compli-cated perturbations in terms of color distribution, brightness, and contrast. To address these challenges, we proposed a novel multiple instance learning (MIL) approach using shuffle patches containing the instances, which adopts the patch-based learning strategy of Vision Transformers. With the re-grouped bags of shuffle instances and their bag-level soft labels, the approach utilizes a MIL head to make the model focus on the features from the pancreatic cancer cells, rather than that from various perturbations in ROSE images. Simultaneously, combined with a classification head, the model can effectively identify the gen-eral distributive patterns across different instances. The results demonstrate the significant improvements in the classification accuracy with more accurate at-tention regions, indicating that the diverse patterns of ROSE images are effec-tively extracted, and the complicated perturbations of ROSE images are signifi-cantly eliminated. It also suggests that the MIL with shuffle instances has great potential in the analysis of cytopathological images. ",Kein DOI-Link verfügbar,2206.03080v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Uncertainty-Based Extensible Codebook for Discrete Federated Learning in   Heterogeneous Data Silos,1970,"  Federated learning (FL), aimed at leveraging vast distributed datasets, confronts a crucial challenge: the heterogeneity of data across different silos. While previous studies have explored discrete representations to enhance model generalization across minor distributional shifts, these approaches often struggle to adapt to new data silos with significantly divergent distributions. In response, we have identified that models derived from FL exhibit markedly increased uncertainty when applied to data silos with unfamiliar distributions. Consequently, we propose an innovative yet straightforward iterative framework, termed Uncertainty-Based Extensible-Codebook Federated Learning (UEFL). This framework dynamically maps latent features to trainable discrete vectors, assesses the uncertainty, and specifically extends the discretization dictionary or codebook for silos exhibiting high uncertainty. Our approach aims to simultaneously enhance accuracy and reduce uncertainty by explicitly addressing the diversity of data distributions, all while maintaining minimal computational overhead in environments characterized by heterogeneous data silos. Through experiments conducted on five datasets, our method has demonstrated its superiority, achieving significant improvements in accuracy (by 3%--22.1%) and uncertainty reduction (by 38.83%--96.24%), thereby outperforming contemporary state-of-the-art methods. The source code is available at https://github.com/destiny301/uefl. ",Kein DOI-Link verfügbar,2402.18888v2,Yes,innovative(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Leveraging LLMs to Predict Affective States via Smartphone Sensor   Features,1970,"  As mental health issues for young adults present a pressing public health concern, daily digital mood monitoring for early detection has become an important prospect. An active research area, digital phenotyping, involves collecting and analysing data from personal digital devices such as smartphones (usage and sensors) and wearables to infer behaviours and mental health. Whilst this data is standardly analysed using statistical and machine learning approaches, the emergence of large language models (LLMs) offers a new approach to make sense of smartphone sensing data. Despite their effectiveness across various domains, LLMs remain relatively unexplored in digital mental health, particularly in integrating mobile sensor data. Our study aims to bridge this gap by employing LLMs to predict affect outcomes based on smartphone sensing data from university students. We demonstrate the efficacy of zero-shot and few-shot embedding LLMs in inferring general wellbeing. Our findings reveal that LLMs can make promising predictions of affect measures using solely smartphone sensing data. This research sheds light on the potential of LLMs for affective state prediction, emphasizing the intricate link between smartphone behavioral patterns and affective states. To our knowledge, this is the first work to leverage LLMs for affective state prediction and digital phenotyping tasks. ",Kein DOI-Link verfügbar,2407.08240v1,Yes,"intricate(1), potent(1)"
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Channel Estimation and Projection for RIS-assisted MIMO Using Zadoff-Chu   Sequences,1970,"  The reconfigurable intelligent surface (RIS) technology is a promising enabler for millimeter wave (mmWave) wireless communications, as it can potentially provide spectral efficiency comparable to the conventional massive multiple-input multiple-output (MIMO) but with significantly lower hardware complexity. In this paper, we focus on the estimation and projection of the uplink RIS-aided massive MIMO channel, which can be time-varying. We propose to let the user equipments (UE) transmit Zadoff-Chu (ZC) sequences and let the base station (BS) conduct maximum likelihood (ML) estimation of the uplink channel. The proposed scheme is computationally efficient: it uses ZC sequences to decouple the estimation of the frequency and time offsets; it uses the space-alternating generalized expectation-maximization (SAGE) method to reduce the high-dimensional problem due to the multipaths to multiple lower-dimensional ones per path. Owing to the estimation of the Doppler frequency offsets, the time-varying channel state can be projected, which can significantly lower the overhead of the pilots for channel estimation. The numerical simulations verify the effectiveness of the proposed scheme. ",Kein DOI-Link verfügbar,2202.10038v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Predicting Affective States from Screen Text Sentiment,1970,"  The proliferation of mobile sensing technologies has enabled the study of various physiological and behavioural phenomena through unobtrusive data collection from smartphone sensors. This approach offers real-time insights into individuals' physical and mental states, creating opportunities for personalised treatment and interventions. However, the potential of analysing the textual content viewed on smartphones to predict affective states remains underexplored. To better understand how the screen text that users are exposed to and interact with can influence their affects, we investigated a subset of data obtained from a digital phenotyping study of Australian university students conducted in 2023. We employed linear regression, zero-shot, and multi-shot prompting using a large language model (LLM) to analyse relationships between screen text and affective states. Our findings indicate that multi-shot prompting substantially outperforms both linear regression and zero-shot prompting, highlighting the importance of context in affect prediction. We discuss the value of incorporating textual and sentiment data for improving affect prediction, providing a basis for future advancements in understanding smartphone use and wellbeing. ",https://doi.org/10.1145/3675094.3678489,2408.12844v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",In-Plane Magnon Valve Effect in Magnetic Insulator/Heavy Metal/ Magnetic   Insulator Device,1970,"  We propose an in-plane magnon valve (MV), a sandwich structure composed of ferromagnetic insulator/heavy metal/ferromagnetic insulator (MI/HM/MI). When the magnetizations of the two MI layers are parallel, the longitudinal conductance in the HM layer is greater than that in the antiparallel state according to the magnetic proximity effect, termed as the in-plane magnon valve effect. We investigate the dependence of MV ratio (MVR), which is the relative change in longitudinal conductance between the parallel and antiparallel MV states, on the difference in electronic structure between magnetized and non-magnetized metal atoms, revealing that MVR can reach 100%. Additionally, the dependence of MVR on the thickness of metal layer is analyzed, revealing an exponential decrease with increasing thickness. Then we investigate the dependence of HM layer conductance on the relative angle between the magnetizations of two MI layers, illustrating the potential of MV as a magneto-sensitive magnonic sensor. We also investigate the effect of Joule heating on the measurement signal based on the spin Seebeck effect. Two designed configurations are proposed according to whether the electron current is parallel or perpendicular to the magnetization of the MI layer. In the parallel configuration, the transverse voltage differs between the parallel and antiparallel MV states. While in the perpendicular configuration, the longitudinal resistance differs. Quantitative numerical results indicate the feasibility of detecting a voltage signal using the first configuration in experiments. Our work contributes valuable insights for the design, development and integration of magnon devices ",Kein DOI-Link verfügbar,2312.17413v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",IDentity with Locality: An ideal hash for gene sequence search,1970,"  Gene sequence search is a fundamental operation in computational genomics. Due to the petabyte scale of genome archives, most gene search systems now use hashing-based data structures such as Bloom Filters (BF). The state-of-the-art systems such as Compact bit-slicing signature index (COBS) and Repeated And Merged Bloom filters (RAMBO) use BF with Random Hash (RH) functions for gene representation and identification. The standard recipe is to cast the gene search problem as a sequence of membership problems testing if each subsequent gene substring (called kmer) of Q is present in the set of kmers of the entire gene database D. We observe that RH functions, which are crucial to the memory and the computational advantage of BF, are also detrimental to the system performance of gene-search systems. While subsequent kmers being queried are likely very similar, RH, oblivious to any similarity, uniformly distributes the kmers to different parts of potentially large BF, thus triggering excessive cache misses and causing system slowdown. We propose a novel hash function called the Identity with Locality (IDL) hash family, which co-locates the keys close in input space without causing collisions. This approach ensures both cache locality and key preservation. IDL functions can be a drop-in replacement for RH functions and help improve the performance of information retrieval systems. We give a simple but practical construction of IDL function families and show that replacing the RH with IDL functions reduces cache misses by a factor of 5x, thus improving query and indexing times of SOTA methods such as COBS and RAMBO by factors up to 2x without compromising their quality. We also provide a theoretical analysis of the false positive rate of BF with IDL functions. Our hash function is the first study that bridges Locality Sensitive Hash (LSH) and RH to obtain cache efficiency. ",Kein DOI-Link verfügbar,2406.14901v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Fixed-price Diffusion Mechanism Design,1970,"  We consider a fixed-price mechanism design setting where a seller sells one item via a social network, but the seller can only directly communicate with her neighbours initially. Each other node in the network is a potential buyer with a valuation derived from a common distribution. With a standard fixed-price mechanism, the seller can only sell the item among her neighbours. To improve her revenue, she needs more buyers to join in the sale. To achieve this, we propose the very first fixed-price mechanism to incentivize the seller's neighbours to inform their neighbours about the sale and to eventually inform all buyers in the network to improve seller's revenue. Compared with the existing mechanisms for the same purpose, our mechanism does not require the buyers to reveal their valuations and it is computationally easy. More importantly, it guarantees that the improved revenue is at least 1/2 of the optimal. ",Kein DOI-Link verfügbar,1905.05450v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Cost Sharing for Connectivity with Budget,1970,"  We consider a cost sharing problem to connect all nodes in a weighted undirected graph, where the weight of each edge represents the cost to use the edge for the connectivity and the cost has to be shared among all connected nodes. There is one node called the source to which all the other nodes want to connect and it does not share the costs of the connectivity. As a node may need to go through other nodes to reach the source, the intermediate nodes may behave strategically to block the connection by cutting the edges adjacent to them. To prevent such strategical behavior, we design cost sharing mechanisms to incentivize all nodes not to cut any edge so that we can minimize the total cost for connecting all the nodes. ",Kein DOI-Link verfügbar,2201.05976v2,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Cost Sharing under Private Valuation and Connection Control,1970,"  We consider a cost sharing problem on a weighted undirected graph, where all the nodes want to connect to a special node called source, and they need to share the total cost (weights) of the used edges. Each node except for the source has a private valuation of the connection, and it may block others' connections by strategically cutting its adjacent edges to reduce its cost share, which may increase the total cost. We aim to design mechanisms to prevent the nodes from misreporting their valuations and cutting their adjacent edges. We first show that it is impossible for such a mechanism to further satisfy budget balance (cover the total cost) and efficiency (maximize social welfare). Then, we design two feasible cost sharing mechanisms that incentivize each node to offer all its adjacent edges and truthfully report its valuation, and also satisfy either budget balance or efficiency. ",Kein DOI-Link verfügbar,2303.03083v1,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Cost Sharing under Private Costs and Connection Control on Directed   Acyclic Graphs,1970,"  We consider a cost sharing problem on a weighted directed acyclic graph (DAG) with a source node to which all the other nodes want to connect. The cost (weight) of each edge is private information reported by multiple contractors, and among them, only one contractor is selected as the builder. All the nodes except for the source need to share the total cost of the used edges. However, they may block others' connections to the source by strategically cutting their outgoing edges to reduce their cost share, which may increase the total cost of connectivity. To minimize the total cost of connectivity, we design a cost sharing mechanism to incentivize each node to offer all its outgoing edges and each contractor to report all the edges' weights truthfully, and show the properties of the proposed mechanism. In addition, our mechanism outperforms the two benchmark mechanisms. ",Kein DOI-Link verfügbar,2311.08903v1,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",DarkGS: Learning Neural Illumination and 3D Gaussians Relighting for   Robotic Exploration in the Dark,1970,"  Humans have the remarkable ability to construct consistent mental models of an environment, even under limited or varying levels of illumination. We wish to endow robots with this same capability. In this paper, we tackle the challenge of constructing a photorealistic scene representation under poorly illuminated conditions and with a moving light source. We approach the task of modeling illumination as a learning problem, and utilize the developed illumination model to aid in scene reconstruction. We introduce an innovative framework that uses a data-driven approach, Neural Light Simulators (NeLiS), to model and calibrate the camera-light system. Furthermore, we present DarkGS, a method that applies NeLiS to create a relightable 3D Gaussian scene model capable of real-time, photorealistic rendering from novel viewpoints. We show the applicability and robustness of our proposed simulator and system in a variety of real-world environments. ",Kein DOI-Link verfügbar,2403.10814v1,Yes,innovative(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Testing of Autonomous Driving Systems: Where Are We and Where Should We   Go?,1970,"  Autonomous driving has shown great potential to reform modern transportation. Yet its reliability and safety have drawn a lot of attention and concerns. Compared with traditional software systems, autonomous driving systems (ADSs) often use deep neural networks in tandem with logic-based modules. This new paradigm poses unique challenges for software testing. Despite the recent development of new ADS testing techniques, it is not clear to what extent those techniques have addressed the needs of ADS practitioners. To fill this gap, we present the first comprehensive study to identify the current practices and needs of ADS testing. We conducted semi-structured interviews with developers from 10 autonomous driving companies and surveyed 100 developers who have worked on autonomous driving systems. A systematic analysis of the interview and survey data revealed 7 common practices and 4 emerging needs of autonomous driving testing. Through a comprehensive literature review, we developed a taxonomy of existing ADS testing techniques and analyzed the gap between ADS research and practitioners' needs. Finally, we proposed several future directions for SE researchers, such as developing test reduction techniques to accelerate simulation-based ADS testing. ",Kein DOI-Link verfügbar,2106.12233v5,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College","Streaming quanta sensors for online, high-performance imaging and vision",1970,"  Recently quanta image sensors (QIS) -- ultra-fast, zero-read-noise binary image sensors -- have demonstrated remarkable imaging capabilities in many challenging scenarios. Despite their potential, the adoption of these sensors is severely hampered by (a) high data rates and (b) the need for new computational pipelines to handle the unconventional raw data. We introduce a simple, low-bandwidth computational pipeline to address these challenges. Our approach is based on a novel streaming representation with a small memory footprint, efficiently capturing intensity information at multiple temporal scales. Updating the representation requires only 16 floating-point operations/pixel, which can be efficiently computed online at the native frame rate of the binary frames. We use a neural network operating on this representation to reconstruct videos in real-time (10-30 fps). We illustrate why such representation is well-suited for these emerging sensors, and how it offers low latency and high frame rate while retaining flexibility for downstream computer vision. Our approach results in significant data bandwidth reductions ~100X and real-time image reconstruction and computer vision -- $10^4$-$10^5$ reduction in computation than existing state-of-the-art approach while maintaining comparable quality. To the best of our knowledge, our approach is the first to achieve online, real-time image reconstruction on QIS. ",Kein DOI-Link verfügbar,2406.00859v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Learning Scalable Structural Representations for Link Prediction with   Bloom Signatures,1970,"  Graph neural networks (GNNs) have shown great potential in learning on graphs, but they are known to perform sub-optimally on link prediction tasks. Existing GNNs are primarily designed to learn node-wise representations and usually fail to capture pairwise relations between target nodes, which proves to be crucial for link prediction. Recent works resort to learning more expressive edge-wise representations by enhancing vanilla GNNs with structural features such as labeling tricks and link prediction heuristics, but they suffer from high computational overhead and limited scalability. To tackle this issue, we propose to learn structural link representations by augmenting the message-passing framework of GNNs with Bloom signatures. Bloom signatures are hashing-based compact encodings of node neighborhoods, which can be efficiently merged to recover various types of edge-wise structural features. We further show that any type of neighborhood overlap-based heuristic can be estimated by a neural network that takes Bloom signatures as input. GNNs with Bloom signatures are provably more expressive than vanilla GNNs and also more scalable than existing edge-wise models. Experimental results on five standard link prediction benchmarks show that our proposed model achieves comparable or better performance than existing edge-wise GNN models while being 3-200 $\times$ faster and more memory-efficient for online inference. ",Kein DOI-Link verfügbar,2312.16784v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Generating Progressive Images from Pathological Transitions via   Diffusion Model,1970,"  Deep learning is widely applied in computer-aided pathological diagnosis, which alleviates the pathologist workload and provide timely clinical analysis. However, most models generally require large-scale annotated data for training, which faces challenges due to the sampling and annotation scarcity in pathological images. The rapid developing generative models shows potential to generate more training samples from recent studies. However, they also struggle in generalization diversity with limited training data, incapable of generating effective samples. Inspired by the pathological transitions between different stages, we propose an adaptive depth-controlled diffusion (ADD) network to generate pathological progressive images for effective data augmentation. This novel approach roots in domain migration, where a hybrid attention strategy guides the bidirectional diffusion, blending local and global attention priorities. With feature measuring, the adaptive depth-controlled strategy ensures the migration and maintains locational similarity in simulating the pathological feature transition. Based on tiny training set (samples less than 500), the ADD yields cross-domain progressive images with corresponding soft-labels. Experiments on two datasets suggest significant improvements in generation diversity, and the effectiveness with generated progressive samples are highlighted in downstream classifications. The code is available at https://github.com/Rowerliu/ADD. ",Kein DOI-Link verfügbar,2311.12316v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Transformer-based Selective Super-Resolution for Efficient Image   Refinement,1970,"  Conventional super-resolution methods suffer from two drawbacks: substantial computational cost in upscaling an entire large image, and the introduction of extraneous or potentially detrimental information for downstream computer vision tasks during the refinement of the background. To solve these issues, we propose a novel transformer-based algorithm, Selective Super-Resolution (SSR), which partitions images into non-overlapping tiles, selects tiles of interest at various scales with a pyramid architecture, and exclusively reconstructs these selected tiles with deep features. Experimental results on three datasets demonstrate the efficiency and robust performance of our approach for super-resolution. Compared to the state-of-the-art methods, the FID score is reduced from 26.78 to 10.41 with 40% reduction in computation cost for the BDD100K dataset. The source code is available at https://github.com/destiny301/SSR. ",Kein DOI-Link verfügbar,2312.05803v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",RecGS: Removing Water Caustic with Recurrent Gaussian Splatting,1970,"  Water caustics are commonly observed in seafloor imaging data from shallow-water areas. Traditional methods that remove caustic patterns from images often rely on 2D filtering or pre-training on an annotated dataset, hindering the performance when generalizing to real-world seafloor data with 3D structures. In this paper, we present a novel method Recurrent Gaussian Splatting (RecGS), which takes advantage of today's photorealistic 3D reconstruction technology, 3DGS, to separate caustics from seafloor imagery. With a sequence of images taken by an underwater robot, we build 3DGS recurrently and decompose the caustic with low-pass filtering in each iteration. In the experiments, we analyze and compare with different methods, including joint optimization, 2D filtering, and deep learning approaches. The results show that our method can effectively separate the caustic from the seafloor, improving the visual appearance, and can be potentially applied on more problems with inconsistent illumination. ",Kein DOI-Link verfügbar,2407.10318v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College","When Cyber-Physical Systems Meet AI: A Benchmark, an Evaluation, and a   Way Forward",1970,"  Cyber-physical systems (CPS) have been broadly deployed in safety-critical domains, such as automotive systems, avionics, medical devices, etc. In recent years, Artificial Intelligence (AI) has been increasingly adopted to control CPS. Despite the popularity of AI-enabled CPS, few benchmarks are publicly available. There is also a lack of deep understanding on the performance and reliability of AI-enabled CPS across different industrial domains. To bridge this gap, we initiate to create a public benchmark of industry-level CPS in seven domains and build AI controllers for them via state-of-the-art deep reinforcement learning (DRL) methods. Based on that, we further perform a systematic evaluation of these AI-enabled systems with their traditional counterparts to identify the current challenges and explore future opportunities. Our key findings include (1) AI controllers do not always outperform traditional controllers, (2) existing CPS testing techniques (falsification, specifically) fall short of analyzing AI-enabled CPS, and (3) building a hybrid system that strategically combines and switches between AI controllers and traditional controllers can achieve better performance across different domains. Our results highlight the need for new testing techniques for AI-enabled CPS and the need for more investigations into hybrid CPS systems to achieve optimal performance and reliability. ",https://doi.org/10.1145/3510457.3513049,2111.04324v2,Yes,strategically(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",TARGET: Automated Scenario Generation from Traffic Rules for Testing   Autonomous Vehicles,1970,"  Ensuring the safety and robustness of autonomous driving systems (ADSs) is imperative. One of the crucial methods towards this assurance is the meticulous construction and execution of test scenarios, a task often regarded as tedious and laborious. In response to this challenge, this paper introduces TARGET, an end-to-end framework designed for the automatic generation of test scenarios grounded in established traffic rules. Specifically, we design a domain-specific language (DSL) with concise and expressive syntax for scenario descriptions. To handle the natural language complexity and ambiguity in traffic rule descriptions, we leverage a large language model to automatically extract knowledge from traffic rules and convert the traffic rule descriptions to DSL representations. Based on these representations, TARGET synthesizes executable test scenario scripts to render the testing scenarios in a simulator. Comprehensive evaluations of the framework were conducted on four distinct ADSs, yielding a total of 217 test scenarios spread across eight diverse maps. These scenarios identify approximately 700 rule violations, collisions, and other significant issues, including navigation failures. Moreover, for each detected anomaly, TARGET provides detailed scenario recordings and log reports, significantly easing the process of troubleshooting and root cause analysis. Two of these causes have been confirmed by the ADS developers; one is corroborated by an existing bug report from the ADS, and the other one is attributed to the limited functionality of the ADS. ",Kein DOI-Link verfügbar,2305.06018v2,Yes,meticulous(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",An ensemble learning approach for software semantic clone detection,1970,"  Code clone is a serious problem in software and has the potential to software defects, maintenance overhead, and licensing violations. Therefore, clone detection is important for reducing maintenance effort and improving code quality during software evolution. A variety of clone detection techniques have been proposed to identify similar code in software. However, few of them can efficiently detect semantic clones (functionally similar code without any syntactic resemblance). Recently, several deep learning based clone detectors are proposed to detect semantic clones. However, these approaches have high cost in data labelling and model training. In this paper, we propose a novel approach that leverages word embedding and ensemble learning techniques to detect semantic clones. Our evaluation on a commonly used clone benchmark, BigCloneBench, shows that our approach significantly improves the precision and recall of semantic clone detection, in comparison to a token-based clone detector, SourcererCC, and another deep learning based clone detector, CDLH. ",Kein DOI-Link verfügbar,2010.04336v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Knowledge-Based Version Incompatibility Detection for Deep Learning,1970,"  Version incompatibility issues are rampant when reusing or reproducing deep learning models and applications. Existing techniques are limited to library dependency specifications declared in PyPI. Therefore, these techniques cannot detect version issues due to undocumented version constraints or issues involving hardware drivers or OS. To address this challenge, we propose to leverage the abundant discussions of DL version issues from Stack Overflow to facilitate version incompatibility detection. We reformulate the problem of knowledge extraction as a Question-Answering (QA) problem and use a pre-trained QA model to extract version compatibility knowledge from online discussions. The extracted knowledge is further consolidated into a weighted knowledge graph to detect potential version incompatibilities when reusing a DL project. Our evaluation results show that (1) our approach can accurately extract version knowledge with 84% accuracy, and (2) our approach can accurately identify 65% of known version issues in 10 popular DL projects with a high precision (92%), while two state-of-the-art approaches can only detect 29% and 6% of these issues with 33% and 17% precision respectively. ",Kein DOI-Link verfügbar,2308.13276v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Where Do Large Language Models Fail When Generating Code?,1970,"  Large Language Models (LLMs) have shown great potential in code generation. However, current LLMs still cannot reliably generate correct code. Moreover, it is unclear what kinds of code generation errors LLMs can make. To address this, we conducted an empirical study to analyze incorrect code snippets generated by six popular LLMs on the HumanEval dataset. We analyzed these errors alongside two dimensions of error characteristics -- semantic characteristics and syntactic characteristics -- to derive a comprehensive code generation error taxonomy for LLMs through open coding and thematic analysis. We then labeled all 557 incorrect code snippets based on this taxonomy. Our results showed that the six LLMs exhibited similar distributions of syntactic characteristics while different distributions of semantic characteristics. Furthermore, we analyzed the correlation between different error characteristics and factors such as task complexity, code length, and test-pass rate. Finally, we highlight the challenges that LLMs may encounter when generating code and propose implications for future research on reliable code generation with LLMs. ",Kein DOI-Link verfügbar,2406.08731v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Shuffle Instances-based Vision Transformer for Pancreatic Cancer ROSE   Image Classification,1970,"  The rapid on-site evaluation (ROSE) technique can signifi-cantly accelerate the diagnosis of pancreatic cancer by im-mediately analyzing the fast-stained cytopathological images. Computer-aided diagnosis (CAD) can potentially address the shortage of pathologists in ROSE. However, the cancerous patterns vary significantly between different samples, making the CAD task extremely challenging. Besides, the ROSE images have complicated perturbations regarding color distribution, brightness, and contrast due to different staining qualities and various acquisition device types. To address these challenges, we proposed a shuffle instances-based Vision Transformer (SI-ViT) approach, which can reduce the perturbations and enhance the modeling among the instances. With the regrouped bags of shuffle instances and their bag-level soft labels, the approach utilizes a regression head to make the model focus on the cells rather than various perturbations. Simultaneously, combined with a classification head, the model can effectively identify the general distributive patterns among different instances. The results demonstrate significant improvements in the classification accuracy with more accurate attention regions, indicating that the diverse patterns of ROSE images are effectively extracted, and the complicated perturbations are significantly reduced. It also suggests that the SI-ViT has excellent potential in analyzing cytopathological images. The code and experimental results are available at https://github.com/sagizty/MIL-SI. ",Kein DOI-Link verfügbar,2208.06833v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",CPIA Dataset: A Comprehensive Pathological Image Analysis Dataset for   Self-supervised Learning Pre-training,1970,"  Pathological image analysis is a crucial field in computer-aided diagnosis, where deep learning is widely applied. Transfer learning using pre-trained models initialized on natural images has effectively improved the downstream pathological performance. However, the lack of sophisticated domain-specific pathological initialization hinders their potential. Self-supervised learning (SSL) enables pre-training without sample-level labels, which has great potential to overcome the challenge of expensive annotations. Thus, studies focusing on pathological SSL pre-training call for a comprehensive and standardized dataset, similar to the ImageNet in computer vision. This paper presents the comprehensive pathological image analysis (CPIA) dataset, a large-scale SSL pre-training dataset combining 103 open-source datasets with extensive standardization. The CPIA dataset contains 21,427,877 standardized images, covering over 48 organs/tissues and about 100 kinds of diseases, which includes two main data types: whole slide images (WSIs) and characteristic regions of interest (ROIs). A four-scale WSI standardization process is proposed based on the uniform resolution in microns per pixel (MPP), while the ROIs are divided into three scales artificially. This multi-scale dataset is built with the diagnosis habits under the supervision of experienced senior pathologists. The CPIA dataset facilitates a comprehensive pathological understanding and enables pattern discovery explorations. Additionally, to launch the CPIA dataset, several state-of-the-art (SOTA) baselines of SSL pre-training and downstream evaluation are specially conducted. The CPIA dataset along with baselines is available at https://github.com/zhanglab2021/CPIA_Dataset. ",Kein DOI-Link verfügbar,2310.17902v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Building Open-Ended Embodied Agent via Language-Policy Bidirectional   Adaptation,1970,"  Building embodied agents on integrating Large Language Models (LLMs) and Reinforcement Learning (RL) have revolutionized human-AI interaction: researchers can now leverage language instructions to plan decision-making for open-ended tasks. However, existing research faces challenges in meeting the requirement of open-endedness. They typically either train LLM/RL models to adapt to a fixed counterpart, limiting exploration of novel skills and hindering the efficacy of human-AI interaction. To this end, we present OpenPAL, a co-training framework comprising two stages: (1) fine-tuning a pre-trained LLM to translate human instructions into goals for planning, and goal-conditioned training a policy for decision-making; (2) co-training to align the LLM and policy, achieving instruction open-endedness. We conducted experiments using Contra, an open-ended FPS game, demonstrating that an agent trained with OpenPAL not only comprehends arbitrary instructions but also exhibits efficient execution. These results suggest that OpenPAL holds the potential to construct open-ended embodied agents in practical scenarios. ",Kein DOI-Link verfügbar,2401.00006v3,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",CellMix: A General Instance Relationship based Method for Data   Augmentation Towards Pathology Image Classification,1970,"  In pathology image analysis, obtaining and maintaining high-quality annotated samples is an extremely labor-intensive task. To overcome this challenge, mixing-based methods have emerged as effective alternatives to traditional preprocessing data augmentation techniques. Nonetheless, these methods fail to fully consider the unique features of pathology images, such as local specificity, global distribution, and inner/outer-sample instance relationships. To better comprehend these characteristics and create valuable pseudo samples, we propose the CellMix framework, which employs a novel distribution-oriented in-place shuffle approach. By dividing images into patches based on the granularity of pathology instances and shuffling them within the same batch, the absolute relationships between instances can be effectively preserved when generating new samples. Moreover, we develop a curriculum learning-inspired, loss-driven strategy to handle perturbations and distribution-related noise during training, enabling the model to adaptively fit the augmented data. Our experiments in pathology image classification tasks demonstrate state-of-the-art (SOTA) performance on 7 distinct datasets. This innovative instance relationship-centered method has the potential to inform general data augmentation approaches for pathology image classification. The associated codes are available at https://github.com/sagizty/CellMix. ",Kein DOI-Link verfügbar,2301.11513v2,Yes,"innovative(1), potent(1)"
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Can Steering Wheel Detect Your Driving Fatigue?,1970,"  Automated Driving System (ADS) has attracted increasing attention from both industrial and academic communities due to its potential for increasing the safety, mobility and efficiency of existing transportation systems. The state-of-the-art ADS follows the human-in-the-loop (HITL) design, where the driver's anomalous behaviour is closely monitored by the system. Though many approaches have been proposed for detecting driver fatigue, they largely depend on vehicle driving parameters and facial features, which lacks reliability. Approaches using physiological based sensors (e.g., electroencephalogram or electrocardiogram) are either too clumsy to wear or impractical to install. In this paper, we propose a novel driver fatigue detection method by embedding surface electromyography (sEMG) sensors on a steering wheel. Compared with the existing methods, our approach is able to collect bio-signals in a non-intrusive way and detect driver fatigue at an earlier stage. The experimental results show that our approach outperforms existing methods with the weighted average F1 scores about 90%. We also propose promising future directions to deploy this approach in real-life settings, such as applying multimodal learning using several supplementary sensors. ",Kein DOI-Link verfügbar,2010.10327v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",A Declarative Metamorphic Testing Framework for Autonomous Driving,1970,"  Autonomous driving has gained much attention from both industry and academia. Currently, Deep Neural Networks (DNNs) are widely used for perception and control in autonomous driving. However, several fatal accidents caused by autonomous vehicles have raised serious safety concerns about autonomous driving models. Some recent studies have successfully used the metamorphic testing technique to detect thousands of potential issues in some popularly used autonomous driving models. However, prior study is limited to a small set of metamorphic relations, which do not reflect rich, real-world traffic scenarios and are also not customizable. This paper presents a novel declarative rule-based metamorphic testing framework called RMT. RMT provides a rule template with natural language syntax, allowing users to flexibly specify an enriched set of testing scenarios based on real-world traffic rules and domain knowledge. RMT automatically parses human-written rules to metamorphic relations using an NLP-based rule parser referring to an ontology list and generates test cases with a variety of image transformation engines. We evaluated RMT on three autonomous driving models. With an enriched set of metamorphic relations, RMT detected a significant number of abnormal model predictions that were not detected by prior work. Through a large-scale human study on Amazon Mechanical Turk, we further confirmed the authenticity of test cases generated by RMT and the validity of detected abnormal model predictions. ",Kein DOI-Link verfügbar,2012.10672v4,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",SAD: Semi-Supervised Anomaly Detection on Dynamic Graphs,1970,"  Anomaly detection aims to distinguish abnormal instances that deviate significantly from the majority of benign ones. As instances that appear in the real world are naturally connected and can be represented with graphs, graph neural networks become increasingly popular in tackling the anomaly detection problem. Despite the promising results, research on anomaly detection has almost exclusively focused on static graphs while the mining of anomalous patterns from dynamic graphs is rarely studied but has significant application value. In addition, anomaly detection is typically tackled from semi-supervised perspectives due to the lack of sufficient labeled data. However, most proposed methods are limited to merely exploiting labeled data, leaving a large number of unlabeled samples unexplored. In this work, we present semi-supervised anomaly detection (SAD), an end-to-end framework for anomaly detection on dynamic graphs. By a combination of a time-equipped memory bank and a pseudo-label contrastive learning module, SAD is able to fully exploit the potential of large unlabeled samples and uncover underlying anomalies on evolving graph streams. Extensive experiments on four real-world datasets demonstrate that SAD efficiently discovers anomalies from dynamic graphs and outperforms existing advanced methods even when provided with only little labeled data. ",Kein DOI-Link verfügbar,2305.13573v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Text as Image: Learning Transferable Adapter for Multi-Label   Classification,1970,"  Pre-trained vision-language models have notably accelerated progress of open-world concept recognition. Their impressive zero-shot ability has recently been transferred to multi-label image classification via prompt tuning, enabling to discover novel labels in an open-vocabulary manner. However, this paradigm suffers from non-trivial training costs, and becomes computationally prohibitive for a large number of candidate labels. To address this issue, we note that vision-language pre-training aligns images and texts in a unified embedding space, making it potential for an adapter network to identify labels in visual modality while be trained in text modality. To enhance such cross-modal transfer ability, a simple yet effective method termed random perturbation is proposed, which enables the adapter to search for potential visual embeddings by perturbing text embeddings with noise during training, resulting in better performance in visual modality. Furthermore, we introduce an effective approach to employ large language models for multi-label instruction-following text generation. In this way, a fully automated pipeline for visual label recognition is developed without relying on any manual data. Extensive experiments on public benchmarks show the superiority of our method in various multi-label classification tasks. ",Kein DOI-Link verfügbar,2312.04160v1,Yes,potent(2)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",From Perfect to Noisy World Simulation: Customizable Embodied   Multi-modal Perturbations for SLAM Robustness Benchmarking,1970,"  Embodied agents require robust navigation systems to operate in unstructured environments, making the robustness of Simultaneous Localization and Mapping (SLAM) models critical to embodied agent autonomy. While real-world datasets are invaluable, simulation-based benchmarks offer a scalable approach for robustness evaluations. However, the creation of a challenging and controllable noisy world with diverse perturbations remains under-explored. To this end, we propose a novel, customizable pipeline for noisy data synthesis, aimed at assessing the resilience of multi-modal SLAM models against various perturbations. The pipeline comprises a comprehensive taxonomy of sensor and motion perturbations for embodied multi-modal (specifically RGB-D) sensing, categorized by their sources and propagation order, allowing for procedural composition. We also provide a toolbox for synthesizing these perturbations, enabling the transformation of clean environments into challenging noisy simulations. Utilizing the pipeline, we instantiate the large-scale Noisy-Replica benchmark, which includes diverse perturbation types, to evaluate the risk tolerance of existing advanced RGB-D SLAM models. Our extensive analysis uncovers the susceptibilities of both neural (NeRF and Gaussian Splatting -based) and non-neural SLAM models to disturbances, despite their demonstrated accuracy in standard benchmarks. Our code is publicly available at https://github.com/Xiaohao-Xu/SLAM-under-Perturbation. ",Kein DOI-Link verfügbar,2406.16850v1,Yes,invaluable(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Sulfur Vacancy Related Optical Transitions in Graded Alloys of MoxW1-xS2   Monolayers,1970,"  Engineering the electronic bandgap is of utmost importance in diverse domains ranging from information processing and communication technology to sensing and renewable energy applications. Transition metal dichalcogenides (TMDCs) provide an ideal platform for achieving this goal through techniques including alloying, doping, and creating in-plane or out-of-plane heterostructures. Here, we report on the synthesis and characterization of atomically controlled two-dimensional graded alloy of MoxW1-xS2, wherein the center region is Mo rich and gradually transitions towards a higher concentration of W atoms at the edges. This unique alloy structure leads to a continuously tunable bandgap, ranging from 1.85 eV in the center to 1.95 eV at the edges consistent with the larger band gap of WS2 relative to MoS2. Aberration-corrected high-angle annular dark-field scanning transmission electron microscopy showed the presence of sulfur monovacancy, VS, whose concentration varied across the graded MoxW1-xS2 layer as a function of Mo content with the highest value in the Mo rich center region. Optical spectroscopy measurements supported by ab initio calculations reveal a doublet electronic state of VS, which was split due to the spin-orbit interaction, with energy levels close to the conduction band or deep in the band gap depending on whether the vacancy is surrounded by W atoms or Mo atoms. This unique electronic configuration of VS in the alloy gave rise to four spin-allowed optical transitions between the VS levels and the valence bands. Our work highlights the potential of simultaneous defect and optical engineering of novel devices based on these 2D monolayers. ",Kein DOI-Link verfügbar,2308.14990v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",PharmacyGPT: The AI Pharmacist,1970,"  In this study, we introduce PharmacyGPT, a novel framework to assess the capabilities of large language models (LLMs) such as ChatGPT and GPT-4 in emulating the role of clinical pharmacists. Our methodology encompasses the utilization of LLMs to generate comprehensible patient clusters, formulate medication plans, and forecast patient outcomes. We conduct our investigation using real data acquired from the intensive care unit (ICU) at the University of North Carolina Chapel Hill (UNC) Hospital. Our analysis offers valuable insights into the potential applications and limitations of LLMs in the field of clinical pharmacy, with implications for both patient care and the development of future AI-driven healthcare solutions. By evaluating the performance of PharmacyGPT, we aim to contribute to the ongoing discourse surrounding the integration of artificial intelligence in healthcare settings, ultimately promoting the responsible and efficacious use of such technologies. ",Kein DOI-Link verfügbar,2307.10432v2,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Photo-degradation Protection in 2D In-Plane Heterostructures Revealed by   Hyperspectral Nanoimaging: the Role of Nano-Interface 2D Alloys,1970,"  Single-layer heterostructures exhibit striking quasiparticle properties and many-body interaction effects that hold promise for a range of applications. However, their properties can be altered by intrinsic and extrinsic defects, thus diminishing their applicability. Therefore, it is of paramount importance to identify defects and understand 2D materials' degradation over time using advanced multimodal imaging techniques as well as stabilize degradation via built-in interface protection. Here we implemented a liquid-phase precursor approach to synthesize 2D in-plane MoS2-WS2 heterostructures exhibiting nanoscale alloyed interfaces and map exotic interface effects during photo-degradation using a novel combination of hyperspectral tip-enhanced photoluminescence, Raman and near-field nanoscopy. Surprisingly, 2D alloyed regions exhibit remarkable thermal and photo-degradation stability providing protection against oxidation. Coupled with surface and interface strain, 2D alloy regions create localized potential wells that concentrate excitonic species via a charge carrier funneling effect. These results provide a clear understanding of the importance of 2D alloys as systems able to withstand degradation effects over time, and could be now used to stabilize optoelectronic devices based on 2D materials. ",Kein DOI-Link verfügbar,2005.11361v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Wafer-scale epitaxial growth of single orientation WS2 monolayers on   sapphire,1970,"  Realization of wafer-scale single-crystal films of transition metal dichalcogenides (TMDs) such as tungsten sulfide requires epitaxial growth and coalescence of oriented domains to form a continuous monolayer. The domains must be oriented in the same crystallographic direction on the substrate to avoid the formation of metallic inversion domain boundaries (IDBs) which are a common feature of layered chalcogenides. Here we demonstrate fully-coalesced single orientation tungsten sulfide monolayers on 2-inch diameter c-plane sapphire by metalorganic chemical vapor deposition using a multi-step growth process. High growth temperatures and sulfur/metal ratios were required to reduce domain misorientation and achieve epitaxial tungsten sulfide monolayers with low in-plane rotational twist (0.09 deg). Transmission electron microscopy analysis reveals that the tungsten sulfide monolayers lack IDBs but instead have translational boundaries that arise when tungsten sulfide domains with slightly off-set lattices merge together. By adjusting the monolayer growth rate, the density of translational boundaries and bilayer coverage were significantly reduced. The preferred orientation of domains is attributed to the presence of steps on the sapphire surface coupled with growth conditions promote surface diffusion and oriented attachment. The transferred tungsten sulfide monolayers show neutral and charged exciton emission at 80K with negligible defect-related luminescence. Back-gated tungsten sulfide field effect transistors exhibited mobility of 16 cm2/Vs. The results demonstrate the potential of achieving wafer-scale TMD monolayers free of inversion domains with properties approaching that of exfoliated flakes. ",Kein DOI-Link verfügbar,2006.10952v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Directivity modulation of exciton emission using single dielectric   nanospheres,1970,"  Coupling emitters with nanoresonators is an effective strategy to control light emission at the subwavelength scale with high efficiency. Low-loss dielectric nanoantennas hold particular promise for this purpose, owing to their strong Mie resonances. Herein, we explore a highly miniaturized platform for the control of emission based on individual subwavelength Si nanospheres (SiNSs) to modulate the directional excitation and exciton emission of two-dimensional transition metal dichalcogenides (2D TMDs). A modified Mie theory for dipole-sphere hybrid systems is derived to instruct the optimal design for desirable modulation performance. Controllable forward-to-backward intensity ratios are experimentally validated in 532 nm laser excitation and 635 nm exciton emission from a monolayer WS2. Versatile light emission control along all device orientations is achieved for different emitters and excitation wavelengths, benefiting from the facile size control and isotropic shape of SiNSs. Simultaneous modulation of excitation and emission via a single SiNS at visible wavelengths significantly improves the efficiency and directivity of TMD exciton emission and leads to the potential of multifunctional integrated photonics. Overall, our work opens promising opportunities for nanophotonics and polaritonic systems, enabling efficient manipulation, enhancement and reconfigurability of light-matter interactions. ",Kein DOI-Link verfügbar,2010.02342v1,Yes,"versatile(1), potent(1)"
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Remote-contact catalysis for target-diameter semiconducting carbon   nanotube array,1970,"  Electrostatic catalysis has been an exciting development in chemical synthesis (beyond enzymes catalysis) in recent years, boosting reaction rates and selectively producing certain reaction products. Most of the studies to date have been focused on using external electric field (EEF) to rearrange the charge distribution in small molecule reactions such as Diels-Alder addition, carbene reaction, etc. However, in order for these EEFs to be effective, a field on the order of 1 V/nm (10 MV/cm) is required, and the direction of the EEF has to be aligned with the reaction axis. Such a large and oriented EEF will be challenging for large-scale implementation, or materials growth with multiple reaction axis or steps. Here, we demonstrate that the energy band at the tip of an individual single-walled carbon nanotube (SWCNT) can be spontaneously shifted in a high-permittivity growth environment, with its other end in contact with a low-work function electrode (e.g., hafnium carbide or titanium carbide). By adjusting the Fermi level at a point where there is a substantial disparity in the density of states (DOS) between semiconducting (s-) and metallic (m-) SWCNTs, we achieve effective electrostatic catalysis for s-SWCNT growth assisted by a weak EEF perturbation (200V/cm). This approach enables the production of high-purity (99.92%) s-SWCNT horizontal arrays with narrow diameter distribution (0.95+-0.04 nm), targeting the requirement of advanced SWCNT-based electronics for future computing. These findings highlight the potential of electrostatic catalysis in precise materials growth, especially for s-SWCNTs, and pave the way for the development of advanced SWCNT-based electronics. ",Kein DOI-Link verfügbar,2404.02981v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Autonomous Investigations over WS$_2$ and Au{111} with Scanning Probe   Microscopy,1970,"  Individual atomic defects in 2D materials impact their macroscopic functionality. Correlating the interplay is challenging, however, intelligent hyperspectral scanning tunneling spectroscopy (STS) mapping provides a feasible solution to this technically difficult and time consuming problem. Here, dense spectroscopic volume is collected autonomously via Gaussian process regression, where convolutional neural networks are used in tandem for spectral identification. Acquired data enable defect segmentation, and a workflow is provided for machine-driven decision making during experimentation with capability for user customization. We provide a means towards autonomous experimentation for the benefit of both enhanced reproducibility and user-accessibility. Hyperspectral investigations on WS$_2$ sulfur vacancy sites are explored, which is combined with local density of states confirmation on the Au{111} herringbone reconstruction. Chalcogen vacancies, pristine WS$_2$, Au face-centered cubic, and Au hexagonal close packed regions are examined and detected by machine learning methods to demonstrate the potential of artificial intelligence for hyperspectral STS mapping. ",https://doi.org/10.1038/s41524-022-00777-9,2110.03351v5,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Giant room-temperature nonlinearities from a monolayer Janus topological   semiconductor,1970,"  Nonlinear optical materials possess wide applications, ranging from terahertz and mid-infrared detection to energy harvesting. Recently, the correlations between nonlinear optical responses and topological properties, such as Berry curvature and the quantum metric tensor, have stimulated great interest. Here, we report giant room-temperature nonlinearities in an emergent non-centrosymmetric two-dimensional topological material, the Janus transition metal dichalcogenides in the 1T' phase, which are synthesized by an advanced atomic-layer substitution method. High harmonic generation, terahertz emission spectroscopy, and second harmonic generation measurements consistently reveal orders-of-the-magnitude enhancement in terahertz-frequency nonlinearities of 1T' MoSSe (e.g., > 50 times higher than 2H MoS$_2$ for 18th order harmonic generation; > 20 times higher than 2H MoS$_2$ for terahertz emission). It is elucidated that such colossal nonlinear optical responses come from topological band mixing and strong inversion symmetry breaking due to the Janus structure. Our work defines general protocols for designing materials with large nonlinearities and preludes the applications of topological materials in optoelectronics down to the monolayer limit. This two-dimensional form of topological materials also constitute a unique platform for examining origin of the anomalous high-harmonic generation, with potential applications as building blocks for scalable attosecond sources. ",Kein DOI-Link verfügbar,2304.00750v1,Yes,potent(1)
0000-0003-4252-6070,Tianyi Zhang,"University of Cambridge, University of Cambridge Clare College",Holistic Evaluation of Language Models,1970,"  Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models. ",Kein DOI-Link verfügbar,2211.09110v2,Yes,potent(1)
0000-0001-5298-7482,Christoph Eigen,"Jesus College, University of Cambridge, University of Cambridge",Observation of Weak Collapse in a Bose-Einstein Condensate,1970,"  We study the collapse of an attractive atomic Bose-Einstein condensate prepared in the uniform potential of an optical-box trap. We characterise the critical point for collapse and the collapse dynamics, observing universal behaviour in agreement with theoretical expectations. Most importantly, we observe a clear experimental signature of the counterintuitive weak collapse, namely that making the system more unstable can result in a smaller particle loss. We experimentally determine the scaling laws that govern the weak-collapse atom loss, providing a benchmark for the general theories of nonlinear wave phenomena. ",https://doi.org/10.1103/PhysRevX.6.041058,1609.00352v2,Yes,potent(1)
0000-0002-0694-5297,Gwenhivir Wyatt-Moon,"University of Cambridge, University of Cambridge Trinity Hall",Inkjet printed circuits with two-dimensional semiconductor inks for   high-performance electronics,1970,"  Air-stable semiconducting inks suitable for complementary logic are key to create low-power printed integrated circuits (ICs). High-performance printable electronic inks with two-dimensional materials have the potential to enable the next generation of high performance, low-cost printed digital electronics. Here we demonstrate air-stable, low voltage (< 5 V) operation of inkjet-printed n-type molybdenum disulfide (MoS2) and p-type indacenodithiophene-co-benzothiadiazole (IDT-BT) field-effect transistors (FETs), estimating a switching time of {\tau} ~ 3.3 {\mu}s for the MoS2 FETs. We achieve this by engineering high-quality MoS2 and air-stable IDT-BT inks suitable for inkjet-printing complementary pairs of n-type MoS2 and p-type IDT-BT FETs. We then integrate MoS2 and IDT-BT FETs to realise inkjet-printed complementary logic inverters with a voltage gain |Av| ~ 4 when in resistive load configuration and |Av| ~ 1.36 in complementary configuration. These results represent a key enabling step towards ubiquitous long-term stable, low-cost printed digital ICs. ",Kein DOI-Link verfügbar,2011.12359v1,Yes,potent(1)
0000-0002-8586-8444,Charlotte Williams,UCL,Predictors of Social Distancing and Mask-Wearing Behavior: Panel Survey   in Seven U.S. States,1970,"  This paper presents preliminary summary results from a longitudinal study of participants in seven U.S. states during the COVID-19 pandemic. In addition to standard socio-economic characteristics, we collect data on various economic preference parameters: time, risk, and social preferences, and risk perception biases. We pay special attention to predictors that are both important drivers of social distancing and are potentially malleable and susceptible to policy levers. We note three important findings: (1) demographic characteristics exert the largest influence on social distancing measures and mask-wearing, (2) we show that individual risk perception and cognitive biases exert a critical role in influencing the decision to adopt social distancing measures, (3) we identify important demographic groups that are most susceptible to changing their social distancing behaviors. These findings can help inform the design of policy interventions regarding targeting specific demographic groups, which can help reduce the transmission speed of the COVID-19 virus. ",Kein DOI-Link verfügbar,2009.13103v1,Yes,potent(1)
0000-0002-6275-2289,David Tuckett,UCL,Measuring Financial Sentiment to Predict Financial Instability: A New   Approach based on Text Analysis,1970,"  Following the financial crisis of the late 2000s, policy makers have shown considerable interest in monitoring financial stability. Several central banks now publish indices of financial stress, which are essentially based upon market related data. In this paper, we examine the potential for improving the indices by deriving information about emotion shifts in the economy. We report on a new approach, based on the content analysis of very large text databases, and termed directed algorithmic text analysis. The algorithm identifies, very rapidly, shifts through time in the relations between two core emotional groups. The method is robust. The same word-list is used to identify the two emotion groups across different studies. Membership of the words in the lists has been validated in psychological experiments. The words consist of everyday English words with no specific economic meaning. Initial results show promise. An emotion index capturing shifts between the two emotion groups in texts potentially referring to the whole US economy improves the one-quarter ahead consensus forecasts for real GDP growth. More specifically, the same indices are shown to Granger cause both the Cleveland and St Louis Federal Reserve Indices of Financial Stress. ",Kein DOI-Link verfügbar,1508.05357v1,Yes,potent(2)
0000-0001-7365-0651,Katherine James,The University of Edinburgh,Drug repurposing prediction for COVID-19 using probabilistic networks   and crowdsourced curation,1970,"  Severe acute respiratory syndrome coronavirus two (SARS-CoV-2), the virus responsible for the coronavirus disease 2019 (COVID-19) pandemic, represents an unprecedented global health challenge. Consequently, a large amount of research into the disease pathogenesis and potential treatments has been carried out in a short time frame. However, developing novel drugs is a costly and lengthy process, and is unlikely to deliver a timely treatment for the pandemic. Drug repurposing, by contrast, provides an attractive alternative, as existing drugs have already undergone many of the regulatory requirements. In this work we used a combination of network algorithms and human curation to search integrated knowledge graphs, identifying drug repurposing opportunities for COVID-19. We demonstrate the value of this approach, reporting on eight potential repurposing opportunities identified, and discuss how this approach could be incorporated into future studies. ",Kein DOI-Link verfügbar,2005.11088v2,Yes,potent(2)
0000-0002-8082-2818,Bernard Mulgrew,The University of Edinburgh,Latent Parameter Estimation in Fusion Networks Using Separable   Likelihoods,1970,"  Multi-sensor state space models underpin fusion applications in networks of sensors. Estimation of latent parameters in these models has the potential to provide highly desirable capabilities such as network self-calibration. Conventional solutions to the problem pose difficulties in scaling with the number of sensors due to the joint multi-sensor filtering involved when evaluating the parameter likelihood. In this article, we propose a separable pseudo-likelihood which is a more accurate approximation compared to a previously proposed alternative under typical operating conditions. In addition, we consider using separable likelihoods in the presence of many objects and ambiguity in associating measurements with objects that originated them. To this end, we use a state space model with a hypothesis based parameterisation, and, develop an empirical Bayesian perspective in order to evaluate separable likelihoods on this model using local filtering. Bayesian inference with this likelihood is carried out using belief propagation on the associated pairwise Markov random field. We specify a particle algorithm for latent parameter estimation in a linear Gaussian state space model and demonstrate its efficacy for network self-calibration using measurements from non-cooperative targets in comparison with alternatives. ",Kein DOI-Link verfügbar,1708.00842v2,Yes,potent(1)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",Tropical and Extratropical Cyclone Detection Using Deep Learning,1970,"  Extracting valuable information from large sets of diverse meteorological data is a time-intensive process. Machine learning methods can help improve both speed and accuracy of this process. Specifically, deep learning image segmentation models using the U-Net structure perform faster and can identify areas missed by more restrictive approaches, such as expert hand-labeling and a priori heuristic methods. This paper discusses four different state-of-the-art U-Net models designed for detection of tropical and extratropical cyclone Regions Of Interest (ROI) from two separate input sources: total precipitable water output from the Global Forecasting System (GFS) model and water vapor radiance images from the Geostationary Operational Environmental Satellite (GOES). These models are referred to as IBTrACS-GFS, Heuristic-GFS, IBTrACS-GOES, and Heuristic-GOES. All four U-Nets are fast information extraction tools and perform with a ROI detection accuracy ranging from 80% to 99%. These are additionally evaluated with the Dice and Tversky Intersection over Union (IoU) metrics, having Dice coefficient scores ranging from 0.51 to 0.76 and Tversky coefficients ranging from 0.56 to 0.74. The extratropical cyclone U-Net model performed 3 times faster than the comparable heuristic model used to detect the same ROI. The U-Nets were specifically selected for their capabilities in detecting cyclone ROI beyond the scope of the training labels. These machine learning models identified more ambiguous and active ROI missed by the heuristic model and hand-labeling methods commonly used in generating real-time weather alerts, having a potentially direct impact on public safety. ",https://doi.org/10.1175/JAMC-D-20-0117.1,2005.09056v1,Yes,potent(1)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",Comparing simulations and test data of a radiation damaged charge-couple   device for the Euclid mission,1970,"  The VIS instrument on board the Euclid mission is a weak-lensing experiment that depends on very precise shape measurements of distant galaxies obtained by a large CCD array. Due to the harsh radiative environment outside the Earth's atmosphere, it is anticipated that the CCDs over the mission lifetime will be degraded to an extent that these measurements will only be possible through the correction of radiation damage effects. We have therefore created a Monte Carlo model that simulates the physical processes taking place when transferring signal through a radiation-damaged CCD. The software is based on Shockley-Read-Hall theory, and is made to mimic the physical properties in the CCD as closely as possible. The code runs on a single electrode level and takes three dimensional trap position, potential structure of the pixel, and multi-level clocking into account. A key element of the model is that it also takes device specific simulations of electron density as a direct input, thereby avoiding to make any analytical assumptions about the size and density of the charge cloud. This paper illustrates how test data and simulated data can be compared in order to further our understanding of the positions and properties of the individual radiation-induced traps. ",https://doi.org/10.1117/1.JATIS.3.2.028001,1710.10958v1,Yes,potent(1)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",An energy consistent discretization of the nonhydrostatic equations in   primitive variables,1970,"  We derive a formulation of the nonhydrostatic equations in spherical geometry with a Lorenz staggered vertical discretization. The combination conserves a discrete energy in exact time integration when coupled with a mimetic horizontal discretization. The formulation is a version of Dubos and Tort (2014) rewritten in terms of primitive variables. It is valid for terrain following mass or height coordinates and for both Eulerian or vertically Lagrangian discretizations. The discretization relies on an extension to Simmons and Burridge (1981) vertical differencing which we show obeys a discrete derivative product rule. This product rule allows us to simplify the treatment of the vertical transport terms. Energy conservation is obtained via a term-by-term balance in the kinetic, internal and potential energy budgets, ensuring an energy-consistent discretization with no spurious sources of energy. We demonstrate convergence with respect to time truncation error in a spectral element code with a HEVI IMEX timestepping algorithm ",https://doi.org/10.1029/2019MS001783,1908.04430v1,Yes,potent(1)
0000-0002-7541-457X,David Hall,"The University of Manchester, The University of Manchester School of Materials",What can robotics research learn from computer vision research?,1970,"  The computer vision and robotics research communities are each strong. However progress in computer vision has become turbo-charged in recent years due to big data, GPU computing, novel learning algorithms and a very effective research methodology. By comparison, progress in robotics seems slower. It is true that robotics came later to exploring the potential of learning -- the advantages over the well-established body of knowledge in dynamics, kinematics, planning and control is still being debated, although reinforcement learning seems to offer real potential. However, the rapid development of computer vision compared to robotics cannot be only attributed to the former's adoption of deep learning. In this paper, we argue that the gains in computer vision are due to research methodology -- evaluation under strict constraints versus experiments; bold numbers versus videos. ",Kein DOI-Link verfügbar,2001.02366v2,Yes,potent(2)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Brain Structure-Function Fusing Representation Learning using   Adversarial Decomposed-VAE for Analyzing MCI,1970,"  Integrating the brain structural and functional connectivity features is of great significance in both exploring brain science and analyzing cognitive impairment clinically. However, it remains a challenge to effectively fuse structural and functional features in exploring the brain network. In this paper, a novel brain structure-function fusing-representation learning (BSFL) model is proposed to effectively learn fused representation from diffusion tensor imaging (DTI) and resting-state functional magnetic resonance imaging (fMRI) for mild cognitive impairment (MCI) analysis. Specifically, the decomposition-fusion framework is developed to first decompose the feature space into the union of the uniform and the unique spaces for each modality, and then adaptively fuse the decomposed features to learn MCI-related representation. Moreover, a knowledge-aware transformer module is designed to automatically capture local and global connectivity features throughout the brain. Also, a uniform-unique contrastive loss is further devised to make the decomposition more effective and enhance the complementarity of structural and functional features. The extensive experiments demonstrate that the proposed model achieves better performance than other competitive methods in predicting and analyzing MCI. More importantly, the proposed model could be a potential tool for reconstructing unified brain networks and predicting abnormal connections during the degenerative processes in MCI. ",Kein DOI-Link verfügbar,2305.14404v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Generative Adversarial Networks: A Survey Towards Private and Secure   Applications,1970,"  Generative Adversarial Networks (GAN) have promoted a variety of applications in computer vision, natural language processing, etc. due to its generative model's compelling ability to generate realistic examples plausibly drawn from an existing distribution of samples. GAN not only provides impressive performance on data generation-based tasks but also stimulates fertilization for privacy and security oriented research because of its game theoretic optimization strategy. Unfortunately, there are no comprehensive surveys on GAN in privacy and security, which motivates this survey paper to summarize those state-of-the-art works systematically. The existing works are classified into proper categories based on privacy and security functions, and this survey paper conducts a comprehensive analysis of their advantages and drawbacks. Considering that GAN in privacy and security is still at a very initial stage and has imposed unique challenges that are yet to be well addressed, this paper also sheds light on some potential privacy and security applications with GAN and elaborates on some future research directions. ",Kein DOI-Link verfügbar,2106.03785v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Graph Convolution Networks Using Message Passing and Multi-Source   Similarity Features for Predicting circRNA-Disease Association,1970,"  Graphs can be used to effectively represent complex data structures. Learning these irregular data in graphs is challenging and still suffers from shallow learning. Applying deep learning on graphs has recently showed good performance in many applications in social analysis, bioinformatics etc. A message passing graph convolution network is such a powerful method which has expressive power to learn graph structures. Meanwhile, circRNA is a type of non-coding RNA which plays a critical role in human diseases. Identifying the associations between circRNAs and diseases is important to diagnosis and treatment of complex diseases. However, there are limited number of known associations between them and conducting biological experiments to identify new associations is time consuming and expensive. As a result, there is a need of building efficient and feasible computation methods to predict potential circRNA-disease associations. In this paper, we propose a novel graph convolution network framework to learn features from a graph built with multi-source similarity information to predict circRNA-disease associations. First we use multi-source information of circRNA similarity, disease and circRNA Gaussian Interaction Profile (GIP) kernel similarity to extract the features using first graph convolution. Then we predict disease associations for each circRNA with second graph convolution. Proposed framework with five-fold cross validation on various experiments shows promising results in predicting circRNA-disease association and outperforms other existing methods. ",Kein DOI-Link verfügbar,2009.07173v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Autism Spectrum Disorder Classification in Children based on Structural   MRI Features Extracted using Contrastive Variational Autoencoder,1970,"  Autism spectrum disorder (ASD) is a highly disabling mental disease that brings significant impairments of social interaction ability to the patients, making early screening and intervention of ASD critical. With the development of the machine learning and neuroimaging technology, extensive research has been conducted on machine classification of ASD based on structural MRI (s-MRI). However, most studies involve with datasets where participants' age are above 5. Few studies conduct machine classification of ASD for participants below 5-year-old, but, with mediocre predictive accuracy. In this paper, we push the boundary of predictive accuracy (above 0.97) of machine classification of ASD in children (age range: 0.92-4.83 years), based on s-MRI features extracted using contrastive variational autoencoder (CVAE). 78 s-MRI, collected from Shenzhen Children's Hospital, are used for training CVAE, which consists of both ASD-specific feature channel and common shared feature channel. The ASD participants represented by ASD-specific features can be easily discriminated from TC participants represented by the common shared features, leading to high classification accuracy. In case of degraded predictive accuracy when data size is extremely small, a transfer learning strategy is proposed here as a potential solution. Finally, we conduct neuroanatomical interpretation based on the correlation between s-MRI features extracted from CVAE and surface area of different cortical regions, which discloses potential biomarkers that could help target treatments of ASD in the future. ",Kein DOI-Link verfügbar,2307.00976v1,Yes,potent(2)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Reconstruction of Hidden Representation for Robust Feature Extraction,1970,"  This paper aims to develop a new and robust approach to feature representation. Motivated by the success of Auto-Encoders, we first theoretical summarize the general properties of all algorithms that are based on traditional Auto-Encoders: 1) The reconstruction error of the input can not be lower than a lower bound, which can be viewed as a guiding principle for reconstructing the input. Additionally, when the input is corrupted with noises, the reconstruction error of the corrupted input also can not be lower than a lower bound. 2) The reconstruction of a hidden representation achieving its ideal situation is the necessary condition for the reconstruction of the input to reach the ideal state. 3) Minimizing the Frobenius norm of the Jacobian matrix of the hidden representation has a deficiency and may result in a much worse local optimum value. We believe that minimizing the reconstruction error of the hidden representation is more robust than minimizing the Frobenius norm of the Jacobian matrix of the hidden representation. Based on the above analysis, we propose a new model termed Double Denoising Auto-Encoders (DDAEs), which uses corruption and reconstruction on both the input and the hidden representation. We demonstrate that the proposed model is highly flexible and extensible and has a potentially better capability to learn invariant and robust feature representations. We also show that our model is more robust than Denoising Auto-Encoders (DAEs) for dealing with noises or inessential features. Furthermore, we detail how to train DDAEs with two different pre-training methods by optimizing the objective function in a combined and separate manner, respectively. Comparative experiments illustrate that the proposed model is significantly better for representation learning than the state-of-the-art models. ",https://doi.org/10.1145/3284174,1710.02844v2,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Topological States in Dimerized Quantum-Dot Chains Created by Atom   Manipulation,1970,"  Topological electronic phases exist in a variety of naturally occurring materials but can also be created artificially. We used a cryogenic scanning tunneling microscope to create dimerized chains of identical quantum dots on a semiconductor surface and to demonstrate that these chains give rise to one-dimensional topological phases. The dots were assembled from charged adatoms, creating a confining potential with single-atom precision acting on electrons in surface states of the semiconductor. Quantum coupling between the dots leads to electronic states localized at the ends of the chains, as well as at deliberately created internal domain walls, in agreement with the predictions of the Su-Schrieffer-Heeger model. Scanning tunneling spectroscopy also reveals deviations from this well-established model manifested in an asymmetric level spectrum and energy shifts of the boundary states. The deviations arise because the dots are charged and hence lead to an onsite potential that varies along the chain. We show that this variation can be mitigated by electrostatic gating using auxiliary charged adatoms, enabling fine-tuning of the boundary states and control of their quantum superposition. The experimental data, which are complemented by theoretical modeling of the potential and the resulting eigenstates, reveal the important role of electrostatics in these engineered quantum structures. ",https://doi.org/10.1103/PhysRevB.105.125418,2112.00801v1,Yes,potent(3)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Topological boundary states in engineered quantum-dot molecules on the   InAs(111)A surface,1970,"  Atom manipulation by scanning tunneling microscopy was used to construct quantum dots on the InAs(111)A surface. Each dot comprised six ionized indium adatoms. The positively charged adatoms create a confining potential acting on surface-state electrons, leading to the emergence of a bound state associated with the dot. By lining up the dots into N-dot chains with alternating tunnel coupling between them, quantum-dot molecules were constructed that revealed electronic boundary states as predicted by the Su-Schrieffer-Heeger (SSH) model of one-dimensional topological phases. Dot chains with odd N were constructed such that they host a single end or domain-wall state, allowing one to probe the localization of the boundary state on a given sublattice by scanning tunneling spectroscopy. We found probability density also on the forbidden sublattice together with an asymmetric energy spectrum of the chain-confined states. This deviation from the SSH model arises because the dots are charged and create a variation in onsite potential along the chain - which does not remove the boundary states but shifts their energy away from the midgap position. Our results demonstrate that topological boundary states can be created in quantum-dot arrays engineered with atomic-scale precision. ",Kein DOI-Link verfügbar,2406.13347v1,Yes,potent(2)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Ophtha-LLaMA2: A Large Language Model for Ophthalmology,1970,"  In recent years, pre-trained large language models (LLMs) have achieved tremendous success in the field of Natural Language Processing (NLP). Prior studies have primarily focused on general and generic domains, with relatively less research on specialized LLMs in the medical field. The specialization and high accuracy requirements for diagnosis in the medical field, as well as the challenges in collecting large-scale data, have constrained the application and development of LLMs in medical scenarios. In the field of ophthalmology, clinical diagnosis mainly relies on doctors' interpretation of reports and making diagnostic decisions. In order to take advantage of LLMs to provide decision support for doctors, we collected three modalities of ophthalmic report data and fine-tuned the LLaMA2 model, successfully constructing an LLM termed the ""Ophtha-LLaMA2"" specifically tailored for ophthalmic disease diagnosis. Inference test results show that even with a smaller fine-tuning dataset, Ophtha-LLaMA2 performs significantly better in ophthalmic diagnosis compared to other LLMs. It demonstrates that the Ophtha-LLaMA2 exhibits satisfying accuracy and efficiency in ophthalmic disease diagnosis, making it a valuable tool for ophthalmologists to provide improved diagnostic support for patients. This research provides a useful reference for the application of LLMs in the field of ophthalmology, while showcasing the immense potential and prospects in this domain. ",Kein DOI-Link verfügbar,2312.04906v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,High-accuracy numerical simulation of black-hole binaries: Computation   of the gravitational-wave energy flux and comparisons with post-Newtonian   approximants,1970,"  Expressions for the gravitational wave (GW) energy flux and center-of-mass energy of a compact binary are integral building blocks of post-Newtonian (PN) waveforms. In this paper, we compute the GW energy flux and GW frequency derivative from a highly accurate numerical simulation of an equal-mass, non-spinning black hole binary. We also estimate the (derivative of the) center-of-mass energy from the simulation by assuming energy balance. We compare these quantities with the predictions of various PN approximants (adiabatic Taylor and Pade models; non-adiabatic effective-one-body (EOB) models). We find that Pade summation of the energy flux does not accelerate the convergence of the flux series; nevertheless, the Pade flux is markedly closer to the numerical result for the whole range of the simulation (about 30 GW cycles). Taylor and Pade models overestimate the increase in flux and frequency derivative close to merger, whereas EOB models reproduce more faithfully the shape of and are closer to the numerical flux, frequency derivative and derivative of energy. We also compare the GW phase of the numerical simulation with Pade and EOB models. Matching numerical and untuned 3.5 PN order waveforms, we find that the phase difference accumulated until $M \omega = 0.1$ is -0.12 radians for Pade approximants, and 0.50 (0.45) radians for an EOB approximant with Keplerian (non-Keplerian) flux. We fit free parameters within the EOB models to minimize the phase difference, and confirm degeneracies among these parameters. By tuning pseudo 4PN order coefficients in the radial potential or in the flux, or, if present, the location of the pole in the flux, we find that the accumulated phase difference can be reduced - if desired - to much less than the estimated numerical phase error (0.02 radians). ",https://doi.org/10.1103/PhysRevD.78.104020,0804.4184v2,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Toward faithful templates for non-spinning binary black holes using the   effective-one-body approach,1970,"  We present an accurate approximation of the full gravitational radiation waveforms generated in the merger of non-eccentric systems of two non-spinning black holes. Utilizing information from recent numerical relativity simulations and the natural flexibility of the effective-one-body (EOB) model, we extend the latter so that it can successfully match the numerical relativity waveforms during the last stages of inspiral, merger and ringdown. By ``successfully'' here, we mean with phase differences < 8% of a gravitational-wave cycle accumulated by the end of the ringdown phase, maximizing only over time of arrival and initial phase. We obtain this result by simply adding a 4-post-Newtonian order correction in the EOB radial potential and determining the (constant) coefficient by imposing high-matching performances with numerical waveforms of mass ratios m1/m2 = 1, 3/2, 2 and 4, m1 and m2 being the individual black-hole masses. The final black-hole mass and spin predicted by the numerical simulations are used to determine the ringdown frequency and decay time of three quasi-normal-mode damped sinusoids that are attached to the EOB inspiral-(plunge) waveform at the EOB light-ring. The EOB waveforms might be tested and further improved in the future by comparison with extremely long and accurate inspiral numerical-relativity waveforms. They may already be employed for coherent searches and parameter estimation of gravitational waves emitted by non-spinning coalescing binary black holes with ground-based laser-interferometer detectors. ",https://doi.org/10.1103/PhysRevD.76.104049,0706.3732v3,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,"Silicon Layer Intercalation of Centimeter-Scale, Epitaxially-Grown   Monolayer Graphene on Ru(0001)",1970,"  We develop a strategy for graphene growth on Ru(0001) followed by silicon-layer intercalation that not only weakens the interaction of graphene with the metal substrate but also retains its superlative properties. This G/Si/Ru architecture, produced by silicon-layer intercalation approach (SIA), was characterized by scanning tunneling microscopy/spectroscopy and angle resolved electron photoemission spectroscopy. These experiments show high structural and electronic qualities of this new composite. The SIA allows for an atomic control of the distance between the graphene and the metal substrate that can be used as a top gate. Our results show potential for the next generation of graphene-based materials with tailored properties. ",https://doi.org/10.1063/1.3687190,1112.4228v1,Yes,potent(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Instruction-ViT: Multi-Modal Prompts for Instruction Learning in ViT,1970,"  Prompts have been proven to play a crucial role in large language models, and in recent years, vision models have also been using prompts to improve scalability for multiple downstream tasks. In this paper, we focus on adapting prompt design based on instruction tuning into a visual transformer model for image classification which we called Instruction-ViT. The key idea is to implement multi-modal prompts (text or image prompt) related to category information to guide the fine-tuning of the model. Based on the experiments of several image captionining tasks, the performance and domain adaptability were improved. Our work provided an innovative strategy to fuse multi-modal prompts with better performance and faster adaptability for visual classification models. ",Kein DOI-Link verfügbar,2305.00201v1,Yes,innovative(1)
0000-0002-9837-454X,Yi Pan,The University of Manchester,Potential of Multimodal Large Language Models for Data Mining of Medical   Images and Free-text Reports,1970,"  Medical images and radiology reports are crucial for diagnosing medical conditions, highlighting the importance of quantitative analysis for clinical decision-making. However, the diversity and cross-source heterogeneity of these data challenge the generalizability of current data-mining methods. Multimodal large language models (MLLMs) have recently transformed many domains, significantly affecting the medical field. Notably, Gemini-Vision-series (Gemini) and GPT-4-series (GPT-4) models have epitomized a paradigm shift in Artificial General Intelligence (AGI) for computer vision, showcasing their potential in the biomedical domain. In this study, we evaluated the performance of the Gemini, GPT-4, and 4 popular large models for an exhaustive evaluation across 14 medical imaging datasets, including 5 medical imaging categories (dermatology, radiology, dentistry, ophthalmology, and endoscopy), and 3 radiology report datasets. The investigated tasks encompass disease classification, lesion segmentation, anatomical localization, disease diagnosis, report generation, and lesion detection. Our experimental results demonstrated that Gemini-series models excelled in report generation and lesion detection but faces challenges in disease classification and anatomical localization. Conversely, GPT-series models exhibited proficiency in lesion segmentation and anatomical localization but encountered difficulties in disease diagnosis and lesion detection. Additionally, both the Gemini series and GPT series contain models that have demonstrated commendable generation efficiency. While both models hold promise in reducing physician workload, alleviating pressure on limited healthcare resources, and fostering collaboration between clinical practitioners and artificial intelligence technologies, substantial enhancements and comprehensive validations remain imperative before clinical deployment. ",Kein DOI-Link verfügbar,2407.05758v1,Yes,"commendable(1), potent(1)"
0000-0002-6203-7867,Qian Yang,The University of Manchester,Exploring the Path of Transformation and Development for Study Abroad   Consultancy Firms in China,1970,"  In recent years, with the changing landscape of international education and the growing demand from Chinese students, study abroad consultancy firms in China need to adopt transformational development strategies to address challenges and maintain competitiveness. This study investigated the relationships between key performance indicators and several factors through a questionnaire survey of 158 consultancy firms. The factors examined included service diversification, technology adoption, talent management, and regulatory compliance. Descriptive statistical analysis was employed to analyze the data. The results showed that service scope diversification was positively correlated with firm performance. Technology adoption was positively correlated with operational efficiency. Talent management was positively correlated with service quality. Regulatory compliance was positively correlated with firm reputation. Consultancy firms that took progressive approaches in diversifying services, adopting new technologies, cultivating talent, and ensuring compliance demonstrated superior performance, efficiency, quality, and reputation compared to their less innovative counterparts. This research provides empirical evidence to support the transformation of Chinese study abroad consultancy firms. It also highlights the need for future studies to consider causality and contextual variations to gain deeper insights into this issue. ",Kein DOI-Link verfügbar,2404.11034v1,Yes,innovative(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,"Towards Prototyping Driverless Vehicle Behaviors, City Design, and   Policies Simultaneously",1970,"  Autonomous Vehicles (AVs) can potentially improve urban living by reducing accidents, increasing transportation accessibility and equity, and decreasing emissions. Realizing these promises requires the innovations of AV driving behaviors, city plans and infrastructure, and traffic and transportation policies to join forces. However, the complex interdependencies among AV, city, and policy design issues can hinder their innovation. We argue the path towards better AV cities is not a process of matching city designs and policies with AVs' technological innovations, but a process of iterative prototyping of all three simultaneously: Innovations can happen step-wise as the knot of AV, city, and policy design loosens and tightens, unwinds and reties. In this paper, we ask: How can innovators innovate AVs, city environments, and policies simultaneously and productively toward better AV cities? The paper has two parts. First, we map out the interconnections among the many AV, city, and policy design decisions, based on a literature review spanning HCI/HRI, transportation science, urban studies, law and policy, operations research, economy, and philosophy. This map can help innovators identify design constraints and opportunities across the traditional AV/city/policy design disciplinary bounds. Second, we review the respective methods for AV, city, and policy design, and identify key barriers in combining them: (1) Organizational barriers to AV-city-policy design collaboration, (2) computational barriers to multi-granularity AV-city-policy simulation, and (3) different assumptions and goals in joint AV-city-policy optimization. We discuss two broad approaches that can potentially address these challenges, namely, ""low-fidelity integrative City-AV-Policy Simulation (iCAPS)"" and ""participatory design optimization"". ",Kein DOI-Link verfügbar,2304.06639v1,Yes,potent(2)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Exploring the Best Practices of Query Expansion with Large Language   Models,1970,"  Large Language Models (LLMs) are foundational in language technologies, particularly in information retrieval (IR). Previous studies have utilized LLMs for query expansion, achieving notable improvements in IR. In this paper, we thoroughly explore the best practice of leveraging LLMs for query expansion. To this end, we introduce a training-free, straightforward yet effective framework called Multi-Text Generation Integration (\textsc{MuGI}). It leverages LLMs to generate multiple pseudo-references, integrating them with queries to enhance both sparse and dense retrievers. Our empirical findings reveal that: (1) Increasing the number of samples from LLMs benefits IR systems; (2) A balance between the query and pseudo-documents, and an effective integration strategy, is critical for high performance; (3) Contextual information from LLMs is essential, even boost a 23M model to outperform a 7B baseline model; (4) Pseudo relevance feedback can further calibrate queries for improved performance; and (5) Query expansion is widely applicable and versatile, consistently enhancing models ranging from 23M to 7B parameters. Our code and all generated references are made available at \url{https://github.com/lezhang7/Retrieval_MuGI} ",Kein DOI-Link verfügbar,2401.06311v3,Yes,"notable(1), versatile(1)"
0000-0002-6203-7867,Qian Yang,The University of Manchester,Discovery of two broad absorption line quasars at redshift about 4.75   using the Lijiang 2.4m telescope,1970,"  The ultraviolet broad absorption lines have been seen in the spectra of quasars at high redshift, and are generally considered to be caused by outflows with velocities from thousands kilometers per second to one tenth of the speed of light. They provide crucial implications for the cosmological structures and physical evolutions related to the feedback of active galactic nuclei (AGNs). Recently, through a dedicated program of optically spectroscopic identifications of selected quasar candidates at redshift 5 by using the Lijiang 2.4 m telescope, we discovered two luminous broad absorption line quasars (BALQSOs) at redshift about 4.75. One of them may even have the potentially highest absorption Balnicity Index (BI) ever found to date, which is remarkably characterized by its deep, broad absorption lines and sub-relativistic outflows. Further physical properties, including the metal abundances, variabilities, evolutions of the supermassive black holes (SMBH) and accretion disks associated with the feedback process, can be investigated with multi-wavelength follow-up observations in the future. ",https://doi.org/10.1007/s11433-015-5685-4,1511.08278v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Faster On-Device Training Using New Federated Momentum Algorithm,1970,"  Mobile crowdsensing has gained significant attention in recent years and has become a critical paradigm for emerging Internet of Things applications. The sensing devices continuously generate a significant quantity of data, which provide tremendous opportunities to develop innovative intelligent applications. To utilize these data to train machine learning models while not compromising user privacy, federated learning has become a promising solution. However, there is little understanding of whether federated learning algorithms are guaranteed to converge. We reconsider model averaging in federated learning and formulate it as a gradient-based method with biased gradients. This novel perspective assists analysis of its convergence rate and provides a new direction for more acceleration. We prove for the first time that the federated averaging algorithm is guaranteed to converge for non-convex problems, without imposing additional assumptions. We further propose a novel accelerated federated learning algorithm and provide a convergence guarantee. Simulated federated learning experiments are conducted to train deep neural networks on benchmark datasets, and experimental results show that our proposed method converges faster than previous approaches. ",Kein DOI-Link verfügbar,2002.02090v1,Yes,innovative(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Attacking Recommender Systems with Augmented User Profiles,1970,"  Recommendation Systems (RS) have become an essential part of many online services. Due to its pivotal role in guiding customers towards purchasing, there is a natural motivation for unscrupulous parties to spoof RS for profits. In this paper, we study the shilling attack: a subsistent and profitable attack where an adversarial party injects a number of user profiles to promote or demote a target item. Conventional shilling attack models are based on simple heuristics that can be easily detected, or directly adopt adversarial attack methods without a special design for RS. Moreover, the study on the attack impact on deep learning based RS is missing in the literature, making the effects of shilling attack against real RS doubtful. We present a novel Augmented Shilling Attack framework (AUSH) and implement it with the idea of Generative Adversarial Network. AUSH is capable of tailoring attacks against RS according to budget and complex attack goals, such as targeting a specific user group. We experimentally show that the attack impact of AUSH is noticeable on a wide range of RS including both classic and modern deep learning based RS, while it is virtually undetectable by the state-of-the-art attack detection model. ",https://doi.org/10.1145/3340531.3411884,2005.08164v2,Yes,pivotal(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Sentence-level Online Handwritten Chinese Character Recognition,1970,"  Single online handwritten Chinese character recognition~(single OLHCCR) has achieved prominent performance. However, in real application scenarios, users always write multiple Chinese characters to form one complete sentence and the contextual information within these characters holds the significant potential to improve the accuracy, robustness and efficiency of sentence-level OLHCCR. In this work, we first propose a simple and straightforward end-to-end network, namely vanilla compositional network~(VCN) to tackle the sentence-level OLHCCR. It couples convolutional neural network with sequence modeling architecture to exploit the handwritten character's previous contextual information. Although VCN performs much better than the state-of-the-art single OLHCCR model, it exposes high fragility when confronting with not well written characters such as sloppy writing, missing or broken strokes. To improve the robustness of sentence-level OLHCCR, we further propose a novel deep spatial-temporal fusion network~(DSTFN). It utilizes a pre-trained autoregresssive framework as the backbone component, which projects each Chinese character into word embeddings, and integrates the spatial glyph features of handwritten characters and their contextual information multiple times at multi-layer fusion module. We also construct a large-scale sentence-level handwriting dataset, named as CSOHD to evaluate models. Extensive experiment results demonstrate that DSTFN achieves the state-of-the-art performance, which presents strong robustness compared with VCN and exiting single OLHCCR models. The in-depth empirical analysis and case studies indicate that DSTFN can significantly improve the efficiency of handwriting input, with the handwritten Chinese character with incomplete strokes being recognized precisely. ",Kein DOI-Link verfügbar,2108.02561v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Clinical Evidence Engine: Proof-of-Concept For A   Clinical-Domain-Agnostic Decision Support Infrastructure,1970,"  Abstruse learning algorithms and complex datasets increasingly characterize modern clinical decision support systems (CDSS). As a result, clinicians cannot easily or rapidly scrutinize the CDSS recommendation when facing a difficult diagnosis or treatment decision in practice. Over-trust or under-trust are frequent. Prior research has explored supporting such assessments by explaining DST data inputs and algorithmic mechanisms. This paper explores a different approach: Providing precisely relevant, scientific evidence from biomedical literature. We present a proof-of-concept system, Clinical Evidence Engine, to demonstrate the technical and design feasibility of this approach across three domains (cardiovascular diseases, autism, cancer). Leveraging Clinical BioBERT, the system can effectively identify clinical trial reports based on lengthy clinical questions (e.g., ""risks of catheter infection among adult patients in intensive care unit who require arterial catheters, if treated with povidone iodine-alcohol""). This capability enables the system to identify clinical trials relevant to diagnostic/treatment hypotheses -- a clinician's or a CDSS's. Further, Clinical Evidence Engine can identify key parts of a clinical trial abstract, including patient population (e.g., adult patients in intensive care unit who require arterial catheters), intervention (povidone iodine-alcohol), and outcome (risks of catheter infection). This capability opens up the possibility of enabling clinicians to 1) rapidly determine the match between a clinical trial and a clinical question, and 2) understand the result and contexts of the trial without extensive reading. We demonstrate this potential by illustrating two example use scenarios of the system. We discuss the idea of designing DST explanations not as specific to a DST or an algorithm, but as a domain-agnostic decision support infrastructure. ",Kein DOI-Link verfügbar,2111.00621v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Optical Variability of the Dwarf AGN NGC 4395 from the Transiting   Exoplanet Survey Satellite,1970,"  We present optical light curves from the Transiting Exoplanet Survey Satellite (TESS) for the archetypical dwarf active galactic nucleus (AGN) in the nearby galaxy NGC 4395 hosting a $\sim 10^5\,M_\odot$ supermassive black hole (SMBH). Significant variability is detected on timescales from weeks to hours before reaching the background noise level. The $\sim$month-long, 30 minute-cadence, high-precision TESS light curve can be well fit by a simple damped random walk (DRW) model, with the damping timescale $\tau_{\rm DRW}$ constrained to be $2.3_{-0.7}^{+1.8}$~days ($1\sigma$). NGC 4395 lies almost exactly on the extrapolation of the $\tau_{\rm DRW}-M_{\rm BH}$ relation measured for AGNs with BH masses that are more than three orders of magnitude larger. The optical variability periodogram can be well fit by a broken power law with the high-frequency slope ($-1.88\pm0.15$) and the characteristic timescale ($\tau_{\rm br}\equiv 1/(2\pi f_{\rm br})=1.4_{-0.5}^{+1.9}\,$days) consistent with the DRW model within 1$\sigma$. This work demonstrates the power of TESS light curves in identifying low-mass accreting SMBHs with optical variability, and a potential global $\tau_{\rm DRW}-M_{\rm BH}$ relation that can be used to estimate SMBH masses with optical variability measurements. ",https://doi.org/10.3847/1538-4357/aba3ce,2005.04491v2,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Diffusion-controlled Alloying of Single-phase Multi-principal Covalent   Transition Metal Carbides with Enhanced Damage tolerance and Exceptional   Thermal Properties,1970,"  Multicomponent alloying has displayed extraordinary potential for producing exceptional structural and functional materials. However, the synthesis of single-phase, multi-principal covalent compounds remains a challenge. Here we present a diffusion-controlled alloying strategy for the successful realization of covalent multi-principal transition metal carbides (MPTMCs) with a single face-centered cubic (FCC) phase. The increased interfacial diffusion promoted by the addition of a nonstoichiometric compound leads to rapid formation of the new single phase at much lower sintering temperature. Direct atomic-level observations via scanning transmission electron microscopy demonstrate that MPTMCs are composed of a single phase with a random distribution of all cations, which holds the key to the unique combinations of improved fracture toughness, superior Vickers hardness, and extremely lower thermal diffusivity achieved in MPTMCs. The present discovery provides a promising approach toward the design and synthesis of next-generation high-performance materials. ",Kein DOI-Link verfügbar,1810.01944v1,Yes,potent(1)
0000-0002-6203-7867,Qian Yang,The University of Manchester,Quasar Photometric Redshifts and Candidate Selection: A New Algorithm   Based on Optical and Mid-Infrared Photometric Data,1970,"  We present a new algorithm to estimate quasar photometric redshifts (photo-$z$s), by considering the asymmetries in the relative flux distributions of quasars. The relative flux models are built with multivariate Skew-t distributions in the multi-dimensional space of relative fluxes as a function of redshift and magnitude. For 151,392 quasars in the SDSS, we achieve a photo-$z$ accuracy, defined as the fraction of quasars with the difference between the photo-$z$ $z_p$ and the spectroscopic redshift $z_s$, $|\Delta z| = |z_s-z_p|/(1+z_s)$ within 0.1, of 74%. Combining the WISE W1 and W2 infrared data with the SDSS data, the photo-$z$ accuracy is enhanced to 87%. Using the Pan-STARRS1 or DECaLS photometry with WISE W1 and W2 data, the photo-$z$ accuracies are 79% and 72%, respectively. The prior probabilities as a function of magnitude for quasars, stars and galaxies are calculated respectively based on (1) the quasar luminosity function; (2) the Milky Way synthetic simulation with the Besan\c{c}on model; (3) the Bayesian Galaxy Photometric Redshift estimation. The relative fluxes of stars are obtained with the Padova isochrones, and the relative fluxes of galaxies are modeled through galaxy templates. We test our classification method to select quasars using the DECaLS $g$, $r$, $z$, and WISE W1 and W2 photometry. The quasar selection completeness is higher than 70% for a wide redshift range $0.5<z<4.5$, and a wide magnitude range $18<r<21.5$ mag. Our photo-$z$ regression and classification method has the potential to extend to future surveys. The photo-$z$ code will be publicly available. ",https://doi.org/10.3847/1538-3881/aa943c,1710.09155v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Membership Inference Attacks and Defenses in Neural Network Pruning,1970,"  Neural network pruning has been an essential technique to reduce the computation and memory requirements for using deep neural networks for resource-constrained devices. Most existing research focuses primarily on balancing the sparsity and accuracy of a pruned neural network by strategically removing insignificant parameters and retraining the pruned model. Such efforts on reusing training samples pose serious privacy risks due to increased memorization, which, however, has not been investigated yet.   In this paper, we conduct the first analysis of privacy risks in neural network pruning. Specifically, we investigate the impacts of neural network pruning on training data privacy, i.e., membership inference attacks. We first explore the impact of neural network pruning on prediction divergence, where the pruning process disproportionately affects the pruned model's behavior for members and non-members. Meanwhile, the influence of divergence even varies among different classes in a fine-grained manner. Enlighten by such divergence, we proposed a self-attention membership inference attack against the pruned neural networks. Extensive experiments are conducted to rigorously evaluate the privacy impacts of different pruning approaches, sparsity levels, and adversary knowledge. The proposed attack shows the higher attack performance on the pruned models when compared with eight existing membership inference attacks. In addition, we propose a new defense mechanism to protect the pruning process by mitigating the prediction divergence based on KL-divergence distance, whose effectiveness has been experimentally demonstrated to effectively mitigate the privacy risks while maintaining the sparsity and accuracy of the pruned models. ",Kein DOI-Link verfügbar,2202.03335v2,Yes,strategically(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,A Comparison Study of Coupled and Decoupled Uplink Heterogeneous   Cellular Networks,1970,"  The evolution of mobile cellular networks has brought great changes of network architecture. For example, heterogeneous cellular network (HetNet) and Ultra dense network (UDN) have been proposed as promising techniques for 5G systems. Dense deployment of base stations (BSs) allows a mobile user to be able to access multiple BSs. Meanwhile the unbalance between UL and DL in HetNets, such as different received SINR threshold and traffic load, etc., becomes increasingly obvious. All these factors naturally inspire us to consider decoupling of uplink and downlink in radio access network. An interesting question is that whether the decoupled uplink (UL) /downlink (DL) access (DUDA) mode outperforms traditional coupled uplink (UL)/downlink (DL) access (CUDA) mode or not, and how big is the performance difference in terms of system rate, spectrum efficiency (SE) and energy efficiency (EE), etc. in HetNets. In this paper, we aim at thoroughly comparing the performance of the two modes based on stochastic geometry theory. In our analytical model, we take into account dynamic transmit power control in UL communication. Specifically, we employ fractional power control (FPC) to model a location-dependent channel state. Numerical results reveals that DUDA mode significantly outperforms CUDA mode in system rate, SE and EE in HetNets. In addition, DUDA mode improves load balance and potential fairness for both different type BSs and associated UEs. ",Kein DOI-Link verfügbar,1502.01887v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Federated Semi-Supervised Domain Adaptation via Knowledge Transfer,1970,"  Given the rapidly changing machine learning environments and expensive data labeling, semi-supervised domain adaptation (SSDA) is imperative when the labeled data from the source domain is statistically different from the partially labeled data from the target domain. Most prior SSDA research is centrally performed, requiring access to both source and target data. However, data in many fields nowadays is generated by distributed end devices. Due to privacy concerns, the data might be locally stored and cannot be shared, resulting in the ineffectiveness of existing SSDA research. This paper proposes an innovative approach to achieve SSDA over multiple distributed and confidential datasets, named by Federated Semi-Supervised Domain Adaptation (FSSDA). FSSDA integrates SSDA with federated learning based on strategically designed knowledge distillation techniques, whose efficiency is improved by performing source and target training in parallel. Moreover, FSSDA controls the amount of knowledge transferred across domains by properly selecting a key parameter, i.e., the imitation parameter. Further, the proposed FSSDA can be effectively generalized to multi-source domain adaptation scenarios. Extensive experiments are conducted to demonstrate the effectiveness and efficiency of FSSDA design. ",Kein DOI-Link verfügbar,2207.10727v2,Yes,"innovative(1), strategically(1)"
0009-0007-4678-0419,Lan Zhang,The University of Manchester,FedZKT: Zero-Shot Knowledge Transfer towards Resource-Constrained   Federated Learning with Heterogeneous On-Device Models,1970,"  Federated learning enables multiple distributed devices to collaboratively learn a shared prediction model without centralizing their on-device data. Most of the current algorithms require comparable individual efforts for local training with the same structure and size of on-device models, which, however, impedes participation from resource-constrained devices. Given the widespread yet heterogeneous devices nowadays, in this paper, we propose an innovative federated learning framework with heterogeneous on-device models through Zero-shot Knowledge Transfer, named by FedZKT. Specifically, FedZKT allows devices to independently determine the on-device models upon their local resources. To achieve knowledge transfer across these heterogeneous on-device models, a zero-shot distillation approach is designed without any prerequisites for private on-device data, which is contrary to certain prior research based on a public dataset or a pre-trained data generator. Moreover, this compute-intensive distillation task is assigned to the server to allow the participation of resource-constrained devices, where a generator is adversarially learned with the ensemble of collected on-device models. The distilled central knowledge is then sent back in the form of the corresponding on-device model parameters, which can be easily absorbed on the device side. Extensive experimental studies demonstrate the effectiveness and robustness of FedZKT towards on-device knowledge agnostic, on-device model heterogeneity, and other challenging federated learning scenarios, such as heterogeneous on-device data and straggler effects. ",Kein DOI-Link verfügbar,2109.03775v2,Yes,innovative(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Towards Robust On-Ramp Merging via Augmented Multimodal Reinforcement   Learning,1970,"  Despite the success of AI-enabled onboard perception, on-ramp merging has been one of the main challenges for autonomous driving. Due to limited sensing range of onboard sensors, a merging vehicle can hardly observe main road conditions and merge properly. By leveraging the wireless communications between connected and automated vehicles (CAVs), a merging CAV has potential to proactively obtain the intentions of nearby vehicles. However, CAVs can be prone to inaccurate observations, such as the noisy basic safety messages (BSM) and poor quality surveillance images. In this paper, we present a novel approach for Robust on-ramp merge of CAVs via Augmented and Multi-modal Reinforcement Learning, named by RAMRL. Specifically, we formulate the on-ramp merging problem as a Markov decision process (MDP) by taking driving safety, comfort driving behavior, and traffic efficiency into account. To provide reliable merging maneuvers, we simultaneously leverage BSM and surveillance images for multi-modal observation, which is used to learn a policy model through proximal policy optimization (PPO). Moreover, to improve data efficiency and provide better generalization performance, we train the policy model with augmented data (e.g., noisy BSM and noisy surveillance images). Extensive experiments are conducted with Simulation of Urban MObility (SUMO) platform under two typical merging scenarios. Experimental results demonstrate the effectiveness and efficiency of our robust on-ramp merging design. ",Kein DOI-Link verfügbar,2208.07307v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,CTS: A Consistency-Based Medical Image Segmentation Model,1970,"  In medical image segmentation tasks, diffusion models have shown significant potential. However, mainstream diffusion models suffer from drawbacks such as multiple sampling times and slow prediction results. Recently, consistency models, as a standalone generative network, have resolved this issue. Compared to diffusion models, consistency models can reduce the sampling times to once, not only achieving similar generative effects but also significantly speeding up training and prediction. However, they are not suitable for image segmentation tasks, and their application in the medical imaging field has not yet been explored. Therefore, this paper applies the consistency model to medical image segmentation tasks, designing multi-scale feature signal supervision modes and loss function guidance to achieve model convergence. Experiments have verified that the CTS model can obtain better medical image segmentation results with a single sampling during the test phase. ",Kein DOI-Link verfügbar,2405.09056v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Residue-based Label Protection Mechanisms in Vertical Logistic   Regression,1970,"  Federated learning (FL) enables distributed participants to collaboratively learn a global model without revealing their private data to each other. Recently, vertical FL, where the participants hold the same set of samples but with different features, has received increased attention. This paper first presents one label inference attack method to investigate the potential privacy leakages of the vertical logistic regression model. Specifically, we discover that the attacker can utilize the residue variables, which are calculated by solving the system of linear equations constructed by local dataset and the received decrypted gradients, to infer the privately owned labels. To deal with this, we then propose three protection mechanisms, e.g., additive noise mechanism, multiplicative noise mechanism, and hybrid mechanism which leverages local differential privacy and homomorphic encryption techniques, to prevent the attack and improve the robustness of the vertical logistic regression. model. Experimental results show that both the additive noise mechanism and the multiplicative noise mechanism can achieve efficient label protection with only a slight drop in model testing accuracy, furthermore, the hybrid mechanism can achieve label protection without any testing accuracy degradation, which demonstrates the effectiveness and efficiency of our protection techniques ",Kein DOI-Link verfügbar,2205.04166v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Double-I Watermark: Protecting Model Copyright for LLM Fine-tuning,1970,"  To support various applications, a prevalent and efficient approach for business owners is leveraging their valuable datasets to fine-tune a pre-trained LLM through the API provided by LLM owners or cloud servers. However, this process carries a substantial risk of model misuse, potentially resulting in severe economic consequences for business owners. Thus, safeguarding the copyright of these customized models during LLM fine-tuning has become an urgent practical requirement, but there are limited existing solutions to provide such protection. To tackle this pressing issue, we propose a novel watermarking approach named ``Double-I watermark''. Specifically, based on the instruct-tuning data, two types of backdoor data paradigms are introduced with trigger in the instruction and the input, respectively. By leveraging LLM's learning capability to incorporate customized backdoor samples into the dataset, the proposed approach effectively injects specific watermarking information into the customized model during fine-tuning, which makes it easy to inject and verify watermarks in commercial scenarios. We evaluate the proposed ""Double-I watermark"" under various fine-tuning methods, demonstrating its harmlessness, robustness, uniqueness, imperceptibility, and validity through both quantitative and qualitative analyses. ",Kein DOI-Link verfügbar,2402.14883v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,BadFusion: 2D-Oriented Backdoor Attacks against 3D Object Detection,1970,"  3D object detection plays an important role in autonomous driving; however, its vulnerability to backdoor attacks has become evident. By injecting ''triggers'' to poison the training dataset, backdoor attacks manipulate the detector's prediction for inputs containing these triggers. Existing backdoor attacks against 3D object detection primarily poison 3D LiDAR signals, where large-sized 3D triggers are injected to ensure their visibility within the sparse 3D space, rendering them easy to detect and impractical in real-world scenarios.   In this paper, we delve into the robustness of 3D object detection, exploring a new backdoor attack surface through 2D cameras. Given the prevalent adoption of camera and LiDAR signal fusion for high-fidelity 3D perception, we investigate the latent potential of camera signals to disrupt the process. Although the dense nature of camera signals enables the use of nearly imperceptible small-sized triggers to mislead 2D object detection, realizing 2D-oriented backdoor attacks against 3D object detection is non-trivial. The primary challenge emerges from the fusion process that transforms camera signals into a 3D space, compromising the association with the 2D trigger to the target output. To tackle this issue, we propose an innovative 2D-oriented backdoor attack against LiDAR-camera fusion methods for 3D object detection, named BadFusion, for preserving trigger effectiveness throughout the entire fusion process. The evaluation demonstrates the effectiveness of BadFusion, achieving a significantly higher attack success rate compared to existing 2D-oriented attacks. ",Kein DOI-Link verfügbar,2405.03884v1,Yes,"innovative(1), potent(1)"
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Hidden Anderson Localization in Disorder-Free Ising-Kondo Lattice,1970,"  Anderson localization (AL) phenomena usually exists in systems with random potential. However, disorder-free quantum many-body systems with local conservation can also exhibit AL or even many-body localization transition. In this work, we show that the AL phase exists in a modified Kondo lattice without external random potential. The density of state, inverse participation ratio and temperature-dependent resistance are computed by classical Monte Carlo simulation, which uncovers the AL phase from previously studied Fermi liquid and Mott insulator regime. The occurrence of AL roots from quenched disorder formed by conservative localized moments. Interestingly, a many-body wavefunction is found, which captures elements in all three paramagnetic phases and is used to compute their quantum entanglement. In light of these findings, we expect the disorder-free AL phenomena can exit in generic translation-invariant quantum many-body systems. ",https://doi.org/10.1088/1674-1056/ab99b0,1907.13507v3,Yes,potent(2)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,S-RAN: Semantic-Aware Radio Access Networks,1970,"  Semantic communication (SemCom) has been a transformative paradigm, emphasizing the precise exchange of meaningful information over traditional bit-level transmissions. However, existing SemCom research, primarily centered on simplified scenarios like single-pair transmissions with direct wireless links, faces significant challenges when applied to real-world radio access networks (RANs). This article introduces a Semantic-aware Radio Access Network (S-RAN), offering a holistic systematic view of SemCom beyond single-pair transmissions. We begin by outlining the S-RAN architecture, introducing new physical components and logical functions along with key design challenges. We then present transceiver design for end-to-end transmission to overcome conventional SemCom transceiver limitations, including static channel conditions, oversimplified background knowledge models, and hardware constraints. Later, we delve into the discussion on radio resource management for multiple users, covering semantic channel modeling, performance metrics, resource management algorithms, and a case study, to elaborate distinctions from resource management for legacy RANs. Finally, we highlight open research challenges and potential solutions. The objective of this article is to serve as a basis for advancing SemCom research into practical wireless systems. ",Kein DOI-Link verfügbar,2407.11161v1,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Coexistence of antiferromagnetism and superconductivity of   t-t$^\prime$-J model on honeycomb lattice,1970,"  Motivated by recent experimental study of antiferromagnetic property of honeycomb compound In$_{3}$Cu$_{2}$VO$_{9}$ [Yan \textit{et al.}, PRB \textbf{85}, 085102 (2012)], we explore possible superconductivity and its coexistence with antiferromagnetism. We use the t-t$^\prime$-J model on the honeycomb lattice as our starting point and employ the slave-boson mean-field theory. In the antiferromagnetic normal state, the characteristic doping evolution of Fermi surface shows that only one effective singe band is active, which suggests that the potential pairing symmetry is the time-reversal symmetry breaking $d+id$, rather than the extended $s$-wave. It is found that this superconducting state coexists with the antiferromagnetism in a broad doping regime, which is consistent with the numerical calculations. The local density of states and its thermodynamic property of the superconducting state has been studied in detail with an effective single-band picture for understanding other physical observable such as superfluid density. The present work may be useful in experimentally exploring possible superconductivity of this kind of materials on the honeycomb lattice and contributes to the understanding of the unconventional superconductivity on general two-dimensional correlated electron systems. ",https://doi.org/10.1016/j.physb.2015.01.010,1404.1795v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,Wireless Resource Optimization in Hybrid Semantic/Bit Communication   Networks,1970,"  Recently, semantic communication (SemCom) has shown great potential in significant resource savings and efficient information exchanges, thus naturally introducing a novel and practical cellular network paradigm where two modes of SemCom and conventional bit communication (BitCom) coexist. Nevertheless, the involved wireless resource management becomes rather complicated and challenging, given the unique background knowledge matching and time-consuming semantic coding requirements in SemCom. To this end, this paper jointly investigates user association (UA), mode selection (MS), and bandwidth allocation (BA) problems in a hybrid semantic/bit communication network (HSB-Net). Concretely, we first identify a unified performance metric of message throughput for both SemCom and BitCom links. Next, we specially develop a knowledge matching-aware two-stage tandem packet queuing model and theoretically derive the average packet loss ratio and queuing latency. Combined with practical constraints, we then formulate a joint optimization problem for UA, MS, and BA to maximize the overall message throughput of HSB-Net. Afterward, we propose an optimal resource management strategy by utilizing a Lagrange primal-dual transformation method and a preference list-based heuristic algorithm with polynomial-time complexity. Numerical results not only demonstrate the accuracy of our analytical queuing model, but also validate the performance superiority of our proposed strategy compared with different benchmarks. ",Kein DOI-Link verfügbar,2404.04162v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,The Gravitational Potential Near the Sun From SEGUE K-dwarf Kinematics,1970,"  To constrain the Galactic gravitational potential near the Sun ($\sim$1.5 kpc), we derive and model the spatial and velocity distribution for a sample of 9000 K-dwarfs that have spectra from SDSS/SEGUE, which yield radial velocities and abundances ([Fe/H] & [$\alpha$/Fe]). We first derive the spatial density distribution for stars of three abundance-selected sub-populations by accounting for the survey's selection function. The vertical profile of these sub-populations are simple exponentials and their vertical dispersion profile is nearly isothermal. To model these data, we apply the `vertical' Jeans Equation, which relates the observable tracer number density and vertical velocity dispersion to the gravitational potential or vertical force. We explore a number of functional forms for the vertical force law, and fit the dispersion and density profiles of all abundance selected sub-populations simultaneously in the same potential, and explore all parameter co-variances using MCMC. Our fits constrain a disk {\it mass} scale height $\lesssim$ 300 pc and the total surface mass density to be $67 \pm 6 M_{\odot} {\rm pc^{-2}}$ at $|z| = 1.0$ kpc of which the contribution from all stars is $42 \pm 5 M_{\odot} {\rm pc^{-2}}$ (presuming a contribution from cold gas of $13 M_{\odot} {\rm pc^{-2}}$). We find significant constraints on the local dark matter density of $0.0065\pm0.0023 M_{\odot} {\rm pc^{-3}}$ ($0.25\pm0.09 {\rm GeV cm^{-3}} $). Together with recent experiments this firms up the best estimate of $0.0075\pm0.0021 M_{\odot} {\rm pc^{-3}}$ ($0.28\pm0.08 {\rm GeV cm^{-3}} $), consistent with global fits of approximately round dark matter halos to kinematic data in the outskirts of the Galaxy. ",https://doi.org/10.1088/0004-637X/772/2/108,1209.0256v2,Yes,potent(3)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,InFi: End-to-End Learning to Filter Input for Resource-Efficiency in   Mobile-Centric Inference,1970,"  Mobile-centric AI applications have high requirements for resource-efficiency of model inference. Input filtering is a promising approach to eliminate the redundancy so as to reduce the cost of inference. Previous efforts have tailored effective solutions for many applications, but left two essential questions unanswered: (1) theoretical filterability of an inference workload to guide the application of input filtering techniques, thereby avoiding the trial-and-error cost for resource-constrained mobile applications; (2) robust discriminability of feature embedding to allow input filtering to be widely effective for diverse inference tasks and input content. To answer them, we first formalize the input filtering problem and theoretically compare the hypothesis complexity of inference models and input filters to understand the optimization potential. Then we propose the first end-to-end learnable input filtering framework that covers most state-of-the-art methods and surpasses them in feature embedding with robust discriminability. We design and implement InFi that supports six input modalities and multiple mobile-centric deployments. Comprehensive evaluations confirm our theoretical results and show that InFi outperforms strong baselines in applicability, accuracy, and efficiency. InFi achieve 8.5x throughput and save 95% bandwidth, while keeping over 90% accuracy, for a video analytics application on mobile platforms. ",Kein DOI-Link verfügbar,2209.13873v3,Yes,potent(1)
0009-0007-4678-0419,Lan Zhang,The University of Manchester,"The Origins of Young Stars in the Direction of the Leading Arm of the   Magellanic Stream: Abundances, Kinematics, and Orbits",1970,"  We explore the origins of the young B-type stars found by Casetti-Dinescu et al.(2014) at the outskirts of the Milky-Way disk in the sky region of Leading Arm of the Magellanic Stream. High-resolution spectroscopic observations made with the MIKE instrument on the Magellan Clay 6.5m telescope for nine stars are added to the previous sample analyzed by Zhang et al. (2017). We compile a sample of fifteen young stars with well-determined stellar types, ages, abundances and kinematics. With proper motions from Gaia DR2 we also derive orbits in a realistic Milky-Way potential. We find that our previous radial-velocity selected LA candidates have substantial orbital angular momentum. The substantial amount of rotational component for these stars is in contrast with the near-polar Magellanic orbit, thus rendering these stars unlikely members of the LA. There are four large orbital-energy stars in our sample. The highest orbital-energy one has an age shorter than the time to disk crossing, with a birthplace $z=2.5$~kpc and $R_{\rm GC}\sim 28$~kpc. Therefore, the origin of this star is uncertain. The remaining three stars have disk runaway origin with birthplaces between 12 and 25 kpc from the Galactic center. Also, the most energetic stars are more metal poor ([Mg/H] =$-0.50\pm0.07$) and with larger He scatter ($\sigma_{\rm [He/H]} = 0.72$) than the inner disk ones ([Mg/H] $=0.12\pm0.36$, $\sigma_{\rm [He/H]} = 0.15$). While the former group's abundance is compatible with that of the Large Magellanic Cloud, it could also reflect the metallicity gradient of the MW disk and their runaway status via different runaway mechanisms. ",https://doi.org/10.3847/1538-4357/aaf560,1812.00198v1,Yes,potent(1)
0000-0001-9633-9313,Alex Levine,The University of Manchester,Equations in virtually class 2 nilpotent groups,1970,"  We give an algorithm that decides whether a single equation in a group that is virtually a class $2$ nilpotent group with a virtually cyclic commutator subgroup, such as the Heisenberg group, admits a solution. This generalises the work of Duchin, Liang and Shapiro to finite extensions. ",https://doi.org/10.46298/jgcc.2022.14.1.9776,2009.10651v7,Yes,potent(1)
0000-0001-9633-9313,Alex Levine,The University of Manchester,"Quadratic Diophantine equations, the Heisenberg group and formal   languages",1970,"  We express the solutions to quadratic equations with two variables in the ring of integers using EDT0L languages. We use this to show that EDT0L languages can be used to describe the solutions to one-variable equations in the Heisenberg group. This is done by reducing the question of solving a one-variable equation in the Heisenberg group to solving an equation in the ring of integers, exploiting the strong link between the ring of integers and nilpotent groups. ",Kein DOI-Link verfügbar,2203.04849v2,Yes,potent(1)
0000-0001-9633-9313,Alex Levine,The University of Manchester,E-disjunctive inverse semigroups,1970,  In this paper we provide an overview of the class of inverse semigroups $S$ such that every congruence on $S$ relates at least one idempotent to a non-idempotent; such inverse semigroups are called $E$-disjunctive. This overview includes the study of the inverse semigroup theoretic structure of $E$-disjunctive semigroups; a large number of natural examples; some asymptotic results establishing the rarity of such inverse semigroups; and a general structure theorem for all inverse semigroups where the building blocks are $E$-disjunctive. ,Kein DOI-Link verfügbar,2405.19825v1,Yes,potent(2)
0000-0001-9633-9313,Alex Levine,The University of Manchester,Post's correspondence problem for hyperbolic and virtually nilpotent   groups,1970,"  Post's Correspondence Problem (the PCP) is a classical decision problem in theoretical computer science that asks whether for pairs of free monoid morphisms $g, h\colon\Sigma^*\to\Delta^*$ there exists any non-trivial $x\in\Sigma^*$ such that $g(x)=h(x)$.   Post's Correspondence Problem for a group $\Gamma$ takes pairs of group homomorphisms $g, h\colon F(\Sigma)\to \Gamma$ instead, and similarly asks whether there exists an $x$ such that $g(x)=h(x)$ holds for non-elementary reasons. The restrictions imposed on $x$ in order to get non-elementary solutions lead to several interpretations of the problem; we consider the natural restriction asking that $x \notin \ker(g) \cap \ker(h)$ and prove that the resulting interpretation of the PCP is undecidable for arbitrary hyperbolic $\Gamma$, but decidable when $\Gamma$ is virtually nilpotent. We also study this problem for group constructions such as subgroups, direct products and finite extensions. This problem is equivalent to an interpretation due to Myasnikov, Nikolev and Ushakov when one map is injective. ",https://doi.org/10.1112/blms.12921,2211.12158v2,Yes,potent(1)
0000-0003-3961-2023,Richard Brown,The University of Manchester,Tracking areas with increased likelihood of surface particle aggregation   in the Gulf of Finland: A first look at persistent Lagrangian Coherent   Structures (LCS),1970,"  We explore the possibility to identify areas of intense patch formation from floating items due to systematic convergence of surface velocity fields by means of a visual comparison of Lagrangian Coherent Structures (LCS) and estimates of areas prone to patch formation using the concept of Finite-Time Compressibility (FTC, a generalisation of the notion of time series of divergence). The LCSs are evaluated using the Finite Time Lyapunov Exponent (FTLE) method. The test area is the Gulf of Finland (GoF) in the Baltic Sea. A basin-wide spatial average of backward FTLE is calculated for the GoF for the first time. This measure of the mixing strength displays a clear seasonal pattern. The evaluated backward FTLE features are linked with potential patch formation regions with high FTC levels. It is shown that areas hosting frequent upwelling or downwelling have consistently stronger than average mixing intensity. The combination of both methods, FTC and LCS, has the potential of being a powerful tool to identify the formation of patches of pollution at the sea surface. ",https://doi.org/10.1016/j.jmarsys.2021.103514,2101.09358v1,Yes,potent(2)
0000-0002-5822-5435,Kaled Alshmrany,The University of Manchester,Finding Security Vulnerabilities in Network Protocol Implementations,1970,"  Implementations of network protocols are often prone to vulnerabilities caused by developers' mistakes when accessing memory regions and dealing with arithmetic operations. Finding practical approaches for checking the security of network protocol implementations has proven to be a challenging problem. The main reason is that the protocol software state-space is too large to be explored. Here we propose a novel verification approach that combines fuzzing with symbolic execution to verify intricate properties in network protocol implementations. We use fuzzing for an initial exploration of the network protocol, while symbolic execution explores both the program paths and protocol states, which were uncovered by fuzzing. From this combination, we automatically generate high-coverage test input packets for a network protocol implementation. We surveyed various approaches based on fuzzing and symbolic execution to understand how these techniques can be effectively combined and then choose a suitable tool to develop further our model on top of it. In our preliminary evaluation, we used ESBMC, Map2Check, and KLEE as software verifiers and SPIKE as fuzzer to check their suitability to verify our network protocol implementations. Our experimental results show that ESBMC can be further developed within our verification framework called \textit{FuSeBMC}, to efficiently and effectively detect intricate security vulnerabilities in network protocol implementations. ",Kein DOI-Link verfügbar,2001.09592v1,Yes,intricate(2)
0000-0003-3206-7851,James Williams,King's College London,Normal Subgroups of Powerful $p$ -groups,1970,"  In this note we show that if $p$ is an odd prime and $G$ is a powerful $p$-group with $N\leq G^{p}$ and $N$ normal in $G$, then $N$ is powerfully nilpotent. An analogous result is proved for $p=2$ when $N\leq G^{4}$. ",Kein DOI-Link verfügbar,1908.07030v1,Yes,potent(1)
0000-0003-3206-7851,James Williams,King's College London,Omegas of Agemos in Powerful Groups,1970,"  In this note we show that for any powerful $p$-group $G$, the subgroup $\Omega_{i}(G^{p^{j}})$ is powerfully nilpotent for all $i,j\geq1$ when $p$ is an odd prime, and $i\geq1$, $j\geq2$ when $p=2$. We provide an example to show why this modification is needed in the case $p=2$. Furthermore we obtain a bound on the powerful nilpotency class of $\Omega_{i}(G^{p^{j}})$. We give an example to show that powerfully nilpotent characteristic subgroups of powerful $p$-groups need not be strongly powerful. ",https://doi.org/10.22108/IJGT.2019.113217.1507,1811.00977v2,Yes,potent(2)
0000-0003-3206-7851,James Williams,King's College London,On finite $p$-groups with powerful subgroups,1970,"  In this paper we investigate the structure of finite $p$-groups with the property that every subgroup of index $p^i$ is powerful for some $i$. For odd primes $p$, we show that under certain conditions these groups must be potent. Then, motivated by a question of Mann, we investigate in detail the case when all maximal subgroups are powerful. We show that for odd $p$ any finite $p$-group $G$ with all maximal subgroups powerful has a regular power structure - with precisely one exceptional case which is a $3$-group of maximal class and order $81$. To show this counterexample is unique we use a computational approach. We briefly discuss the case $p=2$ and some generalisations. ",Kein DOI-Link verfügbar,2101.05720v1,Yes,potent(1)
0000-0003-3206-7851,James Williams,King's College London,Powerfully nilpotent groups of rank 2 or small order,1970,  In this paper we continue the study of powerfully nilpotent groups. These are powerful $p$-groups possessing a central series of a special kind. To each such group one can attach a powerful nilpotency class that leads naturally to the notion of a powerful coclass and classification in terms of an ancestry tree. In this paper we will give a full classification of powerfully nilpotent groups of rank $2$. The classification will then be used to arrive at a precise formula for the number of powerfully nilpotent groups of rank $2$ and order $p^{n}$. We will also give a detailed analysis of the ancestry tree for these groups. The second part of the paper is then devoted to a full classification of powerfully nilpotent groups of order up to $p^{6}$. ,Kein DOI-Link verfügbar,2002.02694v1,Yes,potent(4)
0000-0003-3206-7851,James Williams,King's College London,Powerfully nilpotent groups,1970,"  We introduce a special class of powerful $p$-groups that we call powerfully nilpotent groups that are finite $p$-groups that possess a central series of a special kind. To these we can attach the notion of a powerful nilpotence class that leads naturally to a classification in terms of an `ancestry tree' and powerful coclass. We show that there are finitely many powerfully nilpotent $p$-groups of each given powerful coclass and develop some general theory for this class of groups. We also determine the growth of powerfully nilpotent groups of exponent $p^{2}$ and order $p^{n}$ where $p$ is odd. The number of these is $f(n)=p^{\alpha n^{3}+o(n^{3})}$ where $\alpha=\frac{9+4\sqrt{2}}{394}$. For the larger class of all powerful groups of exponent $p^{2}$ and order $p^{n}$, where $p$ is odd, the number is $p^{\frac{2}{27}n^{3}+o(n^{3})}$. Thus here the class of powerfully nilpotent $p$-groups is large while sparse within the larger class of powerful $p$-groups. ",Kein DOI-Link verfügbar,1811.00962v1,Yes,potent(4)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Implicit semantic-based personalized micro-videos recommendation,1970,"  With the rapid development of mobile Internet and big data, a huge amount of data is generated in the network, but the data that users are really interested in a very small portion. To extract the information that users are interested in from the huge amount of data, the information overload problem needs to be solved. In the era of mobile internet, the user's characteristics and other information should be combined in the massive amount of data to quickly and accurately recommend content to the user, as far as possible to meet the user's personalized needs. Therefore, there is an urgent need to realize high-speed and effective retrieval in tens of thousands of micro-videos. Video data content contains complex meanings, and there are intrinsic connections between video data. For multimodal information, subspace coding learning is introduced to build a coding network from public potential representations to multimodal feature information, taking into account the consistency and complementarity of information under each modality to obtain a public representation of the complete eigenvalue. An end-to-end reordering model based on deep learning and attention mechanism, called interest-related product similarity model based on multimodal data, is proposed for providing top-N recommendations. The multimodal feature learning module, interest-related network module and product similarity recommendation module together form the new model.By conducting extensive experiments on publicly accessible datasets, the results demonstrate the state-of-the-art performance of our proposed algorithm and its effectiveness. ",Kein DOI-Link verfügbar,2205.03297v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Affective Digital Twins for Digital Human: Bridging the Gap in   Human-Machine Affective Interaction,1970,"  In recent years, metaverse and digital humans have become important research and industry areas of focus. However, existing digital humans still lack realistic affective traits, making emotional interaction with humans difficult. Grounded in the developments of artificial intelligence, human-computer interaction, virtual reality, and affective computing, this paper proposes the concept and technical framework of ""Affective Digital Twins for Digital Human"" based on the philosophy of digital twin technology. The paper discusses several key technical issues including affective modeling, affective perception, affective encoding, and affective expression. Based on this, the paper conducts a preliminary imagination of the future application prospects of affective digital twins for digital human, while considering potential problems that may need to be addressed. ",Kein DOI-Link verfügbar,2308.10207v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Topological $p_{x}+ip_{y}$ Superfluid Phase of a Dipolar Fermi Gas in a   2D Optical Lattice,1970,"  In a dipolar Fermi gas, the anisotropic interaction between electric dipoles can be turned into an effectively attractive interaction in the presence of a rotating electric field. We show that the topological $p_{x}+ip_{y}$ superfluid phase can be realized in a single-component dipolar Fermi gas trapped in a 2D square optical lattice with this attractive interaction at low temperatures. The $p_{x}+ip_{y}$ superfluid state has potential applications for topological quantum computing. We obtain the phase diagram of this system at zero temperature. In the weak-coupling limit, the p-wave superfluid phase is stable for all filling factors. As the interaction strength increases, it is stable close to filling factors $n=0$ or $n=1$, and phase separation takes place in between. When the interaction strength is above a threshold, the system is phase separated for any $0<n<1$. The transition temperature of the $p_{x}+ip_{y}$ superfluid state is estimated and the implication for experiments is discussed. ",https://doi.org/10.1103/PhysRevA.86.031603,1202.4924v2,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Metric Residual Networks for Sample Efficient Goal-Conditioned   Reinforcement Learning,1970,"  Goal-conditioned reinforcement learning (GCRL) has a wide range of potential real-world applications, including manipulation and navigation problems in robotics. Especially in such robotics tasks, sample efficiency is of the utmost importance for GCRL since, by default, the agent is only rewarded when it reaches its goal. While several methods have been proposed to improve the sample efficiency of GCRL, one relatively under-studied approach is the design of neural architectures to support sample efficiency. In this work, we introduce a novel neural architecture for GCRL that achieves significantly better sample efficiency than the commonly-used monolithic network architecture. The key insight is that the optimal action-value function Q^*(s, a, g) must satisfy the triangle inequality in a specific sense. Furthermore, we introduce the metric residual network (MRN) that deliberately decomposes the action-value function Q(s,a,g) into the negated summation of a metric plus a residual asymmetric component. MRN provably approximates any optimal action-value function Q^*(s,a,g), thus making it a fitting neural architecture for GCRL. We conduct comprehensive experiments across 12 standard benchmark environments in GCRL. The empirical results demonstrate that MRN uniformly outperforms other state-of-the-art GCRL neural architectures in terms of sample efficiency. ",Kein DOI-Link verfügbar,2208.08133v4,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Stability of Branched Flow from a Quantum Point Contact,1970,"  In classically chaotic systems, small differences in initial conditions are exponentially magnified over time. However, it was observed experimentally that the (necessarily quantum) ""branched flow"" pattern of electron flux from a quantum point contact (QPC) traveling over a random background potential in two-dimensional electron gases(2DEGs) remains substantially invariant to large changes in initial conditions. Since such a potential is classically chaotic and unstable to changes in initial conditions, it was conjectured that the origin of the observed stability is purely quantum mechanical, with no classical analog. In this paper, we show that the observed stability is a result of the physics of the QPC and the nature of the experiment. We show that the same stability can indeed be reproduced classically, or quantum mechanically. In addition, we explore the stability of the branched flow with regards to changes in the eigenmodes of quantum point contact. ",https://doi.org/10.1103/PhysRevLett.111.236804,1309.1814v3,Yes,potent(2)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Chiral orbital magnetism of $p$-orbital bosons in optical lattices,1970,"  Chiral magnetism is a fascinating quantum phenomena that has been found in low-dimensional magnetic materials. It is not only interesting for understanding the concept of chirality, but also important for potential applications in spintronics. Past studies show that chiral magnets require both lack of the inversion symmetry and spin-orbit coupling to induce the Dzyaloshinskii-Moriya (DM) interaction. Here we report that the combination of inversion symmetry breaking and quantum degeneracy of orbital degrees of freedom will provide a new paradigm to achieve the chiral orbital magnetism. By means of the density matrix renormalization group (DMRG) calculation, we demonstrate that the chiral orbital magnetism can be found when considering bosonic atoms loaded in the $p$-band of an optical lattice in the Mott regime. The high tunability of our scheme is also illustrated through simply manipulating the inversion symmetry of the system for the cold atom experimental conditions. ",https://doi.org/10.1103/PhysRevLett.121.015303,1710.08145v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Deep Object Co-segmentation via Spatial-Semantic Network Modulation,1970,"  Object co-segmentation is to segment the shared objects in multiple relevant images, which has numerous applications in computer vision. This paper presents a spatial and semantic modulated deep network framework for object co-segmentation. A backbone network is adopted to extract multi-resolution image features. With the multi-resolution features of the relevant images as input, we design a spatial modulator to learn a mask for each image. The spatial modulator captures the correlations of image feature descriptors via unsupervised learning. The learned mask can roughly localize the shared foreground object while suppressing the background. For the semantic modulator, we model it as a supervised image classification task. We propose a hierarchical second-order pooling module to transform the image features for classification use. The outputs of the two modulators manipulate the multi-resolution features by a shift-and-scale operation so that the features focus on segmenting co-object regions. The proposed model is trained end-to-end without any intricate post-processing. Extensive experiments on four image co-segmentation benchmark datasets demonstrate the superior accuracy of the proposed method compared to state-of-the-art methods. ",Kein DOI-Link verfügbar,1911.12950v1,Yes,intricate(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Anharmonicity Induced Supersolidity In Spin-Orbit Coupled Bose-Einstein   Condensates,1970,"  Supersolid, a fascinating quantum state of matter, features novel phenomena such as the non-classical rotational inertia and transport anomalies. It is a long standing issue of the coexistence of superfluidity and broken translational symmetry in condensed matter physics. By recent experimental advances to create tunable synthetic spin-orbit coupling in ultracold gases, such highly controllable atomic systems would provide new possibilities to access supersolidity with no counterpart in solids. Here we report that the combination of anharmonicity of trapping potential and spin-orbit coupling will provide a new paradigm to achieve supersolids. By means of imaginary time evolution of the Gross-Pitaevskii equation, we demonstrate that a supersolid state can be found when considering a trapped Rashba-type spin-orbit coupled bosonic atoms loaded in a one-dimensional optical lattice. Furthermore, a skyrmion-anti-skyrmion lattice is associated with the appearance of such supersoildity, indicating the topological nontrivial properties of our proposed supersolids. ",https://doi.org/10.1103/PhysRevA.102.033328,1909.11871v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Lorentz Force Correction and Radiation Frequency Property of Charged   Particles in Magnetic Dipole,1970,"  By concern of compression of charge density field, the corrected Lorentz force formula and consequent inference is presented. And further radiation frequency property of an individual charge density field in magnetic dipole is analyzed respectively for radiant property of the charged particle and the emitted electromagnetic wave transfer property between the moving radiant source and observer. As results, the behavior and radiation frequency property of the electron beam in magnetic dipole is interpreted upon the individual's behavior and property. At final, the potential application is put forward for wider interest. ",Kein DOI-Link verfügbar,physics/0304023v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Bayesian Analysis for miRNA and mRNA Interactions Using Expression Data,1970,"  MicroRNAs (miRNAs) are small RNA molecules composed of 19-22 nt, which play important regulatory roles in post-transcriptional gene regulation by inhibiting the translation of the mRNA into proteins or otherwise cleaving the target mRNA. Inferring miRNA targets provides useful information for understanding the roles of miRNA in biological processes that are potentially involved in complex diseases. Statistical methodologies for point estimation, such as the Least Absolute Shrinkage and Selection Operator (LASSO) algorithm, have been proposed to identify the interactions of miRNA and mRNA based on sequence and expression data. In this paper, we propose using the Bayesian LASSO (BLASSO) and the non-negative Bayesian LASSO (nBLASSO) to analyse the interactions between miRNA and mRNA using expression data. The proposed Bayesian methods explore the posterior distributions for those parameters required to model the miRNA-mRNA interactions. These approaches can be used to observe the inferred effects of the miRNAs on the targets by plotting the posterior distributions of those parameters. For comparison purposes, the Least Squares Regression (LSR), Ridge Regression (RR), LASSO, non-negative LASSO (nLASSO), and the proposed Bayesian approaches were applied to four public datasets. We concluded that nLASSO and nBLASSO perform best in terms of sensitivity and specificity. Compared to the point estimate algorithms, which only provide single estimates for those parameters, the Bayesian methods are more meaningful and provide credible intervals, which take into account the uncertainty of the inferred interactions of the miRNA and mRNA. Furthermore, Bayesian methods naturally provide statistical significance to select convincing inferred interactions, while point estimate algorithms require a manually chosen threshold, which is less meaningful, to choose the possible interactions. ",Kein DOI-Link verfügbar,1210.3456v2,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Constraining the anomalous Higgs boson coupling in $H$+$γ$   production,1970,"  Higgs boson production in association with a photon ($H$+$\gamma$) offers a promising channel to test the Higgs boson to photon coupling at various energy scales. Its potential sensitivity to anomalous couplings of the Higgs boson has not been explored with the proton-proton collision data. In this paper, we reinterpret the latest ATLAS $H$+$\gamma$ resonance search results within the Standard Model effective field theory (EFT) framework, using 36.1 fb$^{-1}$ of proton-proton collision data recorded with the ATLAS detector at $\sqrt{s}=13$ TeV. Constraints on the Wilson coefficients of dimension-six EFT operators related to the Higgs boson to photon coupling are provided for the first time in the $H$+$\gamma$ final state at the LHC. ",https://doi.org/10.1088/1674-1137/43/4/043001,1811.02261v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",A Lifelong Learning Approach to Mobile Robot Navigation,1970,"  This paper presents a self-improving lifelong learning framework for a mobile robot navigating in different environments. Classical static navigation methods require environment-specific in-situ system adjustment, e.g. from human experts, or may repeat their mistakes regardless of how many times they have navigated in the same environment. Having the potential to improve with experience, learning-based navigation is highly dependent on access to training resources, e.g. sufficient memory and fast computation, and is prone to forgetting previously learned capability, especially when facing different environments. In this work, we propose Lifelong Learning for Navigation (LLfN) which (1) improves a mobile robot's navigation behavior purely based on its own experience, and (2) retains the robot's capability to navigate in previous environments after learning in new ones. LLfN is implemented and tested entirely onboard a physical robot with a limited memory and computation budget. ",Kein DOI-Link verfügbar,2007.14486v4,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Dual-Role AoI-based Incentive Mechanism for HD map Crowdsourcing,1970,"  A high-quality fresh high-definition (HD) map is vital in enhancing transportation efficiency and safety in autonomous driving. Vehicle-based crowdsourcing offers a promising approach for updating HD maps. However, recruiting crowdsourcing vehicles involves making the challenging tradeoff between the HD map freshness and recruitment costs. Existing studies on HD map crowdsourcing often (1) prioritize maximizing spatial coverage and (2) overlook the dual role of crowdsourcing vehicles in HD maps, as vehicles serve both as contributors and customers of HD maps. This motivates us to propose the Dual-Role Age of Information (AoI) based Incentive Mechanism (DRAIM) to address these issues. % Specifically, we propose the trajectory age of information, incorporating the expected AoI of the HD map and the trajectory, to quantify a vehicle's HD map usage utility, which is freshness- and trajectory-dependent. DRAIM aims to achieve the company's tradeoff between freshness and recruitment costs. ",Kein DOI-Link verfügbar,2405.00353v1,Yes,fresh(4)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Real-world challenges for multi-agent reinforcement learning in   grid-interactive buildings,1970,"  Building upon prior research that highlighted the need for standardizing environments for building control research, and inspired by recently introduced challenges for real life reinforcement learning control, here we propose a non-exhaustive set of nine real world challenges for reinforcement learning control in grid-interactive buildings. We argue that research in this area should be expressed in this framework in addition to providing a standardized environment for repeatability. Advanced controllers such as model predictive control and reinforcement learning (RL) control have both advantages and disadvantages that prevent them from being implemented in real world problems. Comparisons between the two are rare, and often biased. By focusing on the challenges, we can investigate the performance of the controllers under a variety of situations and generate a fair comparison. As a demonstration, we implement the offline learning challenge in CityLearn and study the impact of different levels of domain knowledge and complexity of RL algorithms. We show that the sequence of operations utilized in a rule based controller (RBC) used for offline training affects the performance of the RL agents when evaluated on a set of four energy flexibility metrics. Longer offline learning from an optimized RBC leads to improved performance in the long run. RL agents that learn from a simplified RBC risk poorer performance as the offline learning period increases. We also observe no impact on performance from information sharing amongst agents. We call for a more interdisciplinary effort of the research community to address the real world challenges, and unlock the potential of grid-interactive building ",https://doi.org/10.1016/j.egyai.2022.100202,2112.06127v2,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",A Critical Review of Inductive Logic Programming Techniques for   Explainable AI,1970,"  Despite recent advances in modern machine learning algorithms, the opaqueness of their underlying mechanisms continues to be an obstacle in adoption. To instill confidence and trust in artificial intelligence systems, Explainable Artificial Intelligence has emerged as a response to improving modern machine learning algorithms' explainability. Inductive Logic Programming (ILP), a subfield of symbolic artificial intelligence, plays a promising role in generating interpretable explanations because of its intuitive logic-driven framework. ILP effectively leverages abductive reasoning to generate explainable first-order clausal theories from examples and background knowledge. However, several challenges in developing methods inspired by ILP need to be addressed for their successful application in practice. For example, existing ILP systems often have a vast solution space, and the induced solutions are very sensitive to noises and disturbances. This survey paper summarizes the recent advances in ILP and a discussion of statistical relational learning and neural-symbolic algorithms, which offer synergistic views to ILP. Following a critical review of the recent advances, we delineate observed challenges and highlight potential avenues of further ILP-motivated research toward developing self-explanatory artificial intelligence systems. ",Kein DOI-Link verfügbar,2112.15319v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Spatial enantioseparation of gaseous chiral molecules,1970,"  We explore the spatial enantioseparation of gaseous chiral molecules for the cyclic three-level systems coupled with three electromagnetic fields. Due to molecular rotations, the specific requirements of the polarization directions of the three electromagnetic fields lead to the space-dependent part of the overall phase of the coupling strengths. Thus, the overall phase of the coupling strengths, which differs with $\pi$ for the enantiomers in the cyclic three-level model of chiral molecules, varies intensely in the length scale of the typical wavelength of the applied electromagnetic fields. Under the induced gauge potentials resulting from the space-dependent part of the overall phase and the space-dependent intensities of coupling strengths, we further show spatial enantioseparation for typical parameters of gaseous chiral molecules. ",https://doi.org/10.1103/PhysRevA.104.013113,2103.01758v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Coalescence induced late departure of bubbles improves water   electrolysis efficiency,1970,"  In this study, we examine the effects of bubble collisions and coalescence on electrolysis performance using micro-electrodes in a 0.5 M H2SO4. We observe that the addition of electrolytes such as HClO4, despite increasing conductivity, significantly reduces energy conversion efficiency by inhibiting bubble coalescence. A phase diagram illustrates the trade-off between improved conductivity and inhibition of coalescence with varying concentrations of supporting electrolytes (HClO4 or Na2SO4) at different electrolysis current. Contrary to conventional understanding, we find that larger bubbles resulting from coalescence enhance efficiency. This can be attributed to the early release of smaller bubbles, approximately 10 micron in size, facilitated by the coalescence of the surface microbubble and the just-detached large bubble. Additionally, the merge of two bubbles, at a critical velocity of ~2 m/s, enhances agitation at the electrode surface that significantly improves electrolyte transport within the stagnant layer. These findings challenge conventional perspectives on bubble detachment size and suggest innovative approaches to optimize energy consumption in electrolysis, especially in systems where the bubble coalescence is inhibited, such as alkaline water electrolysis and the chlor-alkali industry. ",Kein DOI-Link verfügbar,2403.12064v1,Yes,innovative(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Tricritical point and solid/liquid/gas phase transition of higher   dimensional AdS black hole in massive gravity,1970,"  By considering the fifth order term of the interaction potential in massive gravity theory, we study the $P-V$ critical behaviors of AdS black hole in $d \geq 7$ dimensional space-time, and find the tricritical point and the solid/liquid/gas phase transition in addition to the Van der Waals-like phase and the reentrant phase transition of the system. The critical phenomena of black holes depend crucially on the number $n$ of interaction potential terms. ",Kein DOI-Link verfügbar,1810.07885v1,Yes,potent(2)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Video Saliency Prediction Using Enhanced Spatiotemporal Alignment   Network,1970,"  Due to a variety of motions across different frames, it is highly challenging to learn an effective spatiotemporal representation for accurate video saliency prediction (VSP). To address this issue, we develop an effective spatiotemporal feature alignment network tailored to VSP, mainly including two key sub-networks: a multi-scale deformable convolutional alignment network (MDAN) and a bidirectional convolutional Long Short-Term Memory (Bi-ConvLSTM) network. The MDAN learns to align the features of the neighboring frames to the reference one in a coarse-to-fine manner, which can well handle various motions. Specifically, the MDAN owns a pyramidal feature hierarchy structure that first leverages deformable convolution (Dconv) to align the lower-resolution features across frames, and then aggregates the aligned features to align the higher-resolution features, progressively enhancing the features from top to bottom. The output of MDAN is then fed into the Bi-ConvLSTM for further enhancement, which captures the useful long-time temporal information along forward and backward timing directions to effectively guide attention orientation shift prediction under complex scene transformation. Finally, the enhanced features are decoded to generate the predicted saliency map. The proposed model is trained end-to-end without any intricate post processing. Extensive evaluations on four VSP benchmark datasets demonstrate that the proposed method achieves favorable performance against state-of-the-art methods. The source codes and all the results will be released. ",Kein DOI-Link verfügbar,2001.00292v1,Yes,intricate(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",An Iterative Co-Training Transductive Framework for Zero Shot Learning,1970,"  In zero-shot learning (ZSL) community, it is generally recognized that transductive learning performs better than inductive one as the unseen-class samples are also used in its training stage. How to generate pseudo labels for unseen-class samples and how to use such usually noisy pseudo labels are two critical issues in transductive learning. In this work, we introduce an iterative co-training framework which contains two different base ZSL models and an exchanging module. At each iteration, the two different ZSL models are co-trained to separately predict pseudo labels for the unseen-class samples, and the exchanging module exchanges the predicted pseudo labels, then the exchanged pseudo-labeled samples are added into the training sets for the next iteration. By such, our framework can gradually boost the ZSL performance by fully exploiting the potential complementarity of the two models' classification capabilities. In addition, our co-training framework is also applied to the generalized ZSL (GZSL), in which a semantic-guided OOD detector is proposed to pick out the most likely unseen-class samples before class-level classification to alleviate the bias problem in GZSL. Extensive experiments on three benchmarks show that our proposed methods could significantly outperform about $31$ state-of-the-art ones. ",Kein DOI-Link verfügbar,2203.16041v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Enhancing Essay Scoring with Adversarial Weights Perturbation and   Metric-specific AttentionPooling,1970,"  The objective of this study is to improve automated feedback tools designed for English Language Learners (ELLs) through the utilization of data science techniques encompassing machine learning, natural language processing, and educational data analytics. Automated essay scoring (AES) research has made strides in evaluating written essays, but it often overlooks the specific needs of English Language Learners (ELLs) in language development. This study explores the application of BERT-related techniques to enhance the assessment of ELLs' writing proficiency within AES.   To address the specific needs of ELLs, we propose the use of DeBERTa, a state-of-the-art neural language model, for improving automated feedback tools. DeBERTa, pretrained on large text corpora using self-supervised learning, learns universal language representations adaptable to various natural language understanding tasks. The model incorporates several innovative techniques, including adversarial training through Adversarial Weights Perturbation (AWP) and Metric-specific AttentionPooling (6 kinds of AP) for each label in the competition.   The primary focus of this research is to investigate the impact of hyperparameters, particularly the adversarial learning rate, on the performance of the model. By fine-tuning the hyperparameter tuning process, including the influence of 6AP and AWP, the resulting models can provide more accurate evaluations of language proficiency and support tailored learning tasks for ELLs. This work has the potential to significantly benefit ELLs by improving their English language proficiency and facilitating their educational journey. ",Kein DOI-Link verfügbar,2401.05433v1,Yes,"innovative(1), potent(1)"
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Toward Agile Maneuvers in Highly Constrained Spaces: Learning from   Hallucination,1970,"  While classical approaches to autonomous robot navigation currently enable operation in certain environments, they break down in tightly constrained spaces, e.g., where the robot needs to engage in agile maneuvers to squeeze between obstacles. Recent machine learning techniques have the potential to address this shortcoming, but existing approaches require vast amounts of navigation experience for training, during which the robot must operate in close proximity to obstacles and risk collision. In this paper, we propose to side-step this requirement by introducing a new machine learning paradigm for autonomous navigation called learning from hallucination (LfH), which can use training data collected in completely safe environments to compute navigation controllers that result in fast, smooth, and safe navigation in highly constrained environments. Our experimental results show that the proposed LfH system outperforms three autonomous navigation baselines on a real robot and generalizes well to unseen environments, including those based on both classical and machine learning techniques. ",Kein DOI-Link verfügbar,2007.14479v4,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",VI-OOD: A Unified Representation Learning Framework for Textual   Out-of-distribution Detection,1970,"  Out-of-distribution (OOD) detection plays a crucial role in ensuring the safety and reliability of deep neural networks in various applications. While there has been a growing focus on OOD detection in visual data, the field of textual OOD detection has received less attention. Only a few attempts have been made to directly apply general OOD detection methods to natural language processing (NLP) tasks, without adequately considering the characteristics of textual data. In this paper, we delve into textual OOD detection with Transformers. We first identify a key problem prevalent in existing OOD detection methods: the biased representation learned through the maximization of the conditional likelihood $p(y\mid x)$ can potentially result in subpar performance. We then propose a novel variational inference framework for OOD detection (VI-OOD), which maximizes the likelihood of the joint distribution $p(x, y)$ instead of $p(y\mid x)$. VI-OOD is tailored for textual OOD detection by efficiently exploiting the representations of pre-trained Transformers. Through comprehensive experiments on various text classification tasks, VI-OOD demonstrates its effectiveness and wide applicability. Our code has been released at \url{https://github.com/liam0949/LLM-OOD}. ",Kein DOI-Link verfügbar,2404.06217v1,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Bloch bound state of spin-orbit-coupled fermions in an optical lattice,1970,"  Understanding fundamentals of few-body physics provides an interesting bottom-up approach for the clarification of many-body properties. The remarkable experimental progress in realizing spin-orbit coupling (SOC) in optical Raman lattices offers a renewed thrust towards discovering novel few-body features induced by the interplay between SOC and optical lattices. Using the Wilson renormalization method to account for high-band effects, we study the low-energy two-body scattering processes of spin-$1/2$ fermions in spin-orbit coupled optical lattices. We demonstrate that, under weak SOC, adding a small lattice potential would destabilize shallow two-body bound states, contrary to conventional wisdom. On the other hand, when lattice is sufficiently deep, two-body bound states are always stabilized by increasing the lattice depth. This intriguing non-monotonic behavior of the bound-state stability derives from the competition between SOC and optical lattices, and can be explained by analyzing the low-energy density of states. We also discuss the impact of high-band effects on such a behavior, as well as potential experimental detections. ",https://doi.org/10.1103/PhysRevA.99.012703,1806.02478v2,Yes,potent(2)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Continuous-time Gaussian Process Trajectory Generation for Multi-robot   Formation via Probabilistic Inference,1970,"  In this paper, we extend a famous motion planning approach GPMP2 to multi-robot cases, yielding a novel centralized trajectory generation method for the multi-robot formation. A sparse Gaussian Process model is employed to represent the continuous-time trajectories of all robots as a limited number of states, which improves computational efficiency due to the sparsity. We add constraints to guarantee collision avoidance between individuals as well as formation maintenance, then all constraints and kinematics are formulated on a factor graph. By introducing a global planner, our proposed method can generate trajectories efficiently for a team of robots which have to get through a width-varying area by adaptive formation change. Finally, we provide the implementation of an incremental replanning algorithm to demonstrate the online operation potential of our proposed framework. The experiments in simulation and real world illustrate the feasibility, efficiency and scalability of our approach. ",Kein DOI-Link verfügbar,2010.13148v3,Yes,potent(1)
0000-0002-8697-234X,Bo Liu,"King's College London, King's College London - Guy's Campus",Automatic Extraction of Medication Names in Tweets as Named Entity   Recognition,1970,"  Social media posts contain potentially valuable information about medical conditions and health-related behavior. Biocreative VII Task 3 focuses on mining this information by recognizing mentions of medications and dietary supplements in tweets. We approach this task by fine tuning multiple BERT-style language models to perform token-level classification, and combining them into ensembles to generate final predictions. Our best system consists of five Megatron-BERT-345M models and achieves a strict F1 score of 0.764 on unseen test data. ",Kein DOI-Link verfügbar,2111.15641v1,Yes,potent(1)
0000-0001-7351-6014,Alexandra Parker,King's College London,A prototype software framework for transferable computational health   economic models and its early application in youth mental health,1970,"  We are developing an economic model to explore multiple topics in Australian youth mental health policy. We want that model to be readily transferable to other jurisdictions. We developed a software framework for authoring transparent, reusable and updatable Computational Health Economic Models (CHEMs) (the software files that implement health economic models). We specified framework user requirements of a template CHEM module that facilitates modular model implementations, a simple programming syntax and tools for authoring new CHEM modules, supplying CHEMs with data, reporting reproducible CHEM analyses, searching for CHEM modules and maintaining a CHEM project website. We implemented the framework as six development version code libraries in the programming language R that integrate with online services for software development and research data archiving. We used the framework to author five development version R libraries of CHEM modules focused on utility mapping in youth mental health. These modules provide tools for variable validation, dataset description, multi-attribute instrument scoring, construction of mapping models, reporting of mapping studies and making out of sample predictions. We assessed these CHEM module libraries as mostly meeting transparency, reusability and updatability criteria that we have previously developed, but requiring more detailed documentation and unit testing of individual modules. Our software framework has potential value as a prototype for future tools to support the development of transferable CHEMs. ",https://doi.org/10.1007/s40273-024-01378-8,2310.14138v2,Yes,potent(1)
0000-0003-2591-3888,Laurent Binet,Université PSL,"Structure, composition, and location of organic matter in the enstatite   chondrite Sahara 97096 (EH3)",1970,"  The insoluble organic matter (IOM) of an unequilibrated enstatite chondrite Sahara (SAH) 97096 has been investigated using a battery of analytical techniques. As the enstatite chondrites are thought to have formed in a reduced environment at higher temperatures than carbonaceous chondrites, they constitute an interesting comparative material to test the heterogeneities of the IOM in the solar system and to constrain the processes that could affect IOM during solar system evolution. The SAH 97096 IOM is found in situ: as submicrometer grains in the network of fine-grained matrix occurring mostly around chondrules and as inclusions in metallic nodules, where the carbonaceous matter appears to be more graphitized. IOM in these two settings has very similar $\delta^{15}N$ and $\delta^{13}C$; this supports the idea that graphitized inclusions in metal could be formed by metal catalytic graphitization of matrix IOM. A detailed comparison between the IOM extracted from a fresh part and a terrestrially weathered part of SAH 97096 shows the similarity between both IOM samples in spite of the high degree of mineral alteration in the latter. The isolated IOM exhibits a heterogeneous polyaromatic macromolecular structure, sometimes highly graphitized, without any detectable free radicals and deuterium-heterogeneity and having mean H- and N-isotopic compositions in the range of values observed for carbonaceous chondrites. It contains some submicrometer-sized areas highly enriched in $^{15}N$ ($\delta^{15}N$ up to 1600 permil). These observations reinforce the idea that the IOM found in carbonaceous chondrites is a common component widespread in the solar system. Most of the features of SAH 97096 IOM could be explained by the thermal modification of this main component. ",https://doi.org/10.1111/j.1945-5100.2011.01306.x,1502.00216v1,Yes,fresh(1)
0000-0002-6229-9103,Alireza Amirshahi,EPFL,Predicting Survey Response with Quotation-based Modeling: A Case Study   on Favorability towards the United States,1970,"  The acquisition of survey responses is a crucial component in conducting research aimed at comprehending public opinion. However, survey data collection can be arduous, time-consuming, and expensive, with no assurance of an adequate response rate. In this paper, we propose a pioneering approach for predicting survey responses by examining quotations using machine learning. Our investigation focuses on evaluating the degree of favorability towards the United States, a topic of interest to many organizations and governments. We leverage a vast corpus of quotations from individuals across different nationalities and time periods to extract their level of favorability. We employ a combination of natural language processing techniques and machine learning algorithms to construct a predictive model for survey responses. We investigate two scenarios: first, when no surveys have been conducted in a country, and second when surveys have been conducted but in specific years and do not cover all the years. Our experimental results demonstrate that our proposed approach can predict survey responses with high accuracy. Furthermore, we provide an exhaustive analysis of the crucial features that contributed to the model's performance. This study has the potential to impact survey research in the field of data science by substantially decreasing the cost and time required to conduct surveys while simultaneously providing accurate predictions of public opinion. ",Kein DOI-Link verfügbar,2305.14086v2,Yes,potent(1)
0000-0002-6229-9103,Alireza Amirshahi,EPFL,MetaWearS: A Shortcut in Wearable Systems Lifecycle with Only a Few   Shots,1970,"  Wearable systems provide continuous health monitoring and can lead to early detection of potential health issues. However, the lifecycle of wearable systems faces several challenges. First, effective model training for new wearable devices requires substantial labeled data from various subjects collected directly by the wearable. Second, subsequent model updates require further extensive labeled data for retraining. Finally, frequent model updating on the wearable device can decrease the battery life in long-term data monitoring. Addressing these challenges, in this paper, we propose MetaWearS, a meta-learning method to reduce the amount of initial data collection required. Moreover, our approach incorporates a prototypical updating mechanism, simplifying the update process by modifying the class prototype rather than retraining the entire model. We explore the performance of MetaWearS in two case studies, namely, the detection of epileptic seizures and the detection of atrial fibrillation. We show that by fine-tuning with just a few samples, we achieve 70% and 82% AUC for the detection of epileptic seizures and the detection of atrial fibrillation, respectively. Compared to a conventional approach, our proposed method performs better with up to 45% AUC. Furthermore, updating the model with only 16 minutes of additional labeled data increases the AUC by up to 5.3%. Finally, MetaWearS reduces the energy consumption for model updates by 456x and 418x for epileptic seizure and AF detection, respectively. ",Kein DOI-Link verfügbar,2408.01988v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Conduction of Ultracold Fermions Through a Mesoscopic Channel,1970,"  In a mesoscopic conductor electric resistance is detected even if the device is defect-free. We engineer and study a cold-atom analog of a mesoscopic conductor. It consists of a narrow channel connecting two macroscopic reservoirs of fermions that can be switched from ballistic to diffusive. We induce a current through the channel and find ohmic conduction, even for a ballistic channel. An analysis of in-situ density distributions shows that in the ballistic case the chemical potential drop occurs at the entrance and exit of the channel, revealing the presence of contact resistance. In contrast, a diffusive channel with disorder displays a chemical potential drop spread over the whole channel. Our approach opens the way towards quantum simulation of mesoscopic devices with quantum gases. ",https://doi.org/10.1126/science.1223175,1203.1927v2,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Superfluidity with disorder in a quantum gas thin film,1970,"  We investigate the properties of a strongly interacting, superfluid gas of 6Li2 Feshbach molecules forming a thin film confined in a quasi two-dimensional channel with a tunable random potential, creating a microscopic disorder. We measure the atomic current and extract the resistance of the film in a two-terminal configuration, and identify a superfluid state at low disorder strength, which evolves into a normal, poorly conducting state for strong disorder. The transition takes place when the chemical potential reaches the percolation threshold of the disorder. The evolution of the conduction properties contrasts with the smooth behavior of the density and compressibility across the transition, measured in-situ at equilibrium. These features suggest the emergence of a glass-like phase at strong disorder. ",Kein DOI-Link verfügbar,1211.7272v1,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Two-terminal transport measurements with cold atoms,1970,"  In the last years, the ability of cold atoms experiments to explore condensed- matter related questions has dramatically progressed. Transport experiments, in particular, have expanded to the point that conductances and other transport coefficients can now be measured in a way directly analogous to solid state physics, extending cold atoms based quantum simulations into the domain of quantum electronic devices. In this topical review, we describe the transport experiments performed with cold gases in the two terminals configuration, with an emphasis on the specific features of cold atomic gases compared to solid state physics. We present the experimental techniques and the main experimental findings, focusing on but not restricted to the recent experiments performed in our group. We eventually discuss the perspectives opened by this approach, the main technical and conceptual challenges for future developments, and the potential applications as a quantum simulator for transport phenomena and mesoscopic physics problems. ",https://doi.org/10.1088/1361-648X/aa74a1,1706.01085v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Light-shift tomography in an optical-dipole trap for neutral atoms,1970,"  We report on light-shift tomography of a cloud of 87 Rb atoms in a far-detuned optical-dipole trap at 1565 nm. Our method is based on standard absorption imaging, but takes advantage of the strong light-shift of the excited state of the imaging transition, which is due to a quasi-resonance of the trapping laser with a higher excited level. We use this method to (i) map the equipotentials of a crossed optical-dipole trap, and (ii) study the thermalisation of an atomic cloud by following the evolution of the potential-energy of atoms during the free-evaporation process. ",https://doi.org/10.1103/PhysRevA.78.031401,0807.3672v1,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Observing the Drop of Resistance in the Flow of a Superfluid Fermi Gas,1970,"  In this work, we investigate the conduction properties of strongly interacting fermions flowing through a quasi two-dimensional, multimode channel, which connects two atomic reservoirs. The atomic current in the channel is controlled using a repulsive potential created by an off-resonant laser beam. In analogy with an electronic field-effect transistor, this gate potential controls the chemical potential in the channel while keeping the temperature imposed by the reservoirs unchanged. With the gate potential as a control parameter, we measure the current through the channel over a large dynamic range and determine the density distribution in the channel region. This allows us to observe the onset of superfluid flow of strongly interacting fermions. These measurements are compared to the case of a weakly interacting Fermi gas. ",https://doi.org/10.1038/nature11613,1210.1426v1,Yes,potent(4)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,"Direct Observation of Fragmentation in a Disordered, Strongly   Interacting Fermi Gas",1970,"  Describing the behaviour of strongly interacting particles in the presence of disorder is among the most challenging problems in quantum many-body physics. The controlled setting of cold atom experiments provides a new avenue to address these challenges [1], complementing studies in solid state physics, where a number of puzzling findings have emerged in experiments using superconducting thin films [2,3]. Here we investigate a strongly interacting thin film of an atomic Fermi gas subject to a random potential. We use high-resolution in-situ imaging [4-7] to resolve the atomic density at the length scale of a single impurity, which would require scanning probe techniques in solid state physics [8]. This allows us to directly observe the fragmentation of the density profile and to extract its percolation properties. Transport measurements in a two-terminal configuration indicate that the fragmentation process is accompanied by a breakdown of superfluidity. Our results suggest that percolation of paired atoms is responsible for the loss of superfluidity, and that disorder is able to increase the binding energy of pairs. ",https://doi.org/10.1103/PhysRevLett.115.045302,1311.5174v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,A cavity quantum electrodynamics implementation of the   Sachdev--Ye--Kitaev model,1970,"  The search for a quantum theory of gravity has led to the discovery of quantum many-body systems that are dual to gravitational models with quantum properties. The perhaps most famous of these systems is the Sachdev-Ye-Kitaev (SYK) model. It features maximal scrambling of quantum information, and opens a potential inroad to experimentally investigating aspects of quantum gravity. A scalable laboratory realisation of this model, however, remains outstanding. Here, we propose a feasible implementation of the SYK model in cavity quantum electrodynamics platforms. Through detailed analytical and numerical demonstrations, we show how driving a cloud of fermionic atoms trapped in a multi-mode optical cavity, and subjecting it to a spatially disordered AC-Stark shift retrieves the physics of the SYK model, with random all-to-all interactions and fast scrambling. Our work provides a blueprint for realising the SYK model in a scalable system, with the prospect of studying holographic quantum matter in the laboratory. ",Kein DOI-Link verfügbar,2303.11343v1,Yes,potent(1)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Observation of Quantized Conductance in Neutral Matter,1970,"  In transport experiments the quantum nature of matter becomes directly evident when changes in conductance occur only in discrete steps, with a size determined solely by Planck's constant h. The observations of quantized steps in the electric conductance have provided important insights into the physics of mesoscopic systems and allowed for the development of quantum electronic devices. Even though quantized conductance should not rely on the presence of electric charges, it has never been observed for neutral, massive particles. In its most fundamental form, the phenomenon requires a quantum degenerate Fermi gas, a ballistic and adiabatic transport channel, and a constriction with dimensions comparable to the Fermi wavelength. Here we report on the observation of quantized conductance in the transport of neutral atoms. We employ high resolution lithography to shape light potentials that realize either a quantum point contact or a quantum wire for atoms. These constrictions are imprinted on a quasi two-dimensional ballistic channel connecting two adjustable reservoirs of quantum degenerate fermionic lithium atoms. By tuning either a gate potential or the transverse confinement of the constrictions, we observe distinct plateaus in the conductance for atoms. The conductance in the first plateau is found to be equal to 1/h, the universal conductance quantum. For low gate potentials we find good agreement between the experimental data and the Landauer formula, with all parameters determined a priori. Our experiment constitutes the cold atom version of a mesoscopic device and can be readily extended to more complex geometries and interacting quantum gases. ",https://doi.org/10.1038/nature14049,1404.6400v1,Yes,potent(3)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Mapping out spin and particle conductances in a quantum point contact,1970,"  We study particle and spin transport in a single mode quantum point contact using a charge neutral, quantum degenerate Fermi gas with tunable, attractive interactions. This yields the spin and particle conductance of the point contact as a function of chemical potential or confinement. The measurements cover a regime from weak attraction, where quantized conductance is observed, to the resonantly interacting superfluid. Spin conductance exhibits a broad maximum when varying the chemical potential at moderate interactions, which signals the emergence of Cooper pairing. In contrast, the particle conductance is unexpectedly enhanced even before the gas is expected to turn into a superfluid, continuously rising from the plateau at 1/h for weak interactions to plateaux-like features at non-universal values as high as 4/h for intermediate interactions. For strong interactions, the particle conductance plateaux disappear and the spin conductance gets suppressed, confirming the spin-insulating character of a superfluid. Our observations document the breakdown of universal conductance quantization as many-body correlations appear. The observed anomalous quantization challenges a Fermi liquid description of the normal phase, shedding new light on the nature of the strongly attractive Fermi gases. ",https://doi.org/10.1073/pnas.1601812113,1511.05961v2,Yes,potent(2)
0000-0002-4610-6682,Jean-Philippe Brantut,EPFL,Breakdown of the Wiedemann-Franz law in a unitary Fermi gas,1970,"  We report on coupled heat and particle transport measurements through a quantum point contact (QPC) connecting two reservoirs of resonantly interacting, finite temperature Fermi gases. After heating one of them, we observe a particle current flowing from cold to hot. We monitor the temperature evolution of the reservoirs and find that the system evolves after an initial response into a non-equilibrium steady state with finite temperature and chemical potential differences across the QPC. In this state any relaxation in the form of heat and particle currents vanishes. From our measurements we extract the transport coefficients of the QPC and deduce a Lorenz number violating the Wiedemann-Franz law by one order of magnitude, a characteristic persisting even for a wide contact. In contrast, the Seebeck coefficient takes a value close to that expected for a non-interacting Fermi gas and shows a smooth decrease as the atom density close to the QPC is increased beyond the superfluid transition. Our work represents a fermionic analog of the fountain effect observed with superfluid helium and poses new challenges for microscopic modeling of the finite temperature dynamics of the unitary Fermi gas. ",https://doi.org/10.1073/pnas.1803336115,1803.00935v1,Yes,potent(1)
0000-0002-8386-0078,Sho Watanabe,EPFL,Ni$_{80}$Fe$_{20}$ Nanotubes with Optimized Spintronic Functionalities   Prepared by Atomic Layer Deposition,1970,"  Permalloy Ni$_{80}$Fe$_{20}$ is one of the key magnetic materials in the field of magnonics. Its potential would be further unveiled if it could be deposited in three dimensional (3D) architectures of sizes down to the nanometer. Atomic Layer Deposition, ALD, is the technique of choice for covering arbitrary shapes with homogeneous thin films. Early successes with ferromagnetic materials include nickel and cobalt. Still, challenges in depositing ferromagnetic alloys reside in the synthesis via decomposing the consituent elements at the same temperature and homogeneously. We report plasma-enhanced ALD to prepare permalloy Ni$_{80}$Fe$_{20}$ thin films and nanotubes using nickelocene and iron(III) tert-butoxide as metal precursors, water as the oxidant agent and an in-cycle plasma enhanced reduction step with hydrogen. We have optimized the ALD cycle in terms of Ni:Fe atomic ratio and functional properties. We obtained a Gilbert damping of 0.013, a resistivity of 28 $\mu\Omega$cm and an anisotropic magnetoresistance effect of 5.6 $\%$ in the planar thin film geometry. We demonstrate that the process also works for covering GaAs nanowires, resulting in permalloy nanotubes with high aspect ratios and diameters of about 150 nm. Individual nanotubes were investigated in terms of crystal phase, composition and spin-dynamic response by microfocused Brillouin Light Scattering. Our results enable NiFe-based 3D spintronics and magnonic devices in curved and complex topology operated in the GHz frequency regime. ",https://doi.org/10.1039/d1nr02291a,2105.01969v1,Yes,potent(1)
0000-0002-5586-5488,David Heim,EPFL,Piezo-optomechanical cantilever modulators for VLSI visible photonics,1970,"  Visible-wavelength very large-scale integration (VLSI) photonic circuits have potential to play important roles in quantum information and sensing technologies. The realization of scalable, high-speed, and low-loss photonic mesh circuits depends on reliable and well-engineered visible photonic components. Here we report a low-voltage optical phase shifter based on piezo-actuated mechanical cantilevers, fabricated on a CMOS compatible, 200 mm wafer-based visible photonics platform. We show linear phase and amplitude modulation with 6 V$_{\pi}$-cm in differential operation, -1.5 dB to -2 dB insertion loss, and up to 40 dB contrast in the 700 nm - 780 nm range. By adjusting selected cantilever parameters, we demonstrate a low-displacement and a high-displacement device, both exhibiting a nearly flat frequency response from DC to a peak mechanical resonance at 23 MHz and 6.8 MHz respectively, which through resonant enhancement of Q~40, further decreases the operating voltage down to 0.15 V$_{\pi}$-cm. ",https://doi.org/10.1063/5.0088424,2201.12447v1,Yes,potent(1)
0000-0002-9870-9477,Kirell Benzi,EPFL,Song Recommendation with Non-Negative Matrix Factorization and Graph   Total Variation,1970,"  This work formulates a novel song recommender system as a matrix completion problem that benefits from collaborative filtering through Non-negative Matrix Factorization (NMF) and content-based filtering via total variation (TV) on graphs. The graphs encode both playlist proximity information and song similarity, using a rich combination of audio, meta-data and social features. As we demonstrate, our hybrid recommendation system is very versatile and incorporates several well-known methods while outperforming them. Particularly, we show on real-world data that our model overcomes w.r.t. two evaluation metrics the recommendation of models solely based on low-rank information, graph-based information or a combination of both. ",Kein DOI-Link verfügbar,1601.01892v2,Yes,versatile(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Gravitational lensing in low-redshift clusters of galaxies: the arc-like   object in Abell 3408 and its lensing interpretation,1970,"  We analyze the seldomly discussed lensing effects expected in low-z clusters (z = 0.05-0.15), using as an example the bright arc (z=0.073) discovered by Campusano and Hardy(1996) near the dominant cD galaxy of the cluster Abell 3408 (z=0.042). We present photometric and spectroscopic observations for both the dominant galaxy and the arc. The mass distribution in A3408 is modeled by scaled versions of the representative distributions derived from studies of clusters at higher redshifts. The two gravitational potentials considered are: i) a ``minimum'' mass case where the mass distribution follows the light profile of the central elliptical galaxy and, ii) a ``maximum'' mass case where a typical massive dark halo is added to the previous case. The observed arc is well reproduced by both models, but rather small magnifications of the source galaxy are implied. The source galaxy is tentatively identified in both the lensing and non-lensing scenarios as being a spiral. The smaller lensed spiral (14.6 h_50^{-1} kpc, M_B=-18.2) predicted by the dark halo model appears to fit the observations marginally better. Furthermore, we found that only the dark halo model predicts a measurable amount of weak shear in the images of faint background galaxies. We conclude that observations, under very good seeing conditions, of week shear in faint background galaxies in the direction of low-redshift galaxy clusters are possible. When the latter are combined with X-ray data, a powerful tool to probe the mass distribution in the very central region of galaxy clusters emerges. ",https://doi.org/10.1086/311252,astro-ph/9712069v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Submillimeter-selected galaxies,1970,"  The first generation of submillimeter(submm)-wave surveys are being carried out using the 450/850-micron SCUBA camera at the JCMT on Mauna Kea. These surveys are potentially sensitive to galaxies at very high redshift, and the galaxies that have been detected so far appear to contribute the greater fraction of the mm/submm-wave background radiation intensity measured by COBE. In order to understand this new population of galaxies, individual examples must be studied in detail across many wavebands; in particular their redshifts must be determined. We discuss the potential selection effects at work in submm-wave surveys and describe the spectral energy distributions of galaxies selected or luminous in the submm waveband. We also describe the general procedure for, and emphasize the difficulty of, identifying optical counterparts to submm-selected galaxies. Finally, we summarize what is known about the redshifts of these galaxies and the source of their luminosity. ",Kein DOI-Link verfügbar,astro-ph/9908111v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Detecting compact dark matter in galaxy clusters via gravitational   microlensing: A2218 & A370,1970,"  After decades of searching, the true nature of dark matter still eludes us. One potential probe of the form of dark matter in galaxy clusters is to search for microlensing variability in the giant arcs and arclets. In this paper, a simple method is introduced to characterize pixel variability in the limit of high optical depth to microlensing. Expanding on earlier work, the expected microlensing signal for two massive clusters, A2218 & A370 is calculated. It is found that the microlensing signal depends sensitively upon the mix of smooth and compact dark matter in the cluster. Comparison of two deep exposures taken with James Webb Space Telescope or two hour long exposures taken with a 30-metre class telescope in two epochs separated by a few years will possibly detect about a few dozen pixels which show strong variability due to microlensing at five sigma level, revealing wealth of information on the microlensing population. ",https://doi.org/10.1111/j.1365-2966.2004.08115.x,astro-ph/0406282v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Complete coordination of robotic fiber positioners for massive   spectroscopic surveys,1970,"  Robotic fiber positioners play a vital role in the generation of massive spectroscopic surveys. The more complete a positioners set is coordinated, the more information its corresponding spectrograph receives during an observation. The complete coordination problem of positioners sets is studied in this paper. We first define the local and the global completeness problems and determine their relationship. We then propose a new artificial potential field according to which the convergences of a positioner and its neighboring positioners are cooperatively taken into account. We also discover the required condition for a complete coordination. We finally explain how the modifications of some of the parameters of a positioners set may resolve its incompleteness coordination scenarios. We verify our accomplishments using simulations. ",https://doi.org/10.1117/1.JATIS.5.4.045002,2005.10448v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Astrobotics: Swarm Robotics for Astrophysical Studies,1970,"  This paper introduces the emerging field of astrobotics, that is, a recently-established branch of robotics to be of service to astrophysics and observational astronomy. We first describe a modern requirement of dark matter studies, i.e., the generation of the map of the observable universe, using astrobots. Astrobots differ from conventional two-degree-of-freedom robotic manipulators in two respects. First, the dense formation of astrobots give rise to the extremely overlapping dynamics of neighboring astrobots which make them severely subject to collisions. Second, the structure of astrobots and their mechanical specifications are specialized due to the embedded optical fibers passed through them. We focus on the coordination problem of astrobots whose solutions shall be collision-free, fast execution, and complete in terms of the astrobots' convergence rates. We also illustrate the significant impact of astrobots assignments to observational targets on the quality of coordination solutions To present the current state of the field, we elaborate the open problems including next-generation astrophysical projects including 20,000 astrobots, and other fields, such as space debris tracking, in which astrobots may be potentially used ",https://doi.org/10.1109/MRA.2020.3044911,2210.02587v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Ultra-deep mid-IR survey of a lensing cluster,1970,"  We present the first results of mid-infrared (MIR) ultra-deep observations towards the lensing cluster Abell 2390 using the ISOCAM infrared camera on-board ESA's Infrared Space Observatory (ISO) satellite. They reveal a large number of luminous MIR sources. Optical and near-infrared (NIR) cross-identification suggests that almost all 15 microns sources and about half of the 7 microns are identified with distant lensed galaxies. Thanks to the gravitational amplification these sources constitute the faintest MIR sources detected. We confirm that the number counts derived at 15 microns show a clear excess of sources with respect to the predictions of a no-evolution model.   The possible extension of the NGST instrumentation from the near-IR (1-5 microns) to the thermal infrared, up to 20 microns (as suggested by the NGST task group report, October 1997) would permit study of this new population of dust-enshrouded AGN/starburst galaxies detected by ISOCAM, up to very high redshifts and with vastly improved spatial resolution. The existence of this population demonstrats that the discrimination of dust contributions, possible in the MIR, must be an important consideration in reaching an understanding of the Universe at high redshift. Therefore we stress that the access of NGST to the thermal infrared would increase tremendously its scientific potential to study the early universe. ",Kein DOI-Link verfügbar,astro-ph/9808131v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Evidence for tidal stripping of dark matter halos in massive   cluster-lenses,1970,"  In this letter, we present the results of our study of galaxy-galaxy lensing in massive cluster-lenses spanning $z = 0.17$ to 0.58, utilizing high-quality archival {\it Hubble Space Telescope} ({\it HST}) data. Local anisotropies in the shear maps are assumed to arise from dark matter substructure within these clusters. Associating the substructure with bright early-type cluster galaxies, we quantify the properties of typical $L^*$ cluster members in a statistical fashion. The fraction of total mass associated with individual galaxies within the inner regions of these clusters ranges from 10--20% implying that the bulk of the dark matter in massive lensing clusters is smoothly distributed. Looking at the properties of the cluster galaxies, we find strong evidence ($>3$-$\sigma$ significance) that a fiducial early-type $L^\ast$ galaxy in these clusters has a mass distribution that is tidally truncated compared to equivalent luminosity galaxies in the field. In fact, we exclude field galaxy scale dark halos for these cluster early-types at $>10$-$\sigma$ significance. We compare the tidal radii obtained from this lensing analysis with the central density of the cluster potentials and find a correlation which is in excellent agreement with theoretical expectations of tidal truncation: $\log [r_t*] \propto (-0.6\pm 0.2) \log [\rho_0]$. ",https://doi.org/10.1086/345399,astro-ph/0207049v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Multi-scale cluster lens mass mapping I. Strong Lensing modelling,1970,"  We propose a novel technique to refine the modelling of galaxy clusters mass distribution using gravitational lensing. The idea is to combine the strengths of both ""parametric"" and ""non-parametric"" methods to improve the quality of the fit. We develop a multi-scale model that allows sharper contrast in regions of higher density where the number of constraints is generally higher. Our model consists of (i) a multi-scale grid of radial basis functions with physically motivated profiles and (ii) a list of galaxy-scale potentials at the location of the cluster member galaxies. This arrangement of potentials of different sizes allows to reach a high resolution for the model with a minimum number of parameters. We apply our model to the well studied cluster Abell 1689. We estimate the quality of our mass reconstruction with a Bayesian MCMC sampler. For a selected subset of multiple images, we manage to halve the errors between the predicted and observed image positions compared to previous studies. This owes to the flexibility of multi-scale models at intermediate scale between cluster and galaxy scale. The software developed for this paper is part of the public lenstool package which can be found at www.oamp.fr/cosmology/lenstool. ",https://doi.org/10.1111/j.1365-2966.2009.14654.x,0901.3792v3,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Cosmological Constraints From Weak Lensing Peak Statistics With CFHT   Stripe 82 Survey,1970,"  We derived constraints on cosmological parameters using weak lensing peak statistics measured on the $\sim130~{\rm deg}^2$ of the Canada-France-Hawaii Telescope Stripe 82 Survey (CS82). This analysis demonstrates the feasibility of using peak statistics in cosmological studies. For our measurements, we considered peaks with signal-to-noise ratio in the range of $\nu=[3,6]$. For a flat $\Lambda$CDM model with only $(\Omega_{\rm m}, \sigma_8)$ as free parameters, we constrained the parameters of the following relation $\Sigma_8=\sigma_8(\Omega_{\rm m}/0.27)^{\alpha}$ to be: $\Sigma_8=0.82 \pm 0.03 $ and $\alpha=0.43\pm 0.02$. The $\alpha$ value found is considerably smaller than the one measured in two-point and three-point cosmic shear correlation analyses, showing a significant complement of peak statistics to standard weak lensing cosmological studies. The derived constraints on $(\Omega_{\rm m}, \sigma_8)$ are fully consistent with the ones from either WMAP9 or Planck. From the weak lensing peak abundances alone, we obtained marginalised mean values of $\Omega_{\rm m}=0.38^{+0.27}_{-0.24}$ and $\sigma_8=0.81\pm 0.26$. Finally, we also explored the potential of using weak lensing peak statistics to constrain the mass-concentration relation of dark matter halos simultaneously with cosmological parameters. ",https://doi.org/10.1093/mnras/stv784,1412.3683v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Systematic or Signal? How dark matter misalignments can bias strong   lensing models of galaxy clusters,1970,"  We explore how assuming that mass traces light in strong gravitational lensing models can lead to systematic errors in the predicted position of multiple images. Using a model based on the galaxy cluster MACSJ0416 (z = 0.397) from the Hubble Frontier Fields, we split each galactic halo into a baryonic and dark matter component. We then shift the dark matter halo such that it no longer aligns with the baryonic halo and investigate how this affects the resulting position of multiple images. We find for physically motivated misalignments in dark halo position, ellipticity, position angle and density profile, that multiple images can move on average by more than 0.2"" with individual images moving greater than 1"". We finally estimate the full error induced by assuming that light traces mass and find that this assumption leads to an expected RMS error of 0.5"", almost the entire error budget observed in the Frontier Fields. Given the large potential contribution from the assumption that light traces mass to the error budget in mass reconstructions, we predict that it should be possible to make a first significant detection and characterisation of dark halo misalignments in the Hubble Frontier Fields with strong lensing. Finally, we find that it may be possible to detect ~1kpc offsets between dark matter and baryons, the smoking gun for self-interacting dark matter, should the correct alignment of multiple images be observed. ",https://doi.org/10.1093/mnras/stw295,1601.06793v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Experimental evaluation of complete safe coordination of astrobots for   Sloan Digital Sky Survey V,1970,"  The data throughput of massive spectroscopic surveys in the course of each observation is directly coordinated with the number of optical fibers which reach their target. In this paper, we evaluate the safety and the performance of the astrobots coordination in SDSS-V by conducting various experimental and simulated tests. We illustrate that our strategy provides a complete coordination condition which depends on the operational characteristics of astrobots, their configurations, and their targets. Namely, a coordination method based on the notion of cooperative artificial potential fields is used to generate safe and complete trajectories for astrobots. Optimal target assignment further improves the performance of the used algorithm in terms of faster convergences and less oscillatory movements. Both random targets and galaxy catalog targets are employed to observe the coordination success of the algorithm in various target distributions. The proposed method is capable of handling all potential collisions in the course of coordination. Once the completeness condition is fulfilled according to initial configuration of astrobots and their targets, the algorithm reaches full convergence of astrobots. Should one assign targets to astrobots using efficient strategies, convergence time as well as the number of oscillations decrease in the course of coordination. Rare incomplete scenarios are simply resolved by trivial modifications of astrobots swarms' parameters. ",https://doi.org/10.1007/s10686-020-09687-4,2012.10656v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,PINION: Physics-informed neural network for accelerating radiative   transfer simulations for cosmic reionization,1970,"  With the advent of the Square Kilometre Array Observatory (SKAO), scientists will be able to directly observe the Epoch of Reionization by mapping the distribution of neutral hydrogen at different redshifts. While physically motivated results can be simulated with radiative transfer codes, these simulations are computationally expensive and can not readily produce the required scale and resolution simultaneously. Here we introduce the Physics-Informed neural Network for reIONization (PINION), which can accurately and swiftly predict the complete 4-D hydrogen fraction evolution from the smoothed gas and mass density fields from pre-computed N-body simulation. We trained PINION on the C$^2$-Ray simulation outputs and a physics constraint on the reionization chemistry equation is enforced. With only five redshift snapshots and a propagation mask as a simplistic approximation of the ionizing photon mean free path, PINION can accurately predict the entire reionization history between $z=6$ and $12$. We evaluate the accuracy of our predictions by analysing the dimensionless power spectra and morphology statistics estimations against C$^2$-Ray results. We show that while the network's predictions are in good agreement with simulation to redshift $z>7$, the network's accuracy suffers for $z<7$ primarily due to the oversimplified propagation mask. We motivate how PINION performance can be drastically improved and potentially generalized to large-scale simulations. ",https://doi.org/10.1093/mnras/stad615,2208.13803v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,An HST Lensing Survey of X-ray Luminous Galaxy Clusters: I. A383,1970,"  We analyse the mass distribution in the core of A383 (z=0.188), one of 12 X-ray luminous clusters at z~0.2 selected for a comprehensive and unbiased study of the mass distribution in massive clusters. Deep HST imaging reveals a wide variety of gravitationally lensed features in A383, including a giant arc formed from the strongly-lensed images of 2 background galaxies, 2 radial arcs, several multiply-imaged arcs and numerous arclets. Based upon the constraints from the various lensed features, we construct a detailed mass model for the central regions of the cluster, taking into account both the cluster-scale potential and perturbations from individual cluster galaxies. Keck spectroscopy of one component of the giant arc identifies it as a star-forming galaxy at z=1.01 and provides an accurate measurement of the cluster mass within the radius of the giant arc (65kpc) of (3.5+/-0.1)*10^13 Mo. Using the weak shear measured from our HST observations we extend our mass model to determine a mass of (1.8+/-0.2)*10^14 Mo within a radius of 250kpc. On smaller scales we employ the radial arcs as probes of the shape of the mass distribution in the cluster core (r<20kpc), and find that the mass profile is more peaked than a single NFW profile. The optical and X-ray properties of A383 indicate the presence of a central cooling flow, for which we derive a mass deposition rate of >200 Mo/yr. We also use the X-ray emission from A383 to obtain independent estimates of the total mass within projected radii of 65 and 250kpc: (4.0+/-1.4)*10^13 Mo and (1.2+/-0.5)*10^14 Mo, which are consistent with the lensing measurements. [Abridged] ",https://doi.org/10.1086/320557,astro-ph/0008315v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,COSMOS: 3D weak lensing and the growth of structure,1970,"  We present a three dimensional cosmic shear analysis of the Hubble Space Telescope COSMOS survey, the largest ever optical imaging program performed in space. We have measured the shapes of galaxies for the tell-tale distortions caused by weak gravitational lensing, and traced the growth of that signal as a function of redshift. Using both 2D and 3D analyses, we measure cosmological parameters Omega_m, the density of matter in the universe, and sigma_8, the normalization of the matter power spectrum. The introduction of redshift information tightens the constraints by a factor of three, and also reduces the relative sampling (or ""cosmic"") variance compared to recent surveys that may be larger but are only two dimensional. From the 3D analysis, we find sigma_8*(Omega_m/0.3)^0.44=0.866+^0.085_-0.068 at 68% confidence limits, including both statistical and potential systematic sources of error in the total budget. Indeed, the absolute calibration of shear measurement methods is now the dominant source of uncertainty. Assuming instead a baseline cosmology to fix the geometry of the universe, we have measured the growth of structure on both linear and non-linear physical scales. Our results thus demonstrate a proof of concept for tomographic analysis techniques that have been proposed for future weak lensing surveys by a dedicated wide-field telescope in space. ",https://doi.org/10.1086/516599,astro-ph/0701480v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Handbook for the GREAT08 Challenge: An image analysis competition for   cosmological lensing,1970,"  The GRavitational lEnsing Accuracy Testing 2008 (GREAT08) Challenge focuses on a problem that is of crucial importance for future observations in cosmology. The shapes of distant galaxies can be used to determine the properties of dark energy and the nature of gravity, because light from those galaxies is bent by gravity from the intervening dark matter. The observed galaxy images appear distorted, although only slightly, and their shapes must be precisely disentangled from the effects of pixelisation, convolution and noise. The worldwide gravitational lensing community has made significant progress in techniques to measure these distortions via the Shear TEsting Program (STEP). Via STEP, we have run challenges within our own community, and come to recognise that this particular image analysis problem is ideally matched to experts in statistical inference, inverse problems and computational learning. Thus, in order to continue the progress seen in recent years, we are seeking an infusion of new ideas from these communities. This document details the GREAT08 Challenge for potential participants. Please visit http://www.great08challenge.info for the latest information. ",https://doi.org/10.1214/08-AOAS222,0802.1214v3,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,A Hubble & Spitzer Space Telescope Survey for Gravitationally-Lensed   Galaxies: Further Evidence for a Significant Population of Low Luminosity   Galaxies beyond Redshift Seven,1970,"  We present the results of a systematic search for gravitationally-lensed continuum Lyman break `drop-outs' beyond a redshift 7 conducted via very deep imaging through six foreground clusters undertaken with the Hubble and Spitzer Space Telescopes. The survey has yielded 10 z-band and 2 J-band drop-out candidates to photometric limits of J_110~=26.2 AB (5sigma). Taking into account the magnifications afforded by our clusters (1-4 magnitudes), we probe the presence of z>7 sources to unlensed limits of J_{110}~=30 AB, fainter than those charted in the Hubble Ultradeep Field. To verify the fidelity of our candidates we conduct a number of tests for instrumental effects which would lead to spurious detections, and carefully evaluate the likelihood of foreground contamination by considering photometric uncertainties in the drop-out signature, the upper limits from stacked IRAC data and the statistics of multiply-imaged sources. Overall, we conclude that we can expect about half of our sample of z-band drop-outs are likely to be at high redshift. An ambitious infrared spectroscopic campaign undertaken with the NIRSPEC spectrograph at the WM Keck Observatory for seven of the most promising candidates failed to detect any Lyman-alpha emission highlighting the challenge of making further progress in this field. While the volume density of high redshift sources will likely remain uncertain until more powerful facilities are available, our data provides the first potentially interesting constraints on the UV luminosity function at z~=7.5 at intrinsically faint limits. We discuss the implications of our results in the context of the hypothesis that the bulk of the reionizing photons in the era 7<z<12 arise in low luminosity galaxies undetected by conventional surveys. ",https://doi.org/10.1086/591312,0803.4391v4,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,"Strong Lensing as a Probe of the Mass Distribution Beyond the Einstein   Radius. Mass & Light in SL2S J08544-0121, a Galaxy Group at z=0.35",1970,"  Precise modelling of strong lensing systems can be affected by external mass distributions, e.g. the group or cluster within which the lens is embedded. In this article, we propose to turn this limitation to our advantage and to use precise strong lensing modelling to probe external mass distributions surrounding the lens. We consider SL2S J08544-0121, a galaxy group at z=0.35 that contains a strong lensing system. A simple elliptical isothermal potential cannot reproduce satisfactorily the strong lensing constraints. We include an external mass perturbation corresponding to the group within which the lens is embedded. The lensing properties of this perturbation are parametrised by its total mass M and a smoothing scale s that quantifies the characteristic scale over which M is distributed. For a range of these parameters, we are able to reproduce accurately the observations. This suggests that light is a good tracer of mass. Interestingly, this also shows that a localised strong lensing analysis (on scales of ~10"") allows us to constrain global properties of the group as a whole (on scales of ~100). Indeed, we constrain the group mass-to-light ratio to be M/L=98+-27 (i band, solar units, not corrected for evolution) and s=20"" +- 9 (2sigma confidence level). We demonstrate that these strong lensing only constraints are due to the perturbed strong lensing configuration, where the main arc is located at ~5"" from the galaxy, whereas its counter-image is found at ~8"". To test independently our resulting strong lensing model, we pursue an independent weak lensing analysis of the group and find a mass-to-light ratio in the range 66-146 (1sigma confidence level). ",https://doi.org/10.1051/0004-6361/200912747,0906.4118v3,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The Type Ia Supernova Rate in Redshift 0.5--0.9 Galaxy Clusters,1970,"  Supernova (SN) rates are potentially powerful diagnostics of metal enrichment and SN physics, particularly in galaxy clusters with their deep, metal-retaining potentials and relatively simple star-formation histories. We have carried out a survey for supernovae (SNe) in galaxy clusters, at a redshift range 0.5<z<0.9, using the Advanced Camera for Surveys (ACS) on the Hubble Space Telescope. We reimaged a sample of 15 clusters that were previously imaged by ACS, thus obtaining two to three epochs per cluster, in which we discovered five likely cluster SNe, six possible cluster SNe Ia, two hostless SN candidates, and several background and foreground events. Keck spectra of the host galaxies were obtained to establish cluster membership. We conducted detailed efficiency simulations, and measured the stellar luminosities of the clusters using Subaru images. We derive a cluster SN rate of 0.35 SNuB +0.17/-0.12 (statistical) \pm0.13 (classification) \pm0.01 (systematic) [where SNuB = SNe (100 yr 10^10 L_B_sun)^-1] and 0.112 SNuM +0.055/-0.039 (statistical) \pm0.042 (classification) \pm0.005 (systematic) [where SNuM = SNe (100 yr 10^10 M_sun)^-1]. As in previous measurements of cluster SN rates, the uncertainties are dominated by small-number statistics. The SN rate in this redshift bin is consistent with the SN rate in clusters at lower redshifts (to within the uncertainties), and shows that there is, at most, only a slight increase of cluster SN rate with increasing redshift. The low and fairly constant SN Ia rate out to z~1 implies that the bulk of the iron mass in clusters was already in place by z~1. The recently observed doubling of iron abundances in the intracluster medium between z=1 and 0, if real, is likely the result of redistribution of existing iron, rather than new production of iron. ",https://doi.org/10.1088/0004-637X/718/2/876,1006.3757v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Hubble Frontier Fields : A High Precision Strong Lensing Analysis of   Galaxy Cluster MACSJ0416.1-2403 using ~200 Multiple Images,1970,"  We present a high-precision mass model of the galaxy cluster MACSJ0416.1-2403, based on a strong-gravitational-lensing analysis of the recently acquired Hubble Space Telescope Frontier Fields (HFF) imaging data. Taking advantage of the unprecedented depth provided by HST/ACS observations in three passbands, we identify 51 new multiply imaged galaxies, quadrupling the previous census and bringing the grand total to 68, comprising 194 individual lensed images. Having selected a subset of the 57 most securely identified multiply imaged galaxies, we use the Lenstool software package to constrain a lens model comprised of two cluster-scale dark-matter halos and 98 galaxy-scale halos. Our best-fit model predicts image positions with an $RMS$ error of 0.68'', which constitutes an improvement of almost a factor of two over previous, pre-HFF models of this cluster. We find the total projected mass inside a 200~kpc aperture to be $(1.60\pm0.01)\times 10^{14}\ M_\odot$, a measurement that offers a three-fold improvement in precision, reaching the percent level for the first time in any cluster. Finally, we quantify the increase in precision of the derived gravitational magnification of high-redshift galaxies and find an improvement by a factor of $\sim$2.5 in the statistical uncertainty. Our findings impressively confirm that HFF imaging has indeed opened the domain of high-precision mass measurements for massive clusters of galaxies. ",https://doi.org/10.1093/mnras/stu1355,1405.3582v3,Yes,impressively(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,SPIDERS: the spectroscopic follow-up of X-ray selected clusters of   galaxies in SDSS-IV,1970,"  SPIDERS (The SPectroscopic IDentification of eROSITA Sources) is a program dedicated to the homogeneous and complete spectroscopic follow-up of X-ray AGN and galaxy clusters over a large area ($\sim$7500 deg$^2$) of the extragalactic sky. SPIDERS is part of the SDSS-IV project, together with the Extended Baryon Oscillation Spectroscopic Survey (eBOSS) and the Time-Domain Spectroscopic Survey (TDSS). This paper describes the largest project within SPIDERS before the launch of eROSITA: an optical spectroscopic survey of X-ray selected, massive ($\sim 10^{14}$ to $10^{15}~M_{\odot}$) galaxy clusters discovered in ROSAT and XMM-Newton imaging. The immediate aim is to determine precise ($\Delta_z \sim 0.001$) redshifts for 4,000-5,000 of these systems out to $z \sim 0.6$. The scientific goal of the program is precision cosmology, using clusters as probes of large-scale structure in the expanding Universe. We present the cluster samples, target selection algorithms and observation strategies. We demonstrate the efficiency of selecting targets using a combination of SDSS imaging data, a robust red-sequence finder and a dedicated prioritization scheme. We describe a set of algorithms and work-flow developed to collate spectra and assign cluster membership, and to deliver catalogues of spectroscopically confirmed clusters. We discuss the relevance of line-of-sight velocity dispersion estimators for the richer systems. We illustrate our techniques by constructing a catalogue of 230 spectroscopically validated clusters ($0.031 < z < 0.658$), found in pilot observations. We discuss two potential science applications of the SPIDERS sample: the study of the X-ray luminosity-velocity dispersion ($L_X-\sigma$) relation and the building of stacked phase-space diagrams. ",https://doi.org/10.1093/mnras/stw2214,1608.08963v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Quasi-Stellar Objects acting as potential Strong Gravitational Lenses in   the SDSS-III BOSS survey,1970,"  We present a sample of 12 Quasi-Stellar Objects (QSOs) potentially acting as strong gravitational lenses on background Emission Line Galaxies (ELG) or Lyman-$\alpha$ Emitters (LAEs) selected selected through a systematic search of the 297301 QSOs in the Sloan Digital Sky Survey (SDSS)-III Data Release 12. Candidates are identified by looking for compound spectra, where emission lines at a redshift larger than that of the quasar can be identified in the residuals after a QSO spectral template is subtracted from the observed spectra. The narrow diameter of BOSS fibers (2"") then ensures that the object responsible for the additional emission lines must lie close to the line of sight of the QSO and hence provides a high probability of lensing. Among the 12 candidates identified, 9 have definite evidence for the presence of a background ELG identified by at least 4 higher-redshift nebular emission lines. The remaining $3$ probable candidates present a strong asymmetrical emission line attributed to a background Lyman-$\alpha$ emitter (LAE). The QSO-ELG (QSO-LAE) lens candidates have QSO lens redshifts in the range $0.24\lesssim z_{QSO} \lesssim 0.66$ ($0.75 \lesssim z_{QSO} \lesssim 1.23$ ) and background galaxy redshifts in the range $0.48 \lesssim z_{S,ELG} \lesssim 0.94$ ($2.17 \lesssim z_{S,LAE} \lesssim 4.48$). We show that the algorithmic search is complete at >90% for QSO-ELG systems, whereas it falls at 40-60% for QSO-LAE, depending on the redshift of the source. Upon confirmation of the lensing nature of the systems, this sample may quadruple the number of known QSOs acting as strong lenses. We have determined the completeness of our search, which allows future studies to compute lensing probabilities of galaxies by QSOs and differentiate between different QSO models. (Abridged) ",https://doi.org/10.1051/0004-6361/201834978,1711.01184v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,High Performance Computing for gravitational lens modeling: single vs   double precision on GPUs and CPUs,1970,"  Strong gravitational lensing is a powerful probe of cosmology and the dark matter distribution. Efficient lensing software is already a necessity to fully use its potential and the performance demands will only increase with the upcoming generation of telescopes. In this paper, we study the possible impact of High Performance Computing techniques on a performance-critical part of the widely used lens modeling software LENSTOOL. We implement the algorithm once as a highly optimized CPU version and once with graphics card acceleration for a simple parametric lens model. In addition, we study the impact of finite machine precision on the lensing algorithm. While double precision is the default choice for scientific applications, we find that single precision can be sufficiently accurate for our purposes and lead to a big speedup. Therefore we develop and present a mixed precision algorithm which only uses double precision when necessary. We measure the performance of the different implementations and find that the use of High Performance Computing Techniques dramatically improves the code performance both on CPUs and GPUs. Compared to the current LENSTOOL implementation on 12 CPU cores, we obtain speedup factors of up to 170. We achieve this optimal performance by using our mixed precision algorithm on a high-end GPU which is common in modern supercomputers. We also show that these techniques reduce the energy consumption by up to 98%. Furthermore, we demonstrate that a highly competitive speedup can be reached with consumer GPUs. While they are an order of magnitude cheaper than the high-end graphics cards, they are rarely used for scientific computations due to their low double precision performance. Our mixed precision algorithm unlocks their full potential. The consumer GPU delivers a speedup which is only a factor of four lower than the best speedup achieved by a high-end GPU. ",Kein DOI-Link verfügbar,1902.03252v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Model BOSS & eBOSS Luminous Red Galaxies at 0.2 < z < 1.0 using SubHalo   Abundance Matching with 3 parameters,1970,"  SubHalo Abundance Matching (SHAM) is an empirical method for constructing galaxy catalogues based on high-resolution $N$-body simulations. We apply SHAM on the UNIT simulation to simulate SDSS BOSS/eBOSS Luminous Red Galaxies (LRGs) within a wide redshift range of $0.2 < z < 1.0$. Besides the typical SHAM scatter parameter $\sigma$, we include $v_{\rm smear}$ and $V_{\rm ceil}$ to take into account the redshift uncertainty and the galaxy incompleteness respectively. These two additional parameters are critical for reproducing the observed 2PCF multipoles on 5--25$\,h^{-1}\,{\rm Mpc}$. The redshift uncertainties obtained from the best-fitting $v_{\rm smear}$ agree with those measured from repeat observations for all SDSS LRGs except for the LOWZ sample. We explore several potential systematics but none of them can explain the discrepancy found in LOWZ. Our explanation is that the LOWZ galaxies might contain another type of galaxies which needs to be treated differently. The evolution of the measured $\sigma$ and $V_{\rm ceil}$ also reveals that the incompleteness of eBOSS galaxies decreases with the redshift. This is the consequence of the magnitude lower limit applied in eBOSS LRG target selection. Our SHAM also set upper limits for the intrinsic scatter of the galaxy--halo relation given a complete galaxy sample: $\sigma_{\rm int}<0.31$ for LOWZ at $0.2<z<0.33$, $\sigma_{\rm int}<0.36$ for LOWZ at $0.33<z<0.43$, and $\sigma_{\rm int}<0.46$ for CMASS at $0.43<z<0.51$. The projected 2PCFs of our SHAM galaxies also agree with the observational ones on the 2PCF fitting range. ",https://doi.org/10.1093/mnras/stac2176,2203.11069v3,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,An unbiased method of measuring the ratio of two data sets,1970,"  In certain cases of astronomical data analysis, the meaningful physical quantity to extract is the ratio $R$ between two data sets. Examples include the lensing ratio, the interloper rate in spectroscopic redshift samples, the decay rate of gravitational potential and $E_G$ to test gravity. However, simply taking the ratio of the two data sets is biased, since it renders (even statistical) errors in the denominator into systematic errors in $R$. Furthermore, it is not optimal in minimizing statistical errors of $R$. Based on Bayesian analysis and the usual assumption of Gaussian error in the data, we derive an analytical expression of the posterior PDF $P(R)$. This result enables fast and unbiased $R$ measurement, with minimal statistical errors. Furthermore, it relies on no underlying model other than the proportionality relation between the two data sets. Even more generally, it applies to the cases where the proportionality relation holds for the underlying physics/statistics instead of the two data sets directly. It also applies to the case of multiple ratios ($R\rightarrow {\bf R}=(R_1,R_2,\cdots)$). We take the lensing ratio as an example to demonstrate our method. We take lenses as DESI imaging survey galaxies, and sources as DECaLS cosmic shear and \emph{Planck} CMB lensing. We restrict the analysis to the ratio between CMB lensing and cosmic shear. The resulting $P(R)$, for multiple lens-shear pairs, are all nearly Gaussian. The S/N of measured $R$ ranges from $4.9$ to $8.4$. We perform several tests to verify the robustness of the above result. ",https://doi.org/10.3847/1538-4365/acda2a,2210.13717v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Cosmic void exclusion models and their impact on the distance scale   measurements from large scale structure,1970,"  Baryonic Acoustic Oscillations (BAOs) studies based on the clustering of voids and matter tracers provide important constraints on cosmological parameters related to the expansion of the Universe. However, modelling the void exclusion effect is an important challenge for fully exploiting the potential of this kind of analyses. We thus develop two numerical methods to describe the clustering of cosmic voids. Neither model requires additional cosmological information beyond that assumed within the galaxy de-wiggled model. The models consist in power spectra whose performance we assess in comparison to a parabolic model on Patchy cubic and light-cone mocks. Moreover, we test their robustness against systematic effects and the reconstruction technique. The void model power spectra and the parabolic model with a fixed parameter provide strongly correlated values for the Alcock-Paczynski ($\alpha$) parameter, for boxes and light-cones likewise. The resulting $\alpha$ values -- for all three models -- are unbiased and their uncertainties are correctly estimated. However, the numerical models show less variation with the fitting range compared to the parabolic one. The Bayesian evidence suggests that the numerical techniques are often favoured compared to the parabolic model. Moreover, the void model power spectra computed on boxes can describe the void clustering from light-cones as well as from boxes. The same void model power spectra can be used for the study of pre- and post-reconstructed data-sets. Lastly, the two numerical techniques are resilient against the studied systematic effects. Consequently, using either of the two new void models, one can more robustly measure cosmological parameters. ",https://doi.org/10.1093/mnras/stad813,2211.04328v2,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,DESI and DECaLS (D&D): galaxy-galaxy lensing measurements with 1% survey   and its forecast,1970,"  The shear measurement from DECaLS (Dark Energy Camera Legacy Survey) provides an excellent opportunity for galaxy-galaxy lensing study with DESI (Dark Energy Spectroscopic Instrument) galaxies, given the large ($\sim 9000$ deg$^2$) sky overlap. We explore this potential by combining the DESI 1\% survey and DECaLS DR8. With $\sim 106$ deg$^2$ sky overlap, we achieve significant detection of galaxy-galaxy lensing for BGS and LRG as lenses. Scaled to the full BGS sample, we expect the statistical errors to improve from $18(12)\%$ to a promising level of $2(1.3)\%$ at $\theta>8^{'}(<8^{'})$. This brings stronger requirements for future systematics control. To fully realize such potential, we need to control the residual multiplicative shear bias $|m|<0.01$ and the bias in the mean redshift $|\Delta z|<0.015$. We also expect significant detection of galaxy-galaxy lensing with DESI LRG/ELG full samples as lenses, and cosmic magnification of ELG through cross-correlation with low-redshift DECaLS shear. {If such systematical error control can be achieved,} we find the advantages of DECaLS, comparing with KiDS (Kilo Degree Survey) and HSC (Hyper-Suprime Cam), are at low redshift, large-scale, and in measuring the shear-ratio (to $\sigma_R\sim 0.04$) and cosmic magnification. ",https://doi.org/10.1093/mnras/stad2221,2301.13434v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,CURLING - I. The Influence of Point-like Image Approximation on the   Outcomes of Cluster Strong Lens Modeling,1970,"  Cluster-scale strong lensing is a powerful tool for exploring the properties of dark matter and constraining cosmological models. However, due to the complex parameter space, pixelized strong lens modeling in galaxy clusters is computationally expensive, leading to the point-source approximation of strongly lensed extended images, potentially introducing systematic biases. Herein, as the first paper of the ClUsteR strong Lens modelIng for the Next-Generation observations (CURLING) program, we use lensing ray-tracing simulations to quantify the biases and uncertainties arising from the point-like image approximation for JWST-like observations. Our results indicate that the approximation works well for reconstructing the total cluster mass distribution, but can bias the magnification measurements near critical curves and the constraints on the cosmological parameters, the total matter density of the Universe $\Omega_{\rm m}$, and dark energy equation of state parameter $w$. To mitigate the biases, we propose incorporating the extended surface brightness distribution of lensed sources into the modeling. This approach reduces the bias in magnification from 46.2 per cent to 0.09 per cent for $\mu \sim 1000$. Furthermore, the median values of cosmological parameters align more closely with the fiducial model. In addition to the improved accuracy, we also demonstrate that the constraining power can be substantially enhanced. In conclusion, it is necessary to model cluster-scale strong lenses with pixelized multiple images, especially for estimating the intrinsic luminosity of highly magnified sources and accurate cosmography in the era of high-precision observations. ",Kein DOI-Link verfügbar,2405.03135v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,Strong gravitational lensing probes of the particle nature of dark   matter,1970,"  There is a vast menagerie of plausible candidates for the constituents of dark matter, both within and beyond extensions of the Standard Model of particle physics. Each of these candidates may have scattering (and other) cross section properties that are consistent with the dark matter abundance, BBN, and the most scales in the matter power spectrum; but which may have vastly different behavior at sub-galactic ""cutoff"" scales, below which dark matter density fluctuations are smoothed out. The only way to quantitatively measure the power spectrum behavior at sub-galactic scales at distances beyond the local universe, and indeed over cosmic time, is through probes available in multiply imaged strong gravitational lenses. Gravitational potential perturbations by dark matter substructure encode information in the observed relative magnifications, positions, and time delays in a strong lens. Each of these is sensitive to a different moment of the substructure mass function and to different effective mass ranges of the substructure. The time delay perturbations, in particular, are proving to be largely immune to the degeneracies and systematic uncertainties that have impacted exploitation of strong lenses for such studies. There is great potential for a coordinated theoretical and observational effort to enable a sophisticated exploitation of strong gravitational lenses as direct probes of dark matter properties. This opportunity motivates this white paper, and drives the need for: a) strong support of the theoretical work necessary to understand all astrophysical consequences for different dark matter candidates; and b) tailored observational campaigns, and even a fully dedicated mission, to obtain the requisite data. ",Kein DOI-Link verfügbar,0902.3219v1,Yes,potent(2)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,"A Spectroscopic Road Map for Cosmic Frontier: DESI, DESI-II, Stage-5",1970,"  In this white paper, we present an experimental road map for spectroscopic experiments beyond DESI. DESI will be a transformative cosmological survey in the 2020s, mapping 40 million galaxies and quasars and capturing a significant fraction of the available linear modes up to z=1.2. DESI-II will pilot observations of galaxies both at much higher densities and extending to higher redshifts. A Stage-5 experiment would build out those high-density and high-redshift observations, mapping hundreds of millions of stars and galaxies in three dimensions, to address the problems of inflation, dark energy, light relativistic species, and dark matter. These spectroscopic data will also complement the next generation of weak lensing, line intensity mapping and CMB experiments and allow them to reach their full potential. ",Kein DOI-Link verfügbar,2209.03585v1,Yes,potent(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The MegaMapper: A Stage-5 Spectroscopic Instrument Concept for the Study   of Inflation and Dark Energy,1970,"  In this white paper, we present the MegaMapper concept. The MegaMapper is a proposed ground-based experiment to measure Inflation parameters and Dark Energy from galaxy redshifts at $2<z<5$. In order to achieve path-breaking results with a mid-scale investment, the MegaMapper combines existing technologies for critical path elements and pushes innovative development in other design areas. To this aim, we envision a 6.5-m Magellan-like telescope, with a newly designed wide field, coupled with DESI spectrographs, and small-pitch robots to achieve multiplexing of at least 26,000. This will match the expected achievable target density in the redshift range of interest and provide a 10x capability over the existing state-of the art, without a 10x increase in project budget. ",Kein DOI-Link verfügbar,2209.04322v1,Yes,innovative(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The Fourteenth Data Release of the Sloan Digital Sky Survey: First   Spectroscopic Data from the extended Baryon Oscillation Spectroscopic Survey   and from the second phase of the Apache Point Observatory Galactic Evolution   Experiment,1970,"  The fourth generation of the Sloan Digital Sky Survey (SDSS-IV) has been in operation since July 2014. This paper describes the second data release from this phase, and the fourteenth from SDSS overall (making this, Data Release Fourteen or DR14). This release makes public data taken by SDSS-IV in its first two years of operation (July 2014-2016). Like all previous SDSS releases, DR14 is cumulative, including the most recent reductions and calibrations of all data taken by SDSS since the first phase began operations in 2000. New in DR14 is the first public release of data from the extended Baryon Oscillation Spectroscopic Survey (eBOSS); the first data from the second phase of the Apache Point Observatory (APO) Galactic Evolution Experiment (APOGEE-2), including stellar parameter estimates from an innovative data driven machine learning algorithm known as ""The Cannon""; and almost twice as many data cubes from the Mapping Nearby Galaxies at APO (MaNGA) survey as were in the previous release (N = 2812 in total). This paper describes the location and format of the publicly available data from SDSS-IV surveys. We provide references to the important technical papers describing how these data have been taken (both targeting and observation details) and processed for scientific use. The SDSS website (www.sdss.org) has been updated for this release, and provides links to data downloads, as well as tutorials and examples of data use. SDSS-IV is planning to continue to collect astronomical data until 2020, and will be followed by SDSS-V. ",https://doi.org/10.3847/1538-4365/aa9e8a,1707.09322v3,Yes,innovative(1)
0000-0002-4616-4989,Jean-Paul Kneib,EPFL,The Sixteenth Data Release of the Sloan Digital Sky Surveys: First   Release from the APOGEE-2 Southern Survey and Full Release of eBOSS Spectra,1970,"  This paper documents the sixteenth data release (DR16) from the Sloan Digital Sky Surveys; the fourth and penultimate from the fourth phase (SDSS-IV). This is the first release of data from the southern hemisphere survey of the Apache Point Observatory Galactic Evolution Experiment 2 (APOGEE-2); new data from APOGEE-2 North are also included. DR16 is also notable as the final data release for the main cosmological program of the Extended Baryon Oscillation Spectroscopic Survey (eBOSS), and all raw and reduced spectra from that project are released here. DR16 also includes all the data from the Time Domain Spectroscopic Survey (TDSS) and new data from the SPectroscopic IDentification of ERosita Survey (SPIDERS) programs, both of which were co-observed on eBOSS plates. DR16 has no new data from the Mapping Nearby Galaxies at Apache Point Observatory (MaNGA) survey (or the MaNGA Stellar Library ""MaStar""). We also preview future SDSS-V operations (due to start in 2020), and summarize plans for the final SDSS-IV data release (DR17). ",https://doi.org/10.3847/1538-4365/ab929e,1912.02905v2,Yes,notable(1)
0000-0002-3223-6320,Adélie Garin,EPFL,A Lattice-Theoretic Perspective on the Persistence Map,1970,"  We provide a naturally isomorphic description of the persistence map from merge trees to barcodes in terms of a monotone map from the partition lattice to the subset lattice. Our description is local, which offers the potential to speed up inverse computations, and brings classical tools in combinatorics to bear on an active area of research in topological data analysis (TDA). ",Kein DOI-Link verfügbar,2203.00643v1,Yes,potent(1)
0009-0001-9739-8849,Luca Arnaboldi,EPFL,Quantitative Analysis of DoS Attacks and Client Puzzles in IoT Systems,1970,"  Denial of Service (DoS) attacks constitute a major security threat to today's Internet. This challenge is especially pertinent to the Internet of Things (IoT) as devices have less computing power, memory and security mechanisms to mitigate DoS attacks. This paper presents a model that mimics the unique characteristics of a network of IoT devices, including components of the system implementing `Crypto Puzzles' - a DoS mitigation technique. We created an imitation of a DoS attack on the system, and conducted a quantitative analysis to simulate the impact such an attack may potentially exert upon the system, assessing the trade off between security and throughput in the IoT system. We model this through stochastic model checking in PRISM and provide evidence that supports this as a valuable method to compare the efficiency of different implementations of IoT systems, exemplified by a case study. ",https://doi.org/10.1007/978-3-319-68063-7_16,1710.11021v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Mathematical Optimization of Resolution Improvement in Structured Light   data by Periodic Scanning Motion: Application for Feedback during Lunar   Landing,1970,"  This research explores the enhancement of lunar landing precision through an advanced structured light system, integrating machine learning, Iterative Learning Control (ILC) and Structured Illumination Microscopy (SIM) techniques. By employing Moire fringe patterns for high-precision scanning maneuvers, the study addresses the limitations of conventional structured light systems. A nonlinear mathematical optimization model is developed to refine the world model, optimizing oscillation frequency and amplitude to improve resolution. The findings suggest that this approach can double the conventional resolution, promising significant advancements in the accuracy of lunar landings, with potential real-time application. ",Kein DOI-Link verfügbar,2408.06628v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Inductive Cognitive Diagnosis for Fast Student Learning in Web-Based   Online Intelligent Education Systems,1970,"  Cognitive diagnosis aims to gauge students' mastery levels based on their response logs. Serving as a pivotal module in web-based online intelligent education systems (WOIESs), it plays an upstream and fundamental role in downstream tasks like learning item recommendation and computerized adaptive testing. WOIESs are open learning environment where numerous new students constantly register and complete exercises. In WOIESs, efficient cognitive diagnosis is crucial to fast feedback and accelerating student learning. However, the existing cognitive diagnosis methods always employ intrinsically transductive student-specific embeddings, which become slow and costly due to retraining when dealing with new students who are unseen during training. To this end, this paper proposes an inductive cognitive diagnosis model (ICDM) for fast new students' mastery levels inference in WOIESs. Specifically, in ICDM, we propose a novel student-centered graph (SCG). Rather than inferring mastery levels through updating student-specific embedding, we derive the inductive mastery levels as the aggregated outcomes of students' neighbors in SCG. Namely, SCG enables to shift the task from finding the most suitable student-specific embedding that fits the response logs to finding the most suitable representations for different node types in SCG, and the latter is more efficient since it no longer requires retraining. To obtain this representation, ICDM consists of a construction-aggregation-generation-transformation process to learn the final representation of students, exercises and concepts. Extensive experiments across real-world datasets show that, compared with the existing cognitive diagnosis methods that are always transductive, ICDM is much more faster while maintains the competitive inference performance for new students. ",https://doi.org/10.1145/3589334.3645589,2404.11290v1,Yes,pivotal(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Autler-Townes doublet in single-photon Rydberg spectra of Cesium atomic   vapor with a 319 nm UV laser,1970,"  We demonstrate the single-photon excitation spectra of cesium Rydberg atoms by means of a Doppler-free purely all-optical detection with a room-temperature vapor cell and a 319 nm ultra-violet (UV) laser. We excite atoms directly from 6S1/2 ground state to 71P3/2 Rydberg state with a narrow-linewidth 319 nm UV laser. The detection of Rydberg states is performed by monitoring the absorption of an 852 nm probe beam in a V-type three-level system. With a strong coupling light, we observe the Autler-Townes doublet and investigate experimentally the dependence of the separation and linewidth on the coupling intensity, which is consistent with the prediction based on the dressed state theory. We further investigate the Rydberg spectra with an external magnetic field. The existence of non-degenerate Zeeman sub-levels results in the broadening and shift of the spectra. It has potential application in sensing magnetic field. ",https://doi.org/10.1007/s00340-019-7151-x,1806.00169v3,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Towards implementation of a magic optical-dipole trap for confining   ground-state and Rydberg-state cesium cold atoms,1970,"  Long ground-Rydberg coherence lifetime is interesting for implementing high-fidelity quantum logic gates, many-body physics, and other quantum information protocols. However, the potential formed by a conventional far-off-resonance red-detuned optical-dipole trap (ODT) is usually repulsive for Rydberg atoms, which will result in fast atom loss and low repetition rate of the experimental sequence. These issues can be addressed by a magic ODT. We performed the calculation of ODT's magic detuning for confinement of cesium ground state and Rydberg state with the same potential well. We used a sum-over-states method to calculate the dynamic polarizabilities of $6S_{1/2}$ ground state and highly-excited ($nS_{1/2}$ and $nP_{3/2}$) Rydberg state of cesium atoms, and identify corresponding magic detuning for optical wavelengths in the range of $850 - 2000$ nm. We estimated the trapping lifetime of cesium Rydberg atoms confined in the magic ODT by including different dissipative mechanisms. Furthermore, we have experimentally realized an 1879.43-nm single-frequency laser system with a watt-level output power for setting up the magic ODT for $6S_{1/2}$ ground-state and $84P_{3/2}$ Rydberg-state cesium cold atoms. ",https://doi.org/10.1088/1361-6455/ab91de,2001.05698v2,Yes,potent(2)
0000-0002-8947-1954,Shuo Liu,EPFL,Training variational quantum algorithms with random gate activation,1970,"  Variational quantum algorithms (VQAs) hold great potentials for near-term applications and are promising to achieve quantum advantage on practical tasks. However, VQAs suffer from severe barren plateau problem as well as have a large probability of being trapped in local minima. In this Letter, we propose a novel training algorithm with random quantum gate activation for VQAs to efficiently address these two issues. This new algorithm processes effectively much fewer training parameters than the conventional plain optimization strategy, which efficiently mitigates barren plateaus with the same expressive capability. Additionally, by randomly adding two-qubit gates to the circuit ansatz, the optimization trajectories can escape from local minima and reach the global minimum more frequently due to more sources of randomness. In real quantum experiments, the new training algorithm can also reduce the quantum computational resources required and be more quantum noise resilient. We apply our training algorithm to solve variational quantum simulation problems for ground states and present convincing results that showcase the advantages of our novel strategy where better performance is achieved by the combination of mitigating barren plateaus, escaping from local minima, and reducing the effect of quantum noises. We further propose that the entanglement phase transition could be one underlying reason why our RA training is so effective. ",https://doi.org/10.1103/PhysRevResearch.5.L032040,2303.08154v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Multistage linguistic conditioning of convolutional layers for speech   emotion recognition,1970,"  In this contribution, we investigate the effectiveness of deep fusion of text and audio features for categorical and dimensional speech emotion recognition (SER). We propose a novel, multistage fusion method where the two information streams are integrated in several layers of a deep neural network (DNN), and contrast it with a single-stage one where the streams are merged in a single point. Both methods depend on extracting summary linguistic embeddings from a pre-trained BERT model, and conditioning one or more intermediate representations of a convolutional model operating on log-Mel spectrograms. Experiments on the MSP-Podcast and IEMOCAP datasets demonstrate that the two fusion methods clearly outperform a shallow (late) fusion baseline and their unimodal constituents, both in terms of quantitative performance and qualitative behaviour. Overall, our multistage fusion shows better quantitative performance, surpassing alternatives on most of our evaluations. This illustrates the potential of multistage fusion in better assimilating text and audio information. ",https://doi.org/10.3389/fcomp.2023.1072479,2110.06650v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,CloudChain: A Cloud Blockchain Using Shared Memory Consensus and RDMA,1970,"  Blockchain technologies can enable secure computing environments among mistrusting parties. Permissioned blockchains are particularly enlightened by companies, enterprises, and government agencies due to their efficiency, customizability, and governance-friendly features. Obviously, seamlessly fusing blockchain and cloud computing can significantly benefit permissioned blockchains; nevertheless, most blockchains implemented on clouds are originally designed for loosely-coupled networks where nodes communicate asynchronously, failing to take advantages of the closely-coupled nature of cloud servers. In this paper, we propose an innovative cloud-oriented blockchain -- CloudChain, which is a modularized three-layer system composed of the network layer, consensus layer, and blockchain layer. CloudChain is based on a shared-memory model where nodes communicate synchronously by direct memory accesses. We realize the shared-memory model with the Remote Direct Memory Access technology, based on which we propose a shared-memory consensus algorithm to ensure presistence and liveness, the two crucial blockchain security properties countering Byzantine nodes. We also implement a CloudChain prototype based on a RoCEv2-based testbed to experimentally validate our design, and the results verify the feasibility and efficiency of CloudChain. ",Kein DOI-Link verfügbar,2106.04122v1,Yes,innovative(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Sensor Misalignment-tolerant AUV Navigation with Passive DoA and Doppler   Measurements,1970,"  We present a sensor misalignment-tolerant AUV navigation method that leverages measurements from an acoustic array and dead reckoned information. Recent studies have demonstrated the potential use of passive acoustic Direction of Arrival (DoA) measurements for AUV navigation without requiring ranging measurements. However, the sensor misalignment between the acoustic array and the attitude sensor was not accounted for. Such misalignment may deteriorate the navigation accuracy. This paper proposes a novel approach that allows simultaneous AUV navigation, beacon localization, and sensor alignment. An Unscented Kalman Filter (UKF) that enables the necessary calculations to be completed at an affordable computational load is developed. A Nonlinear Least Squares (NLS)-based technique is employed to find an initial solution for beacon localization and sensor alignment as early as possible using a short-term window of measurements. Experimental results demonstrate the performance of the proposed method. ",Kein DOI-Link verfügbar,2402.07218v1,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,LVLM-eHub: A Comprehensive Evaluation Benchmark for Large   Vision-Language Models,1970,"  Large Vision-Language Models (LVLMs) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building a LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $47$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDEr for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective pipeline for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. Our LVLM-eHub will be available at https://github.com/OpenGVLab/Multi-Modality-Arena ",Kein DOI-Link verfügbar,2306.09265v1,Yes,innovative(2)
0000-0002-8947-1954,Shuo Liu,EPFL,ConvBench: A Multi-Turn Conversation Evaluation Benchmark with   Hierarchical Capability for Large Vision-Language Models,1970,"  This paper presents ConvBench, a novel multi-turn conversation evaluation benchmark tailored for Large Vision-Language Models (LVLMs). Unlike existing benchmarks that assess individual capabilities in single-turn dialogues, ConvBench adopts a three-level multimodal capability hierarchy, mimicking human cognitive processes by stacking up perception, reasoning, and creativity. Each level focuses on a distinct capability, mirroring the cognitive progression from basic perception to logical reasoning and ultimately to advanced creativity. ConvBench comprises 577 meticulously curated multi-turn conversations encompassing 215 tasks reflective of real-world demands. Automatic evaluations quantify response performance at each turn and overall conversation level. Leveraging the capability hierarchy, ConvBench enables precise attribution of conversation mistakes to specific levels. Experimental results reveal a performance gap between multi-modal models, including GPT4-V, and human performance in multi-turn conversations. Additionally, weak fine-grained perception in multi-modal models contributes to reasoning and creation failures. ConvBench serves as a catalyst for further research aimed at enhancing visual dialogues. ",Kein DOI-Link verfügbar,2403.20194v2,Yes,"meticulous(1), meticulously(1)"
0000-0002-8947-1954,Shuo Liu,EPFL,"An Early Study on Intelligent Analysis of Speech under COVID-19:   Severity, Sleep Quality, Fatigue, and Anxiety",1970,"  The COVID-19 outbreak was announced as a global pandemic by the World Health Organisation in March 2020 and has affected a growing number of people in the past few weeks. In this context, advanced artificial intelligence techniques are brought to the fore in responding to fight against and reduce the impact of this global health crisis. In this study, we focus on developing some potential use-cases of intelligent speech analysis for COVID-19 diagnosed patients. In particular, by analysing speech recordings from these patients, we construct audio-only-based models to automatically categorise the health state of patients from four aspects, including the severity of illness, sleep quality, fatigue, and anxiety. For this purpose, two established acoustic feature sets and support vector machines are utilised. Our experiments show that an average accuracy of .69 obtained estimating the severity of illness, which is derived from the number of days in hospitalisation. We hope that this study can foster an extremely fast, low-cost, and convenient way to automatically detect the COVID-19 disease. ",Kein DOI-Link verfügbar,2005.00096v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,SOS-1K: A Fine-grained Suicide Risk Classification Dataset for Chinese   Social Media Analysis,1970,"  In the social media, users frequently express personal emotions, a subset of which may indicate potential suicidal tendencies. The implicit and varied forms of expression in internet language complicate accurate and rapid identification of suicidal intent on social media, thus creating challenges for timely intervention efforts. The development of deep learning models for suicide risk detection is a promising solution, but there is a notable lack of relevant datasets, especially in the Chinese context. To address this gap, this study presents a Chinese social media dataset designed for fine-grained suicide risk classification, focusing on indicators such as expressions of suicide intent, methods of suicide, and urgency of timing. Seven pre-trained models were evaluated in two tasks: high and low suicide risk, and fine-grained suicide risk classification on a level of 0 to 10. In our experiments, deep learning models show good performance in distinguishing between high and low suicide risk, with the best model achieving an F1 score of 88.39%. However, the results for fine-grained suicide risk classification were still unsatisfactory, with an weighted F1 score of 50.89%. To address the issues of data imbalance and limited dataset size, we investigated both traditional and advanced, large language model based data augmentation techniques, demonstrating that data augmentation can enhance model performance by up to 4.65% points in F1-score. Notably, the Chinese MentalBERT model, which was pre-trained on psychological domain data, shows superior performance in both tasks. This study provides valuable insights for automatic identification of suicidal individuals, facilitating timely psychological intervention on social media platforms. The source code and data are publicly available. ",Kein DOI-Link verfügbar,2404.12659v1,Yes,"notable(1), potent(1)"
0000-0002-8947-1954,Shuo Liu,EPFL,Exploring and Unleashing the Power of Large Language Models in Automated   Code Translation,1970,"  Code translation tools (transpilers) are developed for automatic source-to-source translation. Although learning-based transpilers have shown impressive enhancement against rule-based counterparts, owing to their task-specific pre-training on extensive monolingual corpora. Their current performance still remains unsatisfactory for practical deployment, and the associated training resources are also prohibitively expensive. LLMs pre-trained on huge amounts of human-written code/text have shown remarkable performance in many code intelligence tasks due to their powerful generality, even without task-specific training. Thus, LLMs can potentially circumvent the above limitations, but they have not been exhaustively explored yet. This paper investigates diverse LLMs and learning-based transpilers for automated code translation tasks, finding that: although certain LLMs have outperformed current transpilers, they still have some accuracy issues, where most of the failures are induced by a lack of comprehension of source programs, missing clear instructions on I/O types in translation, and ignoring discrepancies between source and target programs. Enlightened by the above findings, we further propose UniTrans, a Unified code Translation framework, applicable to various LLMs, for unleashing their power in this field. Specifically, UniTrans first crafts a series of test cases for target programs with the assistance of source programs. Next, it harnesses the above auto-generated test cases to augment the code translation and then evaluate their correctness via execution. Afterward, UniTrans further (iteratively) repairs incorrectly translated programs prompted by test case execution results. Extensive experiments are conducted on six settings of translation datasets between Python, Java, and C++. Three recent LLMs of diverse sizes are tested with UniTrans, and all achieve substantial improvements. ",Kein DOI-Link verfügbar,2404.14646v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,A novel EM concentrator with open-concentrator region based on   multi-folded transformation optics,1970,"  Conventional concentrators with inhomogeneous coating materials that fully enclose the destined region pose great challenges for fabrication. In this paper, we propose to design an EM concentrator with homogeneous materials. Distinguished from conventional ones, the elaborately designed EM concentrator features a concentrator region that is open to the outer-world, which is achieved with multi-folded transformation optics method by compressing and folding the coating materials to create window(s). Based on this concept, we also investigate open-rotator and open rotational-concentrator devices, which could simultaneously rotate and store the EM waves in the central destined region. Due to the open nature of our proposed designs, we believe they will find potential applications in remote controlling with impressive new functionalities. ",https://doi.org/10.1038/s41598-018-28050-4,1805.05403v2,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Supervised Learning and Large Language Model Benchmarks on Mental Health   Datasets: Cognitive Distortions and Suicidal Risks in Chinese Social Media,1970,"  On social media, users often express their personal feelings, which may exhibit cognitive distortions or even suicidal tendencies on certain specific topics. Early recognition of these signs is critical for effective psychological intervention. In this paper, we introduce two novel datasets from Chinese social media: SOS-HL-1K for suicidal risk classification and SocialCD-3K for cognitive distortions detection. The SOS-HL-1K dataset contained 1,249 posts and SocialCD-3K dataset was a multi-label classification dataset that containing 3,407 posts. We propose a comprehensive evaluation using two supervised learning methods and eight large language models (LLMs) on the proposed datasets. From the prompt engineering perspective, we experimented with two types of prompt strategies, including four zero-shot and five few-shot strategies. We also evaluated the performance of the LLMs after fine-tuning on the proposed tasks. The experimental results show that there is still a huge gap between LLMs relying only on prompt engineering and supervised learning. In the suicide classification task, this gap is 6.95% points in F1-score, while in the cognitive distortion task, the gap is even more pronounced, reaching 31.53% points in F1-score. However, after fine-tuning, this difference is significantly reduced. In the suicide and cognitive distortion classification tasks, the gap decreases to 4.31% and 3.14%, respectively. This research highlights the potential of LLMs in psychological contexts, but supervised learning remains necessary for more challenging tasks. All datasets and code are made available. ",Kein DOI-Link verfügbar,2309.03564v3,Yes,potent(1)
0000-0002-8947-1954,Shuo Liu,EPFL,Polarization-controlled anisotropic coding metamaterials at terahertz   frequencies,1970,"  Metamaterials based on effective media have achieved a lot of unusual physics (e.g. negative refraction and invisibility cloaking) owing to their abilities to tailor the effective medium parameters that do not exist in nature. Recently, coding metamaterials have been suggested to control electromagnetic waves by designing the coding sequences of digital elements '0' and '1', which possess opposite phase responses. Here, we propose the concept of anisotropic coding metamaterial at terahertz frequencies, in which coding behaviors in different directions are dependent on the polarization status of terahertz waves. We experimentally demonstrate an ultrathin and flexible polarization-controlled anisotropic coding metasurface functioning in the terahertz regime using specially- designed coding elements. By encoding the elements with elaborately-designed digital sequences (in both 1 bit and 2 bits), the x- and y-polarized reflected waves can be deflected or diffused independently in three dimensions. The simulated far-field scattering patterns as well as near-electric-field distributions are given to illustrate the bifunctional performance of the encoded metasurface, which show good agreement to the measurement results. We further demonstrate the abilities of anisotropic coding metasurface to generate beam splitter and realize anomalous reflection and polarization conversion simultaneously, providing powerful controls of differently-polarized terahertz waves. The proposed method enables versatile beam behaviors under orthogonal polarizations using a single metasurface, and hence will promise interesting terahertz devices. ",Kein DOI-Link verfügbar,1509.03692v1,Yes,versatile(1)
0000-0002-8947-1954,Shuo Liu,EPFL,HEAR4Health: A blueprint for making computer audition a staple of modern   healthcare,1970,"  Recent years have seen a rapid increase in digital medicine research in an attempt to transform traditional healthcare systems to their modern, intelligent, and versatile equivalents that are adequately equipped to tackle contemporary challenges. This has led to a wave of applications that utilise AI technologies; first and foremost in the fields of medical imaging, but also in the use of wearables and other intelligent sensors. In comparison, computer audition can be seen to be lagging behind, at least in terms of commercial interest. Yet, audition has long been a staple assistant for medical practitioners, with the stethoscope being the quintessential sign of doctors around the world. Transforming this traditional technology with the use of AI entails a set of unique challenges. We categorise the advances needed in four key pillars: Hear, corresponding to the cornerstone technologies needed to analyse auditory signals in real-life conditions; Earlier, for the advances needed in computational and data efficiency; Attentively, for accounting to individual differences and handling the longitudinal nature of medical data; and, finally, Responsibly, for ensuring compliance to the ethical standards accorded to the field of medicine. ",Kein DOI-Link verfügbar,2301.10477v1,Yes,versatile(1)
0000-0002-8947-1954,Shuo Liu,EPFL,MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large   Vision-Language Models Towards Multitask AGI,1970,"  Large Vision-Language Models (LVLMs) show significant strides in general-purpose multimodal applications such as visual dialogue and embodied navigation. However, existing multimodal evaluation benchmarks cover a limited number of multimodal tasks testing rudimentary capabilities, falling short in tracking LVLM development. In this study, we present MMT-Bench, a comprehensive benchmark designed to assess LVLMs across massive multimodal tasks requiring expert knowledge and deliberate visual recognition, localization, reasoning, and planning. MMT-Bench comprises $31,325$ meticulously curated multi-choice visual questions from various multimodal scenarios such as vehicle driving and embodied navigation, covering $32$ core meta-tasks and $162$ subtasks in multimodal understanding. Due to its extensive task coverage, MMT-Bench enables the evaluation of LVLMs using a task map, facilitating the discovery of in- and out-of-domain tasks. Evaluation results involving $30$ LVLMs such as the proprietary GPT-4V, GeminiProVision, and open-sourced InternVL-Chat, underscore the significant challenges posed by MMT-Bench. We anticipate that MMT-Bench will inspire the community to develop next-generation multimodal foundation models aimed at achieving general-purpose multimodal intelligence. ",Kein DOI-Link verfügbar,2404.16006v1,Yes,"meticulous(1), meticulously(1)"
0000-0001-8069-2411,Nathan Ronceray,EPFL,Modeling the shape of axisymmetric skyrmions in magnetic multilayers,1970,"  We present a comprehensive micromagnetic model of isolated axisymmetric skyrmions in magnetic multilayers with perpendicular anisotropy. Most notably, the essential role of the internal dipolar field is extensively considered with a minimum amount of assumptions on the magnetization profiles. The tri-dimensional structure of the multilayered skyrmions is modeled by their radial profiles in each layer. We first compare the results of the model against a full micromagnetic description in Cartesian coordinates. Our model combines information on both layer-dependent size and chirality of the skyrmions. We also provide a convenient criterion in order to characterize the stability of skyrmions against anisotropic elongations that would break their cylindrical symmetry, which allows to confirm the stability of the determined solutions. Because this model is able to treat magnetization configurations twisted through the thickness of multilayered skyrmions, it can provide predictions on any potential hybrid chirality in skyrmions due to the interplay of Dzyaloshinskii-Moriya and dipolar interactions in multilayers. We finally apply the results of our model to the description of the current-driven dynamics of hybrid chiral skyrmions. Using the Thiele formalism, we show that we can predict the forces exerted on the multilayered skyrmions by vertical spin-polarized currents, which provides a method to conform hybrid skyrmion chiralities and spin-current injection geometries in order to optimize skyrmion motion in multilayers, to the aim of maximizing the current-induced velocity, or canceling the skyrmion Hall angle. ",https://doi.org/10.1103/PhysRevApplied.10.064042,1807.04935v1,Yes,potent(1)
0000-0001-8069-2411,Nathan Ronceray,EPFL,Monitoring electrochemical dynamics through single-molecule imaging of   hBN surface emitters in organic solvents,1970,"  Electrochemical techniques conventionally lack spatial resolution and average local information over an entire electrode. While advancements in spatial resolution have been made through scanning probe methods, monitoring dynamics over large areas is still challenging, and it would be beneficial to be able to decouple the probe from the electrode itself. In this work, we leverage single molecule microscopy to spatiotemporally monitor analyte surface concentrations over a wide area using unmodified hexagonal boron nitride (hBN) in organic solvents. Through a sensing scheme based on redox-active species interactions with fluorescent emitters at the surface of hBN, we observe a linear decrease in the number of emitters under positive voltages applied to a nearby electrode. We find consistent trends in electrode reaction kinetics vs overpotentials between potentiostat-reported currents and optically-read emitter dynamics, showing Tafel slopes greater than 290 mV per decade. Finally, we draw on the capabilities of spectral single molecule localization microscopy (SMLM) to monitor the fluorescent species identity, enabling multiplexed readout. Overall, we show dynamic measurements of analyte concentration gradients at a micrometer-length scale with nanometer-scale depth and precision. Considering the many scalable options for engineering fluorescent emitters with 2D materials, our method holds promise for optically detecting a range of interacting species with unprecedented localization precision. ",Kein DOI-Link verfügbar,2405.10686v1,Yes,potent(2)
0000-0001-6095-5173,Franz-Josef Haug,EPFL,Angular behavior of the absorption limit in thin film silicon solar   cells,1970,"  We investigate the angular behavior of the upper bound of absorption provided by the guided modes in thin film solar cells. We show that the 4n^2 limit can be potentially exceeded in a wide angular and wavelength range using two-dimensional periodic thin film structures. Two models are used to estimate the absorption enhancement; in the first one, we apply the periodicity condition along the thickness of the thin film structure but in the second one, we consider imperfect confinement of the wave to the device. To extract the guided modes, we use an automatized procedure which is established in this work. Through examples, we show that from the optical point of view, thin film structures have a high potential to be improved by changing their shape. Also, we discuss the nature of different optical resonances which can be potentially used to enhance light trapping in the solar cell. We investigate the two different polarization directions for one-dimensional gratings and we show that the transverse magnetic polarization can provide higher values of absorption enhancement. We also propose a way to reduce the angular dependence of the solar cell efficiency by the appropriate choice of periodic pattern. Finally, to get more practical values for the absorption enhancement, we consider the effect of parasitic loss which can significantly reduce the enhancement factor. ",https://doi.org/10.1002/pip.2371,1303.2835v1,Yes,potent(3)
0000-0003-1579-5558,Martin Jaggi,EPFL,Correlating Twitter Language with Community-Level Health Outcomes,1970,"  We study how language on social media is linked to diseases such as atherosclerotic heart disease (AHD), diabetes and various types of cancer. Our proposed model leverages state-of-the-art sentence embeddings, followed by a regression model and clustering, without the need of additional labelled data. It allows to predict community-level medical outcomes from language, and thereby potentially translate these to the individual level. The method is applicable to a wide range of target variables and allows us to discover known and potentially novel correlations of medical outcomes with life-style aspects and other socioeconomic risk factors. ",Kein DOI-Link verfügbar,1906.06465v2,Yes,potent(2)
0000-0003-1579-5558,Martin Jaggi,EPFL,An Accelerated Communication-Efficient Primal-Dual Optimization   Framework for Structured Machine Learning,1970,"  Distributed optimization algorithms are essential for training machine learning models on very large-scale datasets. However, they often suffer from communication bottlenecks. Confronting this issue, a communication-efficient primal-dual coordinate ascent framework (CoCoA) and its improved variant CoCoA+ have been proposed, achieving a convergence rate of $\mathcal{O}(1/t)$ for solving empirical risk minimization problems with Lipschitz continuous losses. In this paper, an accelerated variant of CoCoA+ is proposed and shown to possess a convergence rate of $\mathcal{O}(1/t^2)$ in terms of reducing suboptimality. The analysis of this rate is also notable in that the convergence rate bounds involve constants that, except in extreme cases, are significantly reduced compared to those previously provided for CoCoA+. The results of numerical experiments are provided to show that acceleration can lead to significant performance gains. ",Kein DOI-Link verfügbar,1711.05305v1,Yes,notable(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,Spectral Preconditioning for Gradient Methods on Graded Non-convex   Functions,1970,"  The performance of optimization methods is often tied to the spectrum of the objective Hessian. Yet, conventional assumptions, such as smoothness, do often not enable us to make finely-grained convergence statements -- particularly not for non-convex problems. Striving for a more intricate characterization of complexity, we introduce a unique concept termed graded non-convexity. This allows to partition the class of non-convex problems into a nested chain of subclasses. Interestingly, many traditional non-convex objectives, including partially convex problems, matrix factorizations, and neural networks, fall within these subclasses. As a second contribution, we propose gradient methods with spectral preconditioning, which employ inexact top eigenvectors of the Hessian to address the ill-conditioning of the problem, contingent on the grade. Our analysis reveals that these new methods provide provably superior convergence rates compared to basic gradient descent on applicable problem classes, particularly when large gaps exist between the top eigenvalues of the Hessian. Our theory is validated by numerical experiments executed on multiple practical machine learning problems. ",Kein DOI-Link verfügbar,2402.04843v1,Yes,intricate(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,CoTFormer: A Chain-of-Thought Driven Architecture with Budget-Adaptive   Computation Cost at Inference,1970,"  Scaling language models to larger and deeper sizes has led to significant boosts in performance. Even though the size of these models limits their application in compute-constrained environments, the race to continually develop ever larger and deeper foundational models is underway. At the same time -- regardless of the model size -- task-specific techniques continue to play a pivotal role in achieving optimal downstream performance. One of these techniques, called Chain-of-Thought (CoT), is particularly interesting since, as we point out in this work, it resembles employing a deeper transformer through re-applying the model multiple times. However, a key subtlety in computing the attention of past tokens differentiates CoT from simply applying the model several times. Based on this insight, we propose CoTFormer, a novel architecture which closely mimics CoT at the token level, allowing us to obtain significantly improved accuracies close to much larger models. While applying CoT introduces additional computation costs, we compensate for it by leveraging CoTFormer's special compatibility with token-wise variable depth. Through a compute adaptive model -- which automatically allocates the compute to tokens that need it most -- we show that it is possible to reduce the computation cost significantly without any reduction in accuracy, and with further compute cost reductions possible while maintaining a competitive accuracy. ",Kein DOI-Link verfügbar,2310.10845v2,Yes,pivotal(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,"Modular Clinical Decision Support Networks (MoDN) -- Updatable,   Interpretable, and Portable Predictions for Evolving Clinical Environments",1970,"  Data-driven Clinical Decision Support Systems (CDSS) have the potential to improve and standardise care with personalised probabilistic guidance. However, the size of data required necessitates collaborative learning from analogous CDSS's, which are often unsharable or imperfectly interoperable (IIO), meaning their feature sets are not perfectly overlapping. We propose Modular Clinical Decision Support Networks (MoDN) which allow flexible, privacy-preserving learning across IIO datasets, while providing interpretable, continuous predictive feedback to the clinician.   MoDN is a novel decision tree composed of feature-specific neural network modules. It creates dynamic personalised representations of patients, and can make multiple predictions of diagnoses, updatable at each step of a consultation. The modular design allows it to compartmentalise training updates to specific features and collaboratively learn between IIO datasets without sharing any data. ",Kein DOI-Link verfügbar,2211.06637v1,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,A Distributed Second-Order Algorithm You Can Trust,1970,"  Due to the rapid growth of data and computational resources, distributed optimization has become an active research area in recent years. While first-order methods seem to dominate the field, second-order methods are nevertheless attractive as they potentially require fewer communication rounds to converge. However, there are significant drawbacks that impede their wide adoption, such as the computation and the communication of a large Hessian matrix. In this paper we present a new algorithm for distributed training of generalized linear models that only requires the computation of diagonal blocks of the Hessian matrix on the individual workers. To deal with this approximate information we propose an adaptive approach that - akin to trust-region methods - dynamically adapts the auxiliary model to compensate for modeling errors. We provide theoretical rates of convergence for a wide class of problems including L1-regularized objectives. We also demonstrate that our approach achieves state-of-the-art results on multiple large benchmark datasets. ",Kein DOI-Link verfügbar,1806.07569v1,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,Linear Speedup in Personalized Collaborative Learning,1970,"  Collaborative training can improve the accuracy of a model for a user by trading off the model's bias (introduced by using data from other users who are potentially different) against its variance (due to the limited amount of data on any single user). In this work, we formalize the personalized collaborative learning problem as a stochastic optimization of a task 0 while giving access to N related but different tasks 1,..., N. We provide convergence guarantees for two algorithms in this setting -- a popular collaboration method known as weighted gradient averaging, and a novel bias correction method -- and explore conditions under which we can achieve linear speedup w.r.t. the number of auxiliary tasks N. Further, we also empirically study their performance confirming our theoretical insights. ",Kein DOI-Link verfügbar,2111.05968v4,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,"MultiModN- Multimodal, Multi-Task, Interpretable Modular Networks",1970,"  Predicting multiple real-world tasks in a single model often requires a particularly diverse feature space. Multimodal (MM) models aim to extract the synergistic predictive potential of multiple data types to create a shared feature space with aligned semantic meaning across inputs of drastically varying sizes (i.e. images, text, sound). Most current MM architectures fuse these representations in parallel, which not only limits their interpretability but also creates a dependency on modality availability. We present MultiModN, a multimodal, modular network that fuses latent representations in a sequence of any number, combination, or type of modality while providing granular real-time predictive feedback on any number or combination of predictive tasks. MultiModN's composable pipeline is interpretable-by-design, as well as innately multi-task and robust to the fundamental issue of biased missingness. We perform four experiments on several benchmark MM datasets across 10 real-world tasks (predicting medical diagnoses, academic performance, and weather), and show that MultiModN's sequential MM fusion does not compromise performance compared with a baseline of parallel fusion. By simulating the challenging bias of missing not-at-random (MNAR), this work shows that, contrary to MultiModN, parallel fusion baselines erroneously learn MNAR and suffer catastrophic failure when faced with different patterns of MNAR at inference. To the best of our knowledge, this is the first inherently MNAR-resistant approach to MM modeling. In conclusion, MultiModN provides granular insights, robustness, and flexibility without compromising performance. ",Kein DOI-Link verfügbar,2309.14118v2,Yes,potent(1)
0000-0003-1579-5558,Martin Jaggi,EPFL,MEDITRON-70B: Scaling Medical Pretraining for Large Language Models,1970,"  Large language models (LLMs) can potentially democratize access to medical knowledge. While many efforts have been made to harness and improve LLMs' medical knowledge and reasoning capacities, the resulting models are either closed-source (e.g., PaLM, GPT-4) or limited in scale (<= 13B parameters), which restricts their abilities. In this work, we improve access to large-scale medical LLMs by releasing MEDITRON: a suite of open-source LLMs with 7B and 70B parameters adapted to the medical domain. MEDITRON builds on Llama-2 (through our adaptation of Nvidia's Megatron-LM distributed trainer), and extends pretraining on a comprehensively curated medical corpus, including selected PubMed articles, abstracts, and internationally-recognized medical guidelines. Evaluations using four major medical benchmarks show significant performance gains over several state-of-the-art baselines before and after task-specific finetuning. Overall, MEDITRON achieves a 6% absolute performance gain over the best public baseline in its parameter class and 3% over the strongest baseline we finetuned from Llama-2. Compared to closed-source LLMs, MEDITRON-70B outperforms GPT-3.5 and Med-PaLM and is within 5% of GPT-4 and 10% of Med-PaLM-2. We release our code for curating the medical pretraining corpus and the MEDITRON model weights to drive open-source development of more capable medical LLMs. ",Kein DOI-Link verfügbar,2311.16079v1,Yes,potent(1)
0000-0003-3438-9774,Federico Ronchetti,EPFL,Acceleration of electromagnetic shower development and enhancement of   light yield in oriented scintillating crystals,1970,  We observed a substantial increase of the scintillation light output of lead tungstate (PbWO$_4$) at a small incidence angle with respect to two main lattice axes. This reflects the acceleration of electromagnetic shower development that occurs in the crystalline Strong Field. We measured the scintillation light generated by $120$-$\mathrm{GeV}$ electrons and $10$-$100$-$\mathrm{GeV}$ $\gamma$ rays on thick samples. This result deepens the knowledge of the shower development mechanisms in crystal scintillators and could pave the way to the development of innovative accelerator- and space-borne calorimeters. ,Kein DOI-Link verfügbar,2404.12016v1,Yes,innovative(1)
0000-0002-6374-642X,Barak Gabai,EPFL,Exact quantization and analytic continuation,1970,"  In this paper we give a streamlined derivation of the exact quantization condition (EQC) on the quantum periods of the Schr\""odinger problem in one dimension with a general polynomial potential, based on Wronskian relations. We further generalize the EQC to potentials with a regular singularity, describing spherical symmetric quantum mechanical systems in a given angular momentum sector. We show that the thermodynamic Bethe ansatz (TBA) equations that govern the quantum periods undergo nontrivial monodromies as the angular momentum is analytically continued between integer values in the complex plane. The TBA equations together with the EQC are checked numerically against Hamiltonian truncation at real angular momenta and couplings, and are used to explore the analytic continuation of the spectrum on the complex angular momentum plane in examples. ",Kein DOI-Link verfügbar,2109.07516v2,Yes,potent(2)
0000-0003-3810-9856,Christoph Hennersperger,Technical University of Munich,Towards MRI-Based Autonomous Robotic US Acquisitions: A First   Feasibility Study,1970,"  Robotic ultrasound has the potential to assist and guide physicians during interventions. In this work, we present a set of methods and a workflow to enable autonomous MRI-guided ultrasound acquisitions. Our approach uses a structured-light 3D scanner for patient-to-robot and image-to-patient calibration, which in turn is used to plan 3D ultrasound trajectories. These MRI-based trajectories are followed autonomously by the robot and are further refined online using automatic MRI/US registration. Despite the low spatial resolution of structured light scanners, the initial planned acquisition path can be followed with an accuracy of 2.46 +/- 0.96 mm. This leads to a good initialization of the MRI/US registration: the 3D-scan-based alignment for planning and acquisition shows an accuracy (distance between planned ultrasound and MRI) of 4.47 mm, and 0.97 mm after an online-update of the calibration based on a closed loop registration. ",https://doi.org/10.1109/TMI.2016.2620723,1607.08371v1,Yes,potent(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Optimal Scheduling for Discounted Age Penalty Minimization in Multi-Loop   Networked Control,1970,"  Age-of-information (AoI) is a metric quantifying information freshness at the receiver. Since AoI combines packet generation frequency, packet loss, and delay into a single metric, it has received a lot of research attention as an interface between communication network and application. In this work, we apply AoI to the problem of wireless scheduling for multi-loop networked control systems (NCS), i.e., feedback control loops closed over a shared wireless network. We model the scheduling problem as a Markov decision process (MDP) with AoI as its observable states and derive a relation of control system error and AoI. We further derive a stationary scheduling policy to minimize control error over an infinite horizon. We show that our scheduler outperforms the state-of-the-art scheduling policies for NCS. To the best of our knowledge, this is the first work proposing an AoI-based wireless scheduling policy that minimizes the control error over an infinite horizon for multi-loop NCS. ",Kein DOI-Link verfügbar,1908.01503v3,Yes,fresh(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Probability Analysis of Age of Information in Multi-hop Networks,1970,"  Age-of-information (AoI) is a metric quantifying information freshness at the receiver. It captures the delay together with packet loss and packet generation rate. However, the existing literature focuses on average or peak AoI and neglects the complete distribution. In this work, we consider a N-hop network with time-invariant packet loss probabilities on each link. We derive closed form equations for the probability mass function of AoI. We verify our findings with simulations. Our results show that the performance indicators considered in the literature such as average or peak AoI may give misleading insights into the real AoI performance. ",Kein DOI-Link verfügbar,1911.09957v2,Yes,fresh(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,AoI-based Finite Horizon Scheduling for Heterogeneous Networked Control   Systems,1970,"  Age of information (AoI) measures information freshness at the receiver. AoI may provide insights into quality of service in communication systems. For this reason, it has been used as a cross-layer metric for wireless communication protocols. In this work, we employ AoI to calculate penalty functions for a centralized resource scheduling problem. We consider a single wireless link shared by multiple, heterogeneous control systems where each sub-system has a time-varying packet loss probability. Sub-systems are competing for network resources to improve the accuracy of their remote estimation process. In order to cope with the dynamically changing conditions of the wireless link, we define a finite horizon age-penalty minimization problem and propose a scheduler that takes optimal decisions by looking $H$ slots into the future. The proposed algorithm has a worst-case complexity that grows exponentially with $H$. However, by narrowing down our search space within the constrained set of actions, we are able to decrease the complexity significantly without losing optimality. On the contrary, we show by simulations that the benefit of increasing $H$ w.r.t. remote state estimation performance diminishes after a certain $H$ value. ",Kein DOI-Link verfügbar,2005.02037v1,Yes,fresh(1)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Task-oriented Scheduling for Networked Control Systems: An Age of   Information-Aware Implementation on Software-defined Radios,1970,"  Networked control systems (NCSs) are feedback control loops that are closed over a communication network. Emerging applications, such as telerobotics, drones and autonomous driving are the most prominent examples of such systems. Regular and timely information sharing between the components of NCSs is essential to fulfill the desired control tasks, as stale information can lead to performance degradation or even physical damage. In this work, we consider multiple heterogeneous NCSs that transmit their system state over a shared physical wireless channel towards a gateway node. We conduct a comprehensive experimental study on selected MAC protocols using software-defined radios with state-of-the-art (SotA) solutions that have been designed to increase information freshness and control performance. As a significant improvement over the SotA, we propose a novel contention-free algorithm that is able to outperform the existing solutions by combining their strengths in one protocol. In addition, we propose a new metric called normalized mean squared error that maps the age of information to a dimensionless quantity that captures the expected value of a control system's next transmission. We demonstrate its adoption and effectiveness for wireless resource scheduling in a case study involving multiple inverted pendulums. From our experimental study and results, we observe that value-aware prioritization of the sub-systems contributes to minimizing the negative effects of information staleness on control performance. In particular, as the number of devices increases, the benefit of control-awareness to the quality of control stands out when compared to protocols that focus solely on maximizing information freshness. ",Kein DOI-Link verfügbar,2202.09189v3,Yes,fresh(2)
0000-0002-9787-5975,Onur Ayan,Technical University of Munich,Age-of-Information vs. Value-of-Information Scheduling for Cellular   Networked Control Systems,1970,"  Age-of-Information (AoI) is a recently introduced metric for network operation with sensor applications which quantifies the freshness of data. In the context of networked control systems (NCSs), we compare the worth of the AoI metric with the value-of-information (VoI) metric, which is related to the uncertainty reduction in stochastic processes. First, we show that the uncertainty propagates non-linearly over time depending on system dynamics. Next, we define the value of a new update of the process of interest as a function of AoI and system parameters of the NCSs. We use the aggregated update value as a utility for the centralized scheduling problem in a cellular NCS composed of multiple heterogeneous control loops. By conducting a simulative analysis, we show that prioritizing transmissions with higher VoI improves performance of the NCSs compared with providing fair data freshness to all sub-systems equally. ",Kein DOI-Link verfügbar,1903.05356v1,Yes,fresh(2)
0000-0002-9181-8435,Manuel Rieger,Technical University of Munich,Fast optoelectronic charge state conversion of silicon vacancies in   diamond,1970,"  Group IV vacancy color centers in diamond are promising spin-photon interfaces with strong potential for applications for photonic quantum technologies. Reliable methods for controlling and stabilizing their charge state are urgently needed for scaling to multi-qubit devices. Here, we manipulate the charge state of silicon vacancy (SiV) ensembles by combining luminescence and photo-current spectroscopy. We controllably convert the charge state between the optically active SiV$^-$ and dark SiV$^{2-}$ with MHz rates and 90% contrast by judiciously choosing the local potential applied to in-plane surface electrodes and the laser excitation wavelength. We observe intense SiV$^-$ photoluminescence under hole-capture, measure the intrinsic conversion time from the dark SiV$^{2-}$ to the bright SiV$^-$ to be 36.4(6.7)ms and demonstrate how it can be enhanced by a factor of $10^5$ via optical pumping. Moreover, we obtain new information on the defects that contribute to photo-conductivity, indicating the presence of substitutional nitrogen and divacancies. ",https://doi.org/10.1126/sciadv.adl4265,2310.12288v1,Yes,potent(2)
0000-0001-9013-435X,Martin Schulz,Technical University of Munich,Quantum Algorithms for Solving Ordinary Differential Equations via   Classical Integration Methods,1970,"  Identifying computational tasks suitable for (future) quantum computers is an active field of research. Here we explore utilizing quantum computers for the purpose of solving differential equations. We consider two approaches: (i) basis encoding and fixed-point arithmetic on a digital quantum computer, and (ii) representing and solving high-order Runge-Kutta methods as optimization problems on quantum annealers. As realizations applied to two-dimensional linear ordinary differential equations, we devise and simulate corresponding digital quantum circuits, and implement and run a 6$^{\mathrm{th}}$ order Gauss-Legendre collocation method on a D-Wave 2000Q system, showing good agreement with the reference solution. We find that the quantum annealing approach exhibits the largest potential for high-order implicit integration methods. As promising future scenario, the digital arithmetic method could be employed as an ""oracle"" within quantum search algorithms for inverse problems. ",https://doi.org/10.22331/q-2021-07-13-502,2012.09469v2,Yes,potent(1)
0000-0001-9013-435X,Martin Schulz,Technical University of Munich,Design Principles of Dynamic Resource Management for High-Performance   Parallel Programming Models,1970,"  With Dynamic Resource Management (DRM) the resources assigned to a job can be changed dynamically during its execution. From the system's perspective, DRM opens a new level of flexibility in resource allocation and job scheduling and therefore has the potential to improve system efficiency metrics such as the utilization rate, job throughput, energy efficiency, and responsiveness. From the application perspective, users can tailor the resources they request to their needs offering potential optimizations in queuing time or charged costs. Despite these obvious advantages and many attempts over the last decade to establish DRM in HPC, it remains a concept discussed in academia rather than being successfully deployed on production systems. This stems from the fact that support for DRM requires changes in all the layers of the HPC system software stack including applications, programming models, process managers, and resource management software, as well as an extensive and holistic co-design process to establish new techniques and policies for scheduling and resource optimization. In this work, we therefore start with the assumption that resources are accessible by processes executed either on them (e.g., on CPU) or controlling them (e.g., GPU-offloading). Then, the overall DRM problem can be decomposed into dynamic process management (DPM) and dynamic resource mapping or allocation (DRA). The former determines which processes (or which change in processes) must be managed and the latter identifies the resources where they will be executed. The interfaces for such \mbox{DPM/DPA} in these layers need to be standardized, which requires a careful design to be interoperable while providing high flexibility. Based on a survey of existing approaches we propose design principles, that form the basis of a holistic approach to DMR in HPC and provide a prototype implementation using MPI. ",Kein DOI-Link verfügbar,2403.17107v1,Yes,potent(2)
0000-0001-9013-435X,Martin Schulz,Technical University of Munich,On the Convergence of Malleability and the HPC PowerStack: Exploiting   Dynamism in Over-Provisioned and Power-Constrained HPC Systems,1970,"  Recent High-Performance Computing (HPC) systems are facing important challenges, such as massive power consumption, while at the same time significantly under-utilized system resources. Given the power consumption trends, future systems will be deployed in an over-provisioned manner where more resources are installed than they can afford to power simultaneously. In such a scenario, maximizing resource utilization and energy efficiency, while keeping a given power constraint, is pivotal. Driven by this observation, in this position paper we first highlight the recent trends of resource management techniques, with a particular focus on malleability support (i.e., dynamically scaling resource allocations/requirements for a job), co-scheduling (i.e., co-locating multiple jobs within a node), and power management. Second, we consider putting them together, assess their relationships/synergies, and discuss the functionality requirements in each software component for future over-provisioned and power-constrained HPC systems. Third, we briefly introduce our ongoing efforts on the integration of software tools, which will ultimately lead to the convergence of malleability and power management, as it is designed in the HPC PowerStack initiative. ",https://doi.org/10.1007/978-3-031-23220-6_14,2405.03847v1,Yes,pivotal(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,DPSNN: Spiking Neural Network for Low-Latency Streaming Speech   Enhancement,1970,"  Speech enhancement (SE) improves communication in noisy environments, affecting areas such as automatic speech recognition, hearing aids, and telecommunications. With these domains typically being power-constrained and event-based while requiring low latency, neuromorphic algorithms in the form of spiking neural networks (SNNs) have great potential. Yet, current effective SNN solutions require a contextual sampling window imposing substantial latency, typically around 32ms, too long for many applications. Inspired by Dual-Path Spiking Neural Networks (DPSNNs) in classical neural networks, we develop a two-phase time-domain streaming SNN framework -- the Dual-Path Spiking Neural Network (DPSNN). In the DPSNN, the first phase uses Spiking Convolutional Neural Networks (SCNNs) to capture global contextual information, while the second phase uses Spiking Recurrent Neural Networks (SRNNs) to focus on frequency-related features. In addition, the regularizer suppresses activation to further enhance energy efficiency of our DPSNNs. Evaluating on the VCTK and Intel DNS Datasets, we demonstrate that our approach achieves the very low latency (approximately 5ms) required for applications like hearing aids, while demonstrating excellent signal-to-noise ratio (SNR), perceptual quality, and energy efficiency. ",Kein DOI-Link verfügbar,2408.07388v1,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Markov Chain Block Coordinate Descent,1970,"  The method of block coordinate gradient descent (BCD) has been a powerful method for large-scale optimization. This paper considers the BCD method that successively updates a series of blocks selected according to a Markov chain. This kind of block selection is neither i.i.d. random nor cyclic. On the other hand, it is a natural choice for some applications in distributed optimization and Markov decision process, where i.i.d. random and cyclic selections are either infeasible or very expensive. By applying mixing-time properties of a Markov chain, we prove convergence of Markov chain BCD for minimizing Lipschitz differentiable functions, which can be nonconvex. When the functions are convex and strongly convex, we establish both sublinear and linear convergence rates, respectively. We also present a method of Markov chain inertial BCD. Finally, we discuss potential applications. ",Kein DOI-Link verfügbar,1811.08990v1,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Asynchronous Coordinate Descent under More Realistic Assumptions,1970,"  Asynchronous-parallel algorithms have the potential to vastly speed up algorithms by eliminating costly synchronization. However, our understanding to these algorithms is limited because the current convergence of asynchronous (block) coordinate descent algorithms are based on somewhat unrealistic assumptions. In particular, the age of the shared optimization variables being used to update a block is assumed to be independent of the block being updated. Also, it is assumed that the updates are applied to randomly chosen blocks. In this paper, we argue that these assumptions either fail to hold or will imply less efficient implementations. We then prove the convergence of asynchronous-parallel block coordinate descent under more realistic assumptions, in particular, always without the independence assumption. The analysis permits both the deterministic (essentially) cyclic and random rules for block choices. Because a bound on the asynchronous delays may or may not be available, we establish convergence for both bounded delays and unbounded delays. The analysis also covers nonconvex, weakly convex, and strongly convex functions. We construct Lyapunov functions that directly model both objective progress and delays, so delays are not treated errors or noise. A continuous-time ODE is provided to explain the construction at a high level. ",Kein DOI-Link verfügbar,1705.08494v2,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Backdoor Cleansing with Unlabeled Data,1970,"  Due to the increasing computational demand of Deep Neural Networks (DNNs), companies and organizations have begun to outsource the training process. However, the externally trained DNNs can potentially be backdoor attacked. It is crucial to defend against such attacks, i.e., to postprocess a suspicious model so that its backdoor behavior is mitigated while its normal prediction power on clean inputs remain uncompromised. To remove the abnormal backdoor behavior, existing methods mostly rely on additional labeled clean samples. However, such requirement may be unrealistic as the training data are often unavailable to end users. In this paper, we investigate the possibility of circumventing such barrier. We propose a novel defense method that does not require training labels. Through a carefully designed layer-wise weight re-initialization and knowledge distillation, our method can effectively cleanse backdoor behaviors of a suspicious network with negligible compromise in its normal behavior. In experiments, we show that our method, trained without labels, is on-par with state-of-the-art defense methods trained using labels. We also observe promising defense results even on out-of-distribution data. This makes our method very practical. Code is available at: https://github.com/luluppang/BCU. ",Kein DOI-Link verfügbar,2211.12044v4,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,TraceCaps: A Capsule-based Neural Network for Semantic Segmentation,1970,"  In this paper, we propose a capsule-based neural network model to solve the semantic segmentation problem. By taking advantage of the extractable part-whole dependencies available in capsule layers, we derive the probabilities of the class labels for individual capsules through a recursive, layer-by-layer procedure. We model this procedure as a traceback pipeline and take it as a central piece to build an end-to-end segmentation network. Under the proposed framework, image-level class labels and object boundaries are jointly sought in an explicit manner, which poses a significant advantage over the state-of-the-art fully convolutional network (FCN) solutions. With the capability to extracted part-whole information, our traceback pipeline can potentially be utilized as the building blocks to design interpretable neural networks. Experiments conducted on modified MNIST and neuroimages demonstrate that our model considerably enhance the segmentation performance compared to the leading FCN variants. ",Kein DOI-Link verfügbar,1901.02920v2,Yes,potent(1)
0009-0004-6858-4791,Tao Sun,Technical University of Munich,Mapping the Depths: A Stocktake of Underground Power Distribution in   United States,1970,"  A resilient energy infrastructure is crucial for addressing increasing extreme weather and climate risks. The undergrounding of the power system is one approach to building such resiliency. In this study, we introduce Grid Underground Distribution Statistics (GUDS) for the US, the first nationwide comprehensive assessment of underground electricity distribution at a high spatial granularity. In analyzing this dataset, we find regional differences in underground distribution rates, with generally higher rates for east and west coasts and in northern states, and lower rates in the central US. We also observe relationships between underground rates and factors such as household income levels, degree of urbanization, and vulnerability to natural hazards. Notably, regions with higher electricity rates are not associated with greater proportions of underground distribution, highlighting potential equity issues in infrastructure distribution. By presenting this granular information and insights on underground distribution, our study offers valuable guidance for informing planning and decision-making by policymakers, Independent System Operators, utilities, and end-users. ",Kein DOI-Link verfügbar,2402.06668v1,Yes,potent(1)
0000-0002-4284-7686,Ingemar Bengtsson,"Lund University, Lund University Samhällsvetenskapliga fakulteten","The Frame Potential, on Average",1970,"  A SIC consists of N^2 equiangular unit vectors in an N dimensional Hilbert space. The frame potential is a function of N^2 unit vectors. It has a unique global minimum if the vectors form a SIC, and this property has been made use of in numerical searches for SICs. When the vectors form an orbit of the Heisenberg group the frame potential becomes a function of a single fiducial vector. We analytically compute the average of this function over Hilbert space. We also compute averages when the fiducial vector is placed in certain special subspaces defined by the Clifford group. ",Kein DOI-Link verfügbar,0808.2947v2,Yes,potent(2)
0000-0002-4284-7686,Ingemar Bengtsson,"Lund University, Lund University Samhällsvetenskapliga fakulteten",Energy in Newtonian gravity,1970,"  In Newtonian gravity it is a moot question whether energy should be localized in the field or inside matter. An argument from relativity suggests a compromise in which the contribution from the field in vacuum is positive definite. We show that the same compromise is implied by Noether's theorem applied to a variational principle for perfect fluids, if we assume Dirichlet boundary conditions on the potential. We then analyse a thought experiment due to Bondi and McCrea that gives a clean example of inductive energy transfer by gravity. Some history of the problem is included ",https://doi.org/10.1007/s10701-022-00660-z,2112.06503v2,Yes,potent(1)
0000-0002-4284-7686,Ingemar Bengtsson,"Lund University, Lund University Samhällsvetenskapliga fakulteten",Geometric Phases for Mixed States of the Kitaev Chain,1970,"  The Berry phase has found applications in building topological order parameters for certain condensed matter systems. The question whether some geometric phase for mixed states can serve the same purpose has been raised, and proposals are on the table. We analyze the intricate behaviour of Uhlmann's geometric phase in the Kitaev chain at finite temperature, and then argue that it captures quite different physics from that intended. We also analyze the behaviour of a geometric phase introduced in the context of interferometry. For the Kitaev chain, this phase closely mirrors that of the Berry phase, and we argue that it merits further investigation. ",https://doi.org/10.1098/rsta.2015.0231,1507.00766v2,Yes,intricate(1)
0000-0002-4284-7686,Ingemar Bengtsson,"Lund University, Lund University Samhällsvetenskapliga fakulteten",Linear Dependencies in Weyl-Heisenberg Orbits,1970,"  Five years ago, Lane Hughston showed that some of the symmetric informationally complete positive operator valued measures (SICs) in dimension 3 coincide with the Hesse configuration (a structure well known to algebraic geometers, which arises from the torsion points of a certain elliptic curve). This connection with elliptic curves is signalled by the presence of linear dependencies among the SIC vectors. Here we look for analogous connections between SICs and algebraic geometry by performing computer searches for linear dependencies in higher dimensional SICs. We prove that linear dependencies will always emerge in Weyl-Heisenberg orbits when the fiducial vector lies in a certain subspace of an order 3 unitary matrix. This includes SICs when the dimension is divisible by 3 or equal to 8 mod 9. We examine the linear dependencies in dimension 6 in detail and show that smaller dimensional SICs are contained within this structure, potentially impacting the SIC existence problem. We extend our results to look for linear dependencies in orbits when the fiducial vector lies in an eigenspace of other elements of the Clifford group that are not order 3. Finally, we align our work with recent studies on representations of the Clifford group. ",https://doi.org/10.1007/s11128-013-0609-6,1211.0215v2,Yes,potent(1)
0000-0002-1302-1965,Paul Clarke,"The University of Bristol, University of Bristol",Undersmoothing Causal Estimators with Generative Trees,1970,"  Inferring individualised treatment effects from observational data can unlock the potential for targeted interventions. It is, however, hard to infer these effects from observational data. One major problem that can arise is covariate shift where the data (outcome) conditional distribution remains the same but the covariate (input) distribution changes between the training and test set. In an observational data setting, this problem is materialised in control and treated units coming from different distributions. A common solution is to augment learning methods through reweighing schemes (e.g. propensity scores). These are needed due to model misspecification, but might hurt performance in the individual case. In this paper, we explore a novel generative tree based approach that tackles model misspecification directly, helping downstream estimators achieve better robustness. We show empirically that the choice of model class can indeed significantly affect the final performance and that reweighing methods can struggle in individualised effect estimation. Our proposed approach is competitive with reweighing methods on average treatment effects while performing significantly better on individualised treatment effects. ",https://doi.org/10.1109/ACCESS.2024.3376423,2203.08570v1,Yes,potent(1)
0000-0002-9257-8699,Paul Moran,"The University of Bristol, University of Bristol",Mediation effects that emulate a target randomised trial:   Simulation-based evaluation of ill-defined interventions on multiple   mediators,1970,"  Many epidemiological questions concern potential interventions to alter the pathways presumed to mediate an association. For example, we consider a study that investigates the benefit of interventions in young adulthood for ameliorating the poorer mid-life psychosocial outcomes of adolescent self-harmers relative to their healthy peers. Two methodological challenges arise. Firstly, mediation methods have hitherto mostly focused on the elusive task of discovering pathways, rather than on the evaluation of mediator interventions. Secondly, the complexity of such questions is invariably such that there are no existing data on well-defined interventions (i.e. actual treatments, programs, etc.) capturing the populations, outcomes and time-spans of interest. Instead, researchers must rely on exposure (non-intervention) data to address these questions, such as self-reported substance use and employment. We address the resulting challenges by specifying a target trial addressing three policy-relevant questions, regarding the impacts of hypothetical (rather than actual) interventions that would shift the mediators' distributions (separately, jointly or sequentially) to user-specified distributions that can be emulated with the observed data. We then define novel interventional effects that map to this trial, emulating shifts by setting mediators to random draws from those distributions. We show that estimation using a g-computation method is possible under an expanded set of causal assumptions relative to inference with well-defined interventions. These expanded assumptions reflect the lower level of evidence that is inevitable with ill-defined interventions. Application to the self-harm example using data from the Victorian Adolescent Health Cohort Study illustrates the value of our proposal for informing the design and evaluation of actual interventions in the future. ",Kein DOI-Link verfügbar,1907.06734v3,Yes,potent(1)
0000-0002-1372-9840,Hai Liu,"The University of Bristol, University of Bristol",A semiparametric regression model for paired longitudinal outcomes with   application in childhood blood pressure development,1970,"  This research examines the simultaneous influences of height and weight on longitudinally measured systolic and diastolic blood pressure in children. Previous studies have shown that both height and weight are positively associated with blood pressure. In children, however, the concurrent increases of height and weight have made it all but impossible to discern the effect of height from that of weight. To better understand these influences, we propose to examine the joint effect of height and weight on blood pressure. Bivariate thin plate spline surfaces are used to accommodate the potentially nonlinear effects as well as the interaction between height and weight. Moreover, we consider a joint model for paired blood pressure measures, that is, systolic and diastolic blood pressure, to account for the underlying correlation between the two measures within the same individual. The bivariate spline surfaces are allowed to vary across different groups of interest. We have developed related model fitting and inference procedures. The proposed method is used to analyze data from a real clinical investigation. ",https://doi.org/10.1214/12-AOAS567,1301.2871v1,Yes,potent(1)
0000-0002-1372-9840,Hai Liu,"The University of Bristol, University of Bristol",A Voxel Graph CNN for Object Classification with Event Cameras,1970,"  Event cameras attract researchers' attention due to their low power consumption, high dynamic range, and extremely high temporal resolution. Learning models on event-based object classification have recently achieved massive success by accumulating sparse events into dense frames to apply traditional 2D learning methods. Yet, these approaches necessitate heavy-weight models and are with high computational complexity due to the redundant information introduced by the sparse-to-dense conversion, limiting the potential of event cameras on real-life applications. This study aims to address the core problem of balancing accuracy and model complexity for event-based classification models. To this end, we introduce a novel graph representation for event data to exploit their sparsity better and customize a lightweight voxel graph convolutional neural network (\textit{EV-VGCNN}) for event-based classification. Specifically, (1) using voxel-wise vertices rather than previous point-wise inputs to explicitly exploit regional 2D semantics of event streams while keeping the sparsity;(2) proposing a multi-scale feature relational layer (\textit{MFRL}) to extract spatial and motion cues from each vertex discriminatively concerning its distances to neighbors. Comprehensive experiments show that our model can advance state-of-the-art classification accuracy with extremely low model complexity (merely 0.84M parameters). ",Kein DOI-Link verfügbar,2106.00216v3,Yes,potent(1)
0000-0002-1372-9840,Hai Liu,"The University of Bristol, University of Bristol",Efficient Channel-Hopping Rendezvous Algorithm Based on Available   Channel Set,1970,"  In cognitive radio networks, rendezvous is a fundamental operation by which two cognitive users establish a communication link on a commonly-available channel for communications. Some existing rendezvous algorithms can guarantee that rendezvous can be completed within finite time and they generate channel-hopping (CH) sequences based on the whole channel set. However, some channels may not be available (e.g., they are being used by the licensed users) and these existing algorithms would randomly replace the unavailable channels in the CH sequence. This random replacement is not effective, especially when the number of unavailable channels is large. In this paper, we design a new rendezvous algorithm that attempts rendezvous on the available channels only for faster rendezvous. This new algorithm, called Interleaved Sequences based on Available Channel set (ISAC), constructs an odd sub-sequence and an even sub-sequence and interleaves these two sub-sequences to compose a CH sequence. We prove that ISAC provides guaranteed rendezvous (i.e., rendezvous can be achieved within finite time). We derive the upper bound on the maximum time-to-rendezvous (MTTR) to be O(m) (m is not greater than Q) under the symmetric model and O(mn) (n is not greater than Q) under the asymmetric model, where m and n are the number of available channels of two users and Q is the total number of channels (i.e., all potentially available channels). We conduct extensive computer simulation to demonstrate that ISAC gives significantly smaller MTTR than the existing algorithms. ",Kein DOI-Link verfügbar,1506.01136v1,Yes,potent(1)
0000-0002-6587-6949,Phillip Sloan,"The University of Bristol, University of Bristol",Automated Radiology Report Generation: A Review of Recent Advances,1970,"  Increasing demands on medical imaging departments are taking a toll on the radiologist's ability to deliver timely and accurate reports. Recent technological advances in artificial intelligence have demonstrated great potential for automatic radiology report generation (ARRG), sparking an explosion of research. This survey paper conducts a methodological review of contemporary ARRG approaches by way of (i) assessing datasets based on characteristics, such as availability, size, and adoption rate, (ii) examining deep learning training methods, such as contrastive learning and reinforcement learning, (iii) exploring state-of-the-art model architectures, including variations of CNN and transformer models, (iv) outlining techniques integrating clinical knowledge through multimodal inputs and knowledge graphs, and (v) scrutinising current model evaluation techniques, including commonly applied NLP metrics and qualitative clinical reviews. Furthermore, the quantitative results of the reviewed models are analysed, where the top performing models are examined to seek further insights. Finally, potential new directions are highlighted, with the adoption of additional datasets from other radiological modalities and improved evaluation methods predicted as important areas of future development. ",https://doi.org/10.1109/RBME.2024.3408456,2405.10842v2,Yes,potent(2)
0000-0002-6867-3709,Jiaxi Chen,"The University of Bristol, University of Bristol",Deep Learning-based Image and Video Inpainting: A Survey,1970,"  Image and video inpainting is a classic problem in computer vision and computer graphics, aiming to fill in the plausible and realistic content in the missing areas of images and videos. With the advance of deep learning, this problem has achieved significant progress recently. The goal of this paper is to comprehensively review the deep learning-based methods for image and video inpainting. Specifically, we sort existing methods into different categories from the perspective of their high-level inpainting pipeline, present different deep learning architectures, including CNN, VAE, GAN, diffusion models, etc., and summarize techniques for module design. We review the training objectives and the common benchmark datasets. We present evaluation metrics for low-level pixel and high-level perceptional similarity, conduct a performance evaluation, and discuss the strengths and weaknesses of representative inpainting methods. We also discuss related real-world applications. Finally, we discuss open challenges and suggest potential future research directions. ",Kein DOI-Link verfügbar,2401.03395v1,Yes,potent(1)
0000-0002-5233-5000,Jonathan Ives,"The University of Bristol, University of Bristol",Soft Gripping: Specifying for Trustworthiness,1970,"  Soft robotics is an emerging technology in which engineers create flexible devices for use in a variety of applications. In order to advance the wide adoption of soft robots, ensuring their trustworthiness is essential; if soft robots are not trusted, they will not be used to their full potential. In order to demonstrate trustworthiness, a specification needs to be formulated to define what is trustworthy. However, even for soft robotic grippers, which is one of the most mature areas in soft robotics, the soft robotics community has so far given very little attention to formulating specifications. In this work, we discuss the importance of developing specifications during development of soft robotic systems, and present an extensive example specification for a soft gripper for pick-and-place tasks for grocery items. The proposed specification covers both functional and non-functional requirements, such as reliability, safety, adaptability, predictability, ethics, and regulations. We also highlight the need to promote verifiability as a first-class objective in the design of a soft gripper. ",Kein DOI-Link verfügbar,2307.01159v2,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",On the Performance of Direct Shaping Codes,1970,"  In this work, we study a recently proposed direct shaping code for flash memory. This rate-1 code is designed to reduce the wear for SLC (one bit per cell) flash by minimizing the average fraction of programmed cells when storing structured data. Then we describe an adaptation of this algorithm that provides data shaping for MLC (two bits per cell) flash memory. It makes use of a page-dependent cost model and is designed to be compatible with the standard procedure of row-by-row, page-based, wordline programming. We also give experimental results demonstrating the performance of MLC data shaping codes when applied to English and Chinese language text. We then study the potential error propagation properties of direct shaping codes when used in a noisy flash device. In particular, we model the error propagation as a biased random walk in a multidimensional space. We prove an upper bound on the error propagation probability and propose an algorithm that can numerically approach a lower bound. Finally, we study the asymptotic performance of direct shaping codes. We prove that the SLC direct shaping code is suboptimal in the sense that it can only achieve the minimum average cost for a rate-1 code under certain conditions on the source distribution. ",Kein DOI-Link verfügbar,2007.05638v1,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",Implementation of ACTS for STCF track reconstruction,1970,"  With an electron-positron collider operating at center-of-mass-energy 2-7 GeV and a peak luminosity above $0.5\times10^{35} cm^{-2} s^{-1}$, the STCF physics program will provide an unique platform for in-depth studies of hadron structure and non-perturbative strong interaction as well as probing new physics beyond the Standard Model in the $\tau$-Charm sector, succeeding the present Beijing Electron-Positron Collider. To fulfill the physics targets and further maximize the physics potential at STCF, the STCF tracking software should have capability to reconstruct charged particles with high efficiency and excellent momentum resolution, especially for the charged particles with low transverse momentum down to 50 MeV. A Common Tracking Software (ACTS) providing a set of detector-independent tracking algorithms is adopted for reconstructing charged tracks with the information of two sub-detectors, a $\mu$RWELL-based inner tracker and a drift chamber, at STCF. This is the first demonstration of ACTS for a drift chamber. The implementation details and performance of track reconstruction are presented. ",https://doi.org/10.1088/1748-0221/18/07/P07026,2301.04306v4,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",Sub-MHz spectral dip in a resonator-free twisted gain medium,1970,"  Ultra-narrow optical spectral features resulting from highly dispersive light-matter interactions are essential for a broad range of applications such as spectroscopy, slow-light, and high-precision sensing. Features approaching sub-MHz, or equivalently, Q-factors approaching ~1 billion and beyond, are challenging to obtain in solid-state systems, ultimately limited by loss. We present a novel approach to achieve tunable sub-MHz spectral features, at room temperature, without resonators. We exploit gain-enhanced polarization pulling in a twisted birefringent medium where polarization eigenmodes are frequency-dependent. Using Brillouin gain in a commercial spun fiber, we experimentally achieve a 0.72 MHz spectral dip, the narrowest backward Brillouin scattering feature ever reported. Further optimization can potentially reduce the linewidth to <0.1 MHz. Our approach is simple and broadly applicable, offering on-demand tunability and high sensitivity, with a wide range of applications such as microwave photonic filters, slow and fast light, and optical sensing. ",https://doi.org/10.1038/s41566-022-01015-w,2104.05101v5,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",Foveated Rendering: a State-of-the-Art Survey,1970,"  Recently, virtual reality (VR) technology has been widely used in medical, military, manufacturing, entertainment, and other fields.   These applications must simulate different complex material surfaces, various dynamic objects, and complex physical phenomena, increasing the complexity of VR scenes. Current computing devices cannot efficiently render these complex scenes in real time, and delayed rendering makes the content observed by the user inconsistent with the user's interaction, causing discomfort.   Foveated rendering is a promising technique that can accelerate rendering. It takes advantage of human eyes' inherent features and renders different regions with different qualities without sacrificing perceived visual quality.   Foveated rendering research has a history of 31 years and is mainly focused on solving the following three problems.   The first is to apply perceptual models of the human visual system into foveated rendering. The second is to render the image with different qualities according to foveation principles. The third is to integrate foveated rendering into existing rendering paradigms to improve rendering performance.   In this survey, we review foveated rendering research from 1990 to 2021.   We first revisit the visual perceptual models related to foveated rendering.   Subsequently, we propose a new foveated rendering taxonomy and then classify and review the research on this basis. Finally, we discuss potential opportunities and open questions in the foveated rendering field.   We anticipate that this survey will provide new researchers with a high-level overview of the state of the art in this field, furnish experts with up-to-date information and offer ideas alongside a framework to VR display software and hardware designers and engineers. ",Kein DOI-Link verfügbar,2211.07969v1,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",Pyroresistive response of percolating conductive polymer composites,1970,"  The pyroresistive response of conductive polymer composites (CPCs) has attracted much interest because of its potential applications in many electronic devices requiring a significant responsiveness to changes in external physical parameters such as temperature or electric fields. Although extensive research has been conducted to study how the properties of the polymeric matrix and conductive fillers affect the positive temperature coefficient pyroresistive effect, the understanding of the microscopic mechanism governing such a phenomenon is still incomplete. In particular, to date, there is little body of theoretical research devoted to investigating the effect of the polymer thermal expansion on the electrical connectivity of the conductive phase. Here, we present the results of simulations of model CPCs in which rigid conductive fillers are dispersed in an insulating amorphous matrix. By employing a meshless algorithm to analyze the thermoelastic response of the system, we couple the computed strain field to the electrical connectedness of the percolating conductive particles. We show that the electrical conductivity responds to the local strains that are generated by the mismatch between the thermal expansion of the polymeric and conductive phases and that the conductor-insulator transition is caused by a sudden and global disconnection of the electrical contacts forming the percolating network. ",https://doi.org/10.1103/PhysRevMaterials.8.045602,2406.05461v1,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",LOGO: Video Text Spotting with Language Collaboration and Glyph   Perception Model,1970,"  Video text spotting (VTS) aims to simultaneously localize, recognize and track text instances in videos. To address the limited recognition capability of end-to-end methods, recent methods track the zero-shot results of state-of-the-art image text spotters directly, and achieve impressive performance. However, owing to the domain gap between different datasets, these methods usually obtain limited tracking trajectories on extreme dataset. Fine-tuning transformer-based text spotters on specific datasets could yield performance enhancements, albeit at the expense of considerable training resources. In this paper, we propose a Language Collaboration and Glyph Perception Model, termed LOGO, an innovative framework designed to enhance the performance of conventional text spotters. To achieve this goal, we design a language synergy classifier (LSC) to explicitly discern text instances from background noise in the recognition stage. Specially, the language synergy classifier can output text content or background code based on the legibility of text regions, thus computing language scores. Subsequently, fusion scores are computed by taking the average of detection scores and language scores, and are utilized to re-score the detection results before tracking. By the re-scoring mechanism, the proposed LSC facilitates the detection of low-resolution text instances while filtering out text-like regions. Moreover, the glyph supervision is introduced to enhance the recognition accuracy of noisy text regions. In addition, we propose the visual position mixture module, which can merge the position information and visual features efficiently, and acquire more discriminative tracking features. Extensive experiments on public benchmarks validate the effectiveness of the proposed method. ",Kein DOI-Link verfügbar,2405.19194v2,Yes,innovative(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",High-temperature superconductivity in one-unit-cell FeSe films,1970,"  Since the dramatic interface enhancement of superconducting transition temperature (Tc) was reported in one unit-cell FeSe film grown on SrTiO3 substrate (1-UC FeSe/STO) by molecular beam epitaxy (MBE), related research on this system has become a new frontier in condensed matter physics. In this paper, we present a brief review on this rapidly developing field, mainly focusing on the superconducting properties of 1-UC FeSe/STO. Experimental evidences for the high-temperature superconductivity in 1-UC FeSe/STO, including the direct evidences revealed by transport and diamagnetic measurements, and other evidences from scanning tunneling microscope (STM) and angle-resolved photoemission spectroscopy (ARPES), are overviewed. Potential mechanisms of the enhanced superconductivity are discussed. There are accumulating arguments suggesting that the strengthened Cooper pairing in 1-UC FeSe/STO originates from the interface effects, specifically charge transfer and coupling to phonon modes in TiO2 plane. The study of superconductivity in 1-UC FeSe/STO not only sheds a new light on the mechanism of high-temperature superconductors with layered structures, but also provides the insight to explore new superconductors by interface engineering. ",https://doi.org/10.1088/1361-648X/aa5f26,1701.06855v1,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",The Right to be Forgotten in Federated Learning: An Efficient   Realization with Rapid Retraining,1970,"  In Machine Learning, the emergence of \textit{the right to be forgotten} gave birth to a paradigm named \textit{machine unlearning}, which enables data holders to proactively erase their data from a trained model. Existing machine unlearning techniques focus on centralized training, where access to all holders' training data is a must for the server to conduct the unlearning process. It remains largely underexplored about how to achieve unlearning when full access to all training data becomes unavailable. One noteworthy example is Federated Learning (FL), where each participating data holder trains locally, without sharing their training data to the central server. In this paper, we investigate the problem of machine unlearning in FL systems. We start with a formal definition of the unlearning problem in FL and propose a rapid retraining approach to fully erase data samples from a trained FL model. The resulting design allows data holders to jointly conduct the unlearning process efficiently while keeping their training data locally. Our formal convergence and complexity analysis demonstrate that our design can preserve model utility with high efficiency. Extensive evaluations on four real-world datasets illustrate the effectiveness and performance of our proposed realization. ",https://doi.org/10.1109/INFOCOM48880.2022.9796721,2203.07320v1,Yes,noteworthy(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",Evaluating Text-to-Image Generative Models: An Empirical Study on Human   Image Synthesis,1970,"  In this paper, we present an empirical study introducing a nuanced evaluation framework for text-to-image (T2I) generative models, applied to human image synthesis. Our framework categorizes evaluations into two distinct groups: first, focusing on image qualities such as aesthetics and realism, and second, examining text conditions through concept coverage and fairness. We introduce an innovative aesthetic score prediction model that assesses the visual appeal of generated images and unveils the first dataset marked with low-quality regions in generated human images to facilitate automatic defect detection. Our exploration into concept coverage probes the model's effectiveness in interpreting and rendering text-based concepts accurately, while our analysis of fairness reveals biases in model outputs, with an emphasis on gender, race, and age. While our study is grounded in human imagery, this dual-faceted approach is designed with the flexibility to be applicable to other forms of image generation, enhancing our understanding of generative models and paving the way to the next generation of more sophisticated, contextually aware, and ethically attuned generative models. We will release our code, the data used for evaluating generative models and the dataset annotated with defective areas soon. ",Kein DOI-Link verfügbar,2403.05125v1,Yes,innovative(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",A Capture-gated Fast Neutron Detection Method,1970,"  To address the problem of the shortage of neutron detectors used in radiation portal monitors (RPMs), caused by the 3He supply crisis, research on a cadmium-based capture-gated fast neutron detector is presented in this paper. The detector is composed of many 1 cm * 1 cm * 20 cm plastic scintillator cuboids covered by 0.1 mm thick film of cadmium. The detector uses cadmium to absorb thermal neutrons and produce capture gamma-rays to indicate the detection of neutrons, and uses plastic scintillator to moderate neutrons and register gamma-rays. This design removes the volume competing relationship in traditional 3He counter-based fast neutron detectors, which hinders enhancement of the neutron detection efficiency. Detection efficiency of 21.66 +- 1.22% has been achieved with a 40.4 cm * 40.4 cm * 20 cm overall detector volume. This detector can measure both neutrons and gamma-rays simultaneously. A small detector (20.2 cm * 20.2 cm * 20 cm) demonstrated a 3.3 % false alarm rate for a 252Cf source with a neutron yield of 1841 n/s from 50 cm away within 15 seconds measurement time. It also demonstrated a very low (< 0.06%) false alarm rate for a 3.21 * 105 Bq 137Cs source. This detector offers a potential single-detector replacement for both neutron and the gamma-ray detectors in RPM systems. ",https://doi.org/10.1088/1674-1137/40/7/076201,1603.03529v1,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",Characterizing Commodity Serverless Computing Platforms,1970,"  Serverless computing has become a new trending paradigm in cloud computing, allowing developers to focus on the development of core application logic and rapidly construct the prototype via the composition of independent functions. With the development and prosperity of serverless computing, major cloud vendors have successively rolled out their commodity serverless computing platforms. However, the characteristics of these platforms have not been systematically studied. Measuring these characteristics can help developers to select the most adequate serverless computing platform and develop their serverless-based applications in the right way. To fill this knowledge gap, we present a comprehensive study on characterizing mainstream commodity serverless computing platforms, including AWS Lambda, Google Cloud Functions, Azure Functions, and Alibaba Cloud Function Compute. Specifically, we conduct both qualitative analysis and quantitative analysis. In qualitative analysis, we compare these platforms from three aspects (i.e., development, deployment, and runtime) based on their official documentation to construct a taxonomy of characteristics. In quantitative analysis, we analyze the runtime performance of these platforms from multiple dimensions with well-designed benchmarks. First, we analyze three key factors that can influence the startup latency of serverless-based applications. Second, we compare the resource efficiency of different platforms with 16 representative benchmarks. Finally, we measure their performance difference when dealing with different concurrent requests, and explore the potential causes in a black-box fashion. Based on the results of both qualitative and quantitative analysis, we derive a series of findings and provide insightful implications for both developers and cloud vendors. ",Kein DOI-Link verfügbar,2012.00992v3,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",A Comprehensive Study of Jailbreak Attack versus Defense for Large   Language Models,1970,"  Large Language Models (LLMS) have increasingly become central to generating content with potential societal impacts. Notably, these models have demonstrated capabilities for generating content that could be deemed harmful. To mitigate these risks, researchers have adopted safety training techniques to align model outputs with societal values to curb the generation of malicious content. However, the phenomenon of ""jailbreaking"", where carefully crafted prompts elicit harmful responses from models, persists as a significant challenge. This research conducts a comprehensive analysis of existing studies on jailbreaking LLMs and their defense techniques. We meticulously investigate nine attack techniques and seven defense techniques applied across three distinct language models: Vicuna, LLama, and GPT-3.5 Turbo. We aim to evaluate the effectiveness of these attack and defense techniques. Our findings reveal that existing white-box attacks underperform compared to universal techniques and that including special tokens in the input significantly affects the likelihood of successful attacks. This research highlights the need to concentrate on the security facets of LLMs. Additionally, we contribute to the field by releasing our datasets and testing framework, aiming to foster further research into LLM security. We believe these contributions will facilitate the exploration of security measures within this domain. ",Kein DOI-Link verfügbar,2402.13457v2,Yes,"meticulous(1), potent(1), meticulously(1)"
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",LIRE: listwise reward enhancement for preference alignment,1970,"  Recently, tremendous strides have been made to align the generation of Large Language Models (LLMs) with human values to mitigate toxic or unhelpful content. Leveraging Reinforcement Learning from Human Feedback (RLHF) proves effective and is widely adopted by researchers. However, implementing RLHF is complex, and its sensitivity to hyperparameters renders achieving stable performance and scalability challenging. Furthermore, prevailing approaches to preference alignment primarily concentrate on pairwise comparisons, with limited exploration into multi-response scenarios, thereby overlooking the potential richness within the candidate pool. For the above reasons, we propose a new approach: Listwise Reward Enhancement for Preference Alignment (LIRE), a gradient-based reward optimization approach that incorporates the offline rewards of multiple responses into a streamlined listwise framework, thus eliminating the need for online sampling during training. LIRE is straightforward to implement, requiring minimal parameter tuning, and seamlessly aligns with the pairwise paradigm while naturally extending to multi-response scenarios. Moreover, we introduce a self-enhancement algorithm aimed at iteratively refining the reward during training. Our experiments demonstrate that LIRE consistently outperforms existing methods across several benchmarks on dialogue and summarization tasks, with good transferability to out-of-distribution data, assessed using proxy reward models and human annotators. ",Kein DOI-Link verfügbar,2405.13516v2,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",DistillSeq: A Framework for Safety Alignment Testing in Large Language   Models using Knowledge Distillation,1970,"  Large Language Models (LLMs) have showcased their remarkable capabilities in diverse domains, encompassing natural language understanding, translation, and even code generation. The potential for LLMs to generate harmful content is a significant concern. This risk necessitates rigorous testing and comprehensive evaluation of LLMs to ensure safe and responsible use. However, extensive testing of LLMs requires substantial computational resources, making it an expensive endeavor. Therefore, exploring cost-saving strategies during the testing phase is crucial to balance the need for thorough evaluation with the constraints of resource availability. To address this, our approach begins by transferring the moderation knowledge from an LLM to a small model. Subsequently, we deploy two distinct strategies for generating malicious queries: one based on a syntax tree approach, and the other leveraging an LLM-based method. Finally, our approach incorporates a sequential filter-test process designed to identify test cases that are prone to eliciting toxic responses. Our research evaluated the efficacy of DistillSeq across four LLMs: GPT-3.5, GPT-4.0, Vicuna-13B, and Llama-13B. In the absence of DistillSeq, the observed attack success rates on these LLMs stood at 31.5% for GPT-3.5, 21.4% for GPT-4.0, 28.3% for Vicuna-13B, and 30.9% for Llama-13B. However, upon the application of DistillSeq, these success rates notably increased to 58.5%, 50.7%, 52.5%, and 54.4%, respectively. This translated to an average escalation in attack success rate by a factor of 93.0% when compared to scenarios without the use of DistillSeq. Such findings highlight the significant enhancement DistillSeq offers in terms of reducing the time and resource investment required for effectively testing LLMs. ",https://doi.org/10.1145/3650212.3680304,2407.10106v3,Yes,potent(1)
0000-0002-2051-440X,Yi Liu,"The University of Bristol, University of Bristol",Accelerating Sparse Approximate Matrix Multiplication on GPUs,1970,"  Although the matrix multiplication plays a vital role in computational linear algebra, there are few efficient solutions for matrix multiplication of the near-sparse matrices. The Sparse Approximate Matrix Multiply (SpAMM) is one of the algorithms to fill the performance gap neglected by traditional optimizations for dense/sparse matrix multiplication. However, existing SpAMM algorithms fail to exploit the performance potential of GPUs for acceleration. In this paper, we present cuSpAMM, the first parallel SpAMM algorithm optimized for multiple GPUs. Several performance optimizations have been proposed, including algorithm re-design to adapt to the thread parallelism, blocking strategies for memory access optimization, and the acceleration with the tensor core. In addition, we scale cuSpAMM to run on multiple GPUs with an effective load balance scheme. We evaluate cuSpAMM on both synthesized and real-world datasets on multiple GPUs. The experiment results show that cuSpAMM achieves significant performance speedup compared to vendor optimized cuBLAS and cuSPARSE libraries. ",https://doi.org/10.1007/s11227-022-04334-5,2103.13042v1,Yes,potent(1)
0000-0002-6639-9486,Gregory Schwartz,"The University of Bristol, University of Bristol",Retina-inspired Object Motion Segmentation,1970,"  Dynamic Vision Sensors (DVS) have emerged as a revolutionary technology with a high temporal resolution that far surpasses RGB cameras. DVS technology draws biological inspiration from photoreceptors and the initial retinal synapse. Our research showcases the potential of additional retinal functionalities to extract visual features. We provide a domain-agnostic and efficient algorithm for ego-motion compensation based on Object Motion Sensitivity (OMS), one of the multiple robust features computed within the mammalian retina. We develop a framework based on experimental neuroscience that translates OMS' biological circuitry to a low-overhead algorithm. OMS processes DVS data from dynamic scenes to perform pixel-wise object motion segmentation. Using a real and a synthetic dataset, we highlight OMS' ability to differentiate object motion from ego-motion, bypassing the need for deep networks. This paper introduces a bio-inspired computer vision method that dramatically reduces the number of parameters by a factor of 1000 compared to prior works. Our work paves the way for robust, high-speed, and low-bandwidth decision-making for in-sensor computations. ",Kein DOI-Link verfügbar,2408.09454v1,Yes,potent(1)
0000-0002-1767-3901,Richard Owen,"The University of Bristol, University of Bristol",Addressing contingency in algorithmic (mis)information classification:   Toward a responsible machine learning agenda,1970,"  Machine learning (ML) enabled classification models are becoming increasingly popular for tackling the sheer volume and speed of online misinformation and other content that could be identified as harmful. In building these models, data scientists need to take a stance on the legitimacy, authoritativeness and objectivity of the sources of ``truth"" used for model training and testing. This has political, ethical and epistemic implications which are rarely addressed in technical papers. Despite (and due to) their reported high accuracy and performance, ML-driven moderation systems have the potential to shape online public debate and create downstream negative impacts such as undue censorship and the reinforcing of false beliefs. Using collaborative ethnography and theoretical insights from social studies of science and expertise, we offer a critical analysis of the process of building ML models for (mis)information classification: we identify a series of algorithmic contingencies--key moments during model development that could lead to different future outcomes, uncertainty and harmful effects as these tools are deployed by social media platforms. We conclude by offering a tentative path toward reflexive and responsible development of ML tools for moderating misinformation and other harmful content online. ",https://doi.org/10.1080/23299460.2023.2222514,2210.09014v2,Yes,potent(1)
0000-0002-6478-1403,Majid Mirmehdi,"The University of Bristol, University of Bristol",Weakly-Supervised Completion Moment Detection using Temporal Attention,1970,"  Monitoring the progression of an action towards completion offers fine grained insight into the actor's behaviour. In this work, we target detecting the completion moment of actions, that is the moment when the action's goal has been successfully accomplished. This has potential applications from surveillance to assistive living and human-robot interactions. Previous effort required human annotations of the completion moment for training (i.e. full supervision). In this work, we present an approach for moment detection from weak video-level labels. Given both complete and incomplete sequences, of the same action, we learn temporal attention, along with accumulated completion prediction from all frames in the sequence. We also demonstrate how the approach can be used when completion moment supervision is available. We evaluate and compare our approach on actions from three datasets, namely HMDB, UCF101 and RGBD-AC, and show that temporal attention improves detection in both weakly-supervised and fully-supervised settings. ",Kein DOI-Link verfügbar,1910.09920v1,Yes,potent(1)
0000-0002-6478-1403,Majid Mirmehdi,"The University of Bristol, University of Bristol",Automated Radiology Report Generation: A Review of Recent Advances,1970,"  Increasing demands on medical imaging departments are taking a toll on the radiologist's ability to deliver timely and accurate reports. Recent technological advances in artificial intelligence have demonstrated great potential for automatic radiology report generation (ARRG), sparking an explosion of research. This survey paper conducts a methodological review of contemporary ARRG approaches by way of (i) assessing datasets based on characteristics, such as availability, size, and adoption rate, (ii) examining deep learning training methods, such as contrastive learning and reinforcement learning, (iii) exploring state-of-the-art model architectures, including variations of CNN and transformer models, (iv) outlining techniques integrating clinical knowledge through multimodal inputs and knowledge graphs, and (v) scrutinising current model evaluation techniques, including commonly applied NLP metrics and qualitative clinical reviews. Furthermore, the quantitative results of the reviewed models are analysed, where the top performing models are examined to seek further insights. Finally, potential new directions are highlighted, with the adoption of additional datasets from other radiological modalities and improved evaluation methods predicted as important areas of future development. ",https://doi.org/10.1109/RBME.2024.3408456,2405.10842v2,Yes,potent(2)
0000-0002-6478-1403,Majid Mirmehdi,"The University of Bristol, University of Bristol",A Guide to the SPHERE 100 Homes Study Dataset,1970,"  The SPHERE project has developed a multi-modal sensor platform for health and behavior monitoring in residential environments. So far, the SPHERE platform has been deployed for data collection in approximately 50 homes for duration up to one year. This technical document describes the format and the expected content of the SPHERE dataset(s) under preparation. It includes a list of some data quality problems (both known to exist in the dataset(s) and potential ones), their workarounds, and other information important to people working with the SPHERE data, software, and hardware. This document does not aim to be an exhaustive descriptor of the SPHERE dataset(s); it also does not aim to discuss or validate the potential scientific uses of the SPHERE data. ",Kein DOI-Link verfügbar,1805.11907v2,Yes,potent(2)
0009-0005-6435-6617,Alexander Heemels,Delft University of Technology,Gradient descent-based freeform optics design using algorithmic   differentiable non-sequential ray tracing,1970,"  Algorithmic differentiable ray tracing is a new paradigm that allows one to solve the forward problem of how light propagates through an optical system while obtaining gradients of the simulation results with respect to parameters specifying the optical system. Specifically, the use of algorithmically differentiable non-sequential ray tracing provides an opportunity in the field of illumination design. We demonstrate its potential by designing freeform lenses that project a prescribed irradiance distribution onto a plane. The challenge consists in finding a suitable surface geometry of the lens so that the light emitted by a light source is redistributed into a desired irradiance distribution. We discuss the crucial steps allowing the non-sequential ray tracer to be differentiable. The obtained gradients are used to optimize the geometry of the freeform, and we investigate the effectiveness of adding a multi-layer perceptron neural network to the optimization that outputs parameters defining the freeform lens. Lenses are designed for various sources such as collimated ray bundles or point sources, and finally, a grid of point sources approximating an extended source. The obtained lens designs are finally validated using the commercial non-sequential ray tracer LightTools. ",https://doi.org/10.1007/s11081-023-09841-9,2302.12031v1,Yes,potent(1)
0000-0003-3610-594X,Yuan Chen,Delft University of Technology,Representation Learning for Integrating Multi-domain Outcomes to   Optimize Individualized Treatments,1970,"  For mental disorders, patients' underlying mental states are non-observed latent constructs which have to be inferred from observed multi-domain measurements such as diagnostic symptoms and patient functioning scores. Additionally, substantial heterogeneity in the disease diagnosis between patients needs to be addressed for optimizing individualized treatment policy in order to achieve precision medicine. To address these challenges, we propose an integrated learning framework that can simultaneously learn patients' underlying mental states and recommend optimal treatments for each individual. This learning framework is based on the measurement theory in psychiatry for modeling multiple disease diagnostic measures as arising from the underlying causes (true mental states). It allows incorporation of the multivariate pre- and post-treatment outcomes as well as biological measures while preserving the invariant structure for representing patients' latent mental states. A multi-layer neural network is used to allow complex treatment effect heterogeneity. Optimal treatment policy can be inferred for future patients by comparing their potential mental states under different treatments given the observed multi-domain pre-treatment measurements. Experiments on simulated data and a real-world clinical trial data show that the learned treatment polices compare favorably to alternative methods on heterogeneous treatment effects, and have broad utilities which lead to better patient outcomes on multiple domains. ",Kein DOI-Link verfügbar,2011.00094v1,Yes,potent(1)
0000-0002-2469-7207,Scott Spurlock,"The University of Glasgow, University of Glasgow",Imagining Computing Education Assessment after Generative AI,1970,"  In the contemporary landscape of computing education, the ubiquity of Generative Artificial Intelligence has significantly disrupted traditional assessment methods, rendering them obsolete and prompting educators to seek innovative alternatives. This research paper explores the challenges posed by Generative AI in the assessment domain and the persistent attempts to circumvent its impact. Despite various efforts to devise workarounds, the academic community is yet to find a comprehensive solution. Amidst this struggle, ungrading emerges as a potential yet under-appreciated solution to the assessment dilemma. Ungrading, a pedagogical approach that involves moving away from traditional grading systems, has faced resistance due to its perceived complexity and the reluctance of educators to depart from conventional assessment practices. However, as the inadequacies of current assessment methods become increasingly evident in the face of Generative AI, the time is ripe to reconsider and embrace ungrading. ",Kein DOI-Link verfügbar,2401.04601v1,Yes,"innovative(1), potent(1)"
0000-0003-0988-8854,Colin McInnes,University of Glasgow,Concept-of-Operations Disposal Analysis of Spacecraft by Gossamer   Structure,1970,"  A gossamer structure for end-of-life disposal of spacecraft to mitigate space debris is considered in comparison with other end-of-life disposal concepts to determine when it would be preferable. A needs analysis, potential use cases, and concept-of-operations are developed. A survey of disposal strategies is presented for comparison prior to a down-selection of viable competing techniques; solar sailing, high and low-thrust propulsion, and electrodynamic tethers. A parametric comparison of the down-selection competing techniques is presented. Exploiting solar radiation pressure on the structure is of limited value. Atmospheric drag augmentation was found to be of most benefit for end-of-life disposal when an entirely passive means is required, allowing the gossamer device to act as a fail-safe. This is applicable to only low and medium mass spacecraft, or spacecraft that are unlikely to survive atmospheric re-entry, hence minimizing risk to human life. It does not significantly alter the operating ceiling altitude but does the maximum allowable end-of-life mass. Peak mass benefit occurs in the altitude range 550 - 650 km and is largely independent of de-orbit time. ",Kein DOI-Link verfügbar,1410.6727v1,Yes,potent(1)
0000-0001-6061-5355,Alban Ponse,"CWI and University of Amsterdam, University of Amsterdam",Fracpairs and fractions over a reduced commutative ring,1970,"  In the well-known construction of the field of fractions of an integral domain, division by zero is excluded. We introduce ""fracpairs"" as pairs subject to laws consistent with the use of the pair as a fraction, but do not exclude denominators to be zero. We investigate fracpairs over a reduced commutative ring (a commutative ring that has no nonzero nilpotent elements) and provide these with natural definitions for addition, multiplication, and additive and multiplicative inverse. We find that modulo a simple congruence these fracpairs constitute a ""common meadow"", which is a commutative monoid both for addition and multiplication, extended with a weak additive inverse, a multiplicative inverse except for zero, and an additional element ""a"" that is the image of the multiplicative inverse on zero and that propagates through all operations. Considering ""a"" as an error-value supports the intuition.   The equivalence classes of fracpairs thus obtained are called common cancellation fractions (cc-fractions), and cc-fractions over the integers constitute a homomorphic pre-image of the common meadow Qa, the field Q of rational numbers expanded with an a-totalized inverse. Moreover, the initial common meadow is isomorphic to the initial algebra of cc-fractions over the integer numbers. Next, we define canonical term algebras for cc-fractions over the integers and some meadows that model the rational numbers expanded with a totalized inverse, and provide some negative results concerning their associated term rewriting properties. Then we consider reduced commutative rings in which the sum of two squares plus one cannot be a zero divisor: by extending the equivalence relation on fracpairs we obtain an initial algebra that is isomorphic to Qa. Finally, we express negative conjectures concerning alternative specifications for these (concrete) datatypes. ",https://doi.org/10.1016/j.indag.2016.01.007,1411.4410v2,Yes,potent(1)
0000-0003-1291-7951,Shashank Gupta,University of Amsterdam,"Comparison of pipeline, sequence-to-sequence, and GPT models for   end-to-end relation extraction: experiments with the rare disease use-case",1970,"  End-to-end relation extraction (E2ERE) is an important and realistic application of natural language processing (NLP) in biomedicine. In this paper, we aim to compare three prevailing paradigms for E2ERE using a complex dataset focused on rare diseases involving discontinuous and nested entities. We use the RareDis information extraction dataset to evaluate three competing approaches (for E2ERE): NER $\rightarrow$ RE pipelines, joint sequence to sequence models, and generative pre-trained transformer (GPT) models. We use comparable state-of-the-art models and best practices for each of these approaches and conduct error analyses to assess their failure modes. Our findings reveal that pipeline models are still the best, while sequence-to-sequence models are not far behind; GPT models with eight times as many parameters are worse than even sequence-to-sequence models and lose to pipeline models by over 10 F1 points. Partial matches and discontinuous entities caused many NER errors contributing to lower overall E2E performances. We also verify these findings on a second E2ERE dataset for chemical-protein interactions. Although generative LM-based methods are more suitable for zero-shot settings, when training data is available, our results show that it is better to work with more conventional models trained and tailored for E2ERE. More innovative methods are needed to marry the best of the both worlds from smaller encoder-decoder pipeline models and the larger GPT models to improve E2ERE. As of now, we see that well designed pipeline models offer substantial performance gains at a lower cost and carbon footprint for E2ERE. Our contribution is also the first to conduct E2ERE for the RareDis dataset. ",Kein DOI-Link verfügbar,2311.13729v2,Yes,innovative(1)
0000-0003-1291-7951,Shashank Gupta,University of Amsterdam,Optimal Baseline Corrections for Off-Policy Contextual Bandits,1970,"  The off-policy learning paradigm allows for recommender systems and general ranking applications to be framed as decision-making problems, where we aim to learn decision policies that optimize an unbiased offline estimate of an online reward metric. With unbiasedness comes potentially high variance, and prevalent methods exist to reduce estimation variance. These methods typically make use of control variates, either additive (i.e., baseline corrections or doubly robust methods) or multiplicative (i.e., self-normalisation). Our work unifies these approaches by proposing a single framework built on their equivalence in learning scenarios. The foundation of our framework is the derivation of an equivalent baseline correction for all of the existing control variates. Consequently, our framework enables us to characterize the variance-optimal unbiased estimator and provide a closed-form solution for it. This optimal estimator brings significantly improved performance in both evaluation and learning, and minimizes data requirements. Empirical observations corroborate our theoretical findings. ",https://doi.org/10.1145/3640457.3688105,2405.05736v2,Yes,potent(1)
0000-0003-1291-7951,Shashank Gupta,University of Amsterdam,Anomalous Threshold Reduction from <100> Uniaxial Strain for a   Low-Threshold Ge Laser,1970,"  We theoretically investigate the effect of <100> uniaxial strain on a Ge-on-Si laser using deformation potentials. We predict a sudden and dramatic ~200x threshold reduction upon applying sufficient uniaxial tensile strain to the Ge gain medium. This anomalous reduction is accompanied by an abrupt jump in the emission wavelength and is explained by how the light-hole band raises relative to the heavy-hole band due to uniaxial strain. Approximately 3.2% uniaxial strain is required to achieve this anomalous threshold reduction for 1x1019 cm-3 n-type doping, and a complex interaction between uniaxial strain and n-type doping is observed. This anomalous threshold reduction represents a substantial performance advantage for uniaxially strained Ge lasers relative to other forms of Ge band engineering such as biaxial strain or tin alloying. Achieving this critical combination of uniaxial strain and doping for the anomalous threshold reduction is dramatically more relevant to practical devices than realizing a direct band gap. ",https://doi.org/10.1016/j.optcom.2016.05.030,1506.08403v1,Yes,potent(1)
0000-0003-1291-7951,Shashank Gupta,University of Amsterdam,AppWorld: A Controllable World of Apps and People for Benchmarking   Interactive Coding Agents,1970,"  Autonomous agents that address day-to-day digital tasks (e.g., ordering groceries for a household), must not only operate multiple apps (e.g., notes, messaging, shopping app) via APIs, but also generate rich code with complex control flow in an iterative manner based on their interaction with the environment. However, existing benchmarks for tool use are inadequate, as they only cover tasks that require a simple sequence of API calls.   To remedy this gap, we built $\textbf{AppWorld Engine}$, a high-quality execution environment (60K lines of code) of 9 day-to-day apps operable via 457 APIs and populated with realistic digital activities simulating the lives of ~100 fictitious users. We then created $\textbf{AppWorld Benchmark}$ (40K lines of code), a suite of 750 natural, diverse, and challenging autonomous agent tasks requiring rich and interactive code generation. It supports robust programmatic evaluation with state-based unit tests, allowing for different ways of completing a task while also checking for unexpected changes, i.e., collateral damage. The state-of-the-art LLM, GPT-4o, solves only ~49% of our 'normal' tasks and ~30% of 'challenge' tasks, while other models solve at least 16% fewer. This highlights the benchmark's difficulty and AppWorld's potential to push the frontiers of interactive coding agents. The project website is available at https://appworld.dev/. ",Kein DOI-Link verfügbar,2407.18901v1,Yes,potent(1)
0000-0003-1291-7951,Shashank Gupta,University of Amsterdam,Direct Bandgap Light Emission from Strained Ge Nanowires Coupled with   High-Q Optical Cavities,1970,"  A silicon-compatible light source is the final missing piece for completing high-speed, low-power on-chip optical interconnects. In this paper, we present a germanium-based light emitter that encompasses all the aspects of potential low-threshold lasers: highly strained germanium gain medium, strain-induced pseudo-heterostructure, and high-Q optical cavity. Our light emitting structure presents greatly enhanced photoluminescence into cavity modes with measured quality factors of up to 2,000. The emission wavelength is tuned over more than 400 nm with a single lithography step. We find increased optical gain in optical cavities formed with germanium under high (>2.3%) tensile strain. Through quantitative analysis of gain/loss mechanisms, we find that free carrier absorption from the hole bands dominates the gain, resulting in no net gain even from highly strained, n-type doped germanium. ",https://doi.org/10.1021/acs.nanolett.5b03976,1508.01255v1,Yes,potent(1)
0000-0003-1291-7951,Shashank Gupta,University of Amsterdam,Low Resistance III-V Hetero-contacts to N-Ge,1970,"  We experimentally study III-V/Ge heterostructure and demonstrate InGaAs hetero-contacts to n-Ge with a wide range of In % and achieve low contact resistivity ($\rho_C$) of $5\times10^{-8} \Omega\cdot cm^2$ for Ge doping of $3 \times 10^{19} cm^{-3}$. This results from re-directing the charge neutrality level (CNL) near the conduction band and benefiting from low effective mass for high electron transmission. For the first time, we observe that the heterointerface presents no temperature dependence despite the two different conduction minimum valley locations of III-V ($\Gamma$-valley) and Ge (L-valley), which potentially stems from elastic trap-assisted tunneling through defect states at the interface generated by dislocations. The hetero-interface plays a dominant role in the overall $\rho_C$ below $\approx 1 \times 10^{-7} \Omega \cdot cm^2$, which can be further improved with large active dopant concentration in Ge by co-doping. ",Kein DOI-Link verfügbar,2106.15099v1,Yes,potent(1)
0000-0003-1291-7951,Shashank Gupta,University of Amsterdam,Low-threshold optically pumped lasing in highly strained Ge nanowires,1970,"  The integration of efficient, miniaturized group IV lasers into CMOS architecture holds the key to the realization of fully functional photonic-integrated circuits. Despite several years of progress, however, all group IV lasers reported to date exhibit impractically high thresholds owing to their unfavorable bandstructures. Highly strained germanium with its fundamentally altered bandstructure has emerged as a potential low-threshold gain medium, but there has yet to be any successful demonstration of lasing from this seemingly promising material system. Here, we demonstrate a low-threshold, compact group IV laser that employs germanium nanowire under a 1.6% uniaxial tensile strain as the gain medium. The amplified material gain in strained germanium can sufficiently surmount optical losses at 83 K, thus allowing the first observation of multimode lasing with an optical pumping threshold density of ~3.0 kW cm^-^2. Our demonstration opens up a new horizon of group IV lasers for photonic-integrated circuits. ",https://doi.org/10.1038/s41467-017-02026-w,1708.04568v1,Yes,potent(1)
0000-0002-9484-6474,Constantinos Constantinou,"University Of Birmingham, University of Birmingham",Enforcing causality in nonrelativistic equations of state at finite   temperature,1970,"  We present a thermodynamically consistent method by which equations of state based on nonrelativistic potential models can be modified so that they respect causality at high densities, both at zero and finite temperature (entropy). We illustrate the application of the method using the high density phase parametrization of the well known APR model in its pure neutron matter configuration as an example. We also show that, for models with only contact interactions, the adiabatic speed of sound is independent of the temperature in the limit of very large temperature. This feature is approximately valid for models with finite-range interactions as well, insofar as the temperature dependence they introduce to the Landau effective mass is weak. In addition, our study reveals that in first principle nonrelativistic models of hot and dense matter, contributions from higher than two-body interactions must be screened at high density to preserve causality. ",https://doi.org/10.1103/PhysRevC.95.055802,1702.06952v1,Yes,potent(1)
0000-0002-9484-6474,Constantinos Constantinou,"University Of Birmingham, University of Birmingham",Framework for phase transitions between the Maxwell and Gibbs   constructions,1970,"  By taking the nucleon-to-quark phase transition within a neutron star as an example, we present a thermodynamically consistent method to calculate the equation of state of ambient matter so that transitions that are intermediate to those of the familiar Maxwell and Gibbs constructions can be described. This method does not address the poorly known surface tension between the two phases microscopically (as, for example, in the calculation of the core pasta phases via the Wigner-Seitz approximation) but instead combines the local and global charge neutrality conditions characteristic of the Maxwell and Gibbs constructions, respectively. Overall charge neutrality is achieved by dividing the leptons to those that obey local charge neutrality (Maxwell) and those that maintain global charge neutrality (Gibbs). The equation of state is obtained by using equilibrium constraints derived from minimizing the total energy density. The results of this minimization are then used to calculate neutron star mass-radius curves, tidal deformabilities, equilibrium and adiabatic sound speeds, and nonradial $g$-mode oscillation frequencies for several intermediate constructions. Various quantities of interest transform smoothly from their Gibbs structures to those of Maxwell as the local-to-total electron ratio $\eta$, introduced to mimic the hadron-to-quark interface tension from $0$ (Gibbs) to $\infty$ (Maxwell), is raised from $0$ to $1$. A notable exception is the $g$-mode frequency for the specific case of $\eta=1$ for which a gap appears between the quark and hadronic branches. ",https://doi.org/10.1103/PhysRevD.107.074013,2302.04289v2,Yes,notable(1)
0000-0002-9484-6474,Constantinos Constantinou,"University Of Birmingham, University of Birmingham",Degenerate limit thermodynamics beyond leading order for models of dense   matter,1970,"  Analytical formulas for next-to-leading order temperature corrections to the thermal state variables of interacting nucleons in bulk matter are derived in the degenerate limit. The formalism developed is applicable to a wide class of non-relativistic and relativistic models of hot and dense matter currently used in nuclear physics and astrophysics (supernovae, proto-neutron stars and neutron star mergers) as well as in condensed matter physics. We consider the general case of arbitrary dimensionality of momentum space and an arbitrary degree of relativity (for relativistic mean-field theoretical models). For non-relativistic zero-range interactions, knowledge of the Landau effective mass suffices to compute next-to-leading order effects, but in the case of finite-range interactions, momentum derivatives of the Landau effective mass function up to second order are required. Numerical computations are performed to compare results from our analytical formulas with the exact results for zero- and finite-range potential and relativistic mean-field theoretical models. In all cases, inclusion of next-to-leading order temperature effects substantially extends the ranges of partial degeneracy for which the analytical treatment remains valid. ",Kein DOI-Link verfügbar,1507.07874v1,Yes,potent(1)
0000-0002-9484-6474,Constantinos Constantinou,"University Of Birmingham, University of Birmingham",Thermal properties of supernova matter: The bulk homogeneous phase,1970,"  We investigate the thermal properties of the potential model equation of state of Akmal, Pandharipande and Ravenhall. This equation of state approximates the microscopic model calculations of Akmal and Pandharipande, which feature a neutral pion condensate. We treat the bulk homogeneous phase for isospin asymmetries ranging from symmetric nuclear matter to pure neutron matter and for temperatures and densities relevant for simulations of core-collapse supernovae, proto-neutron stars, and neutron star mergers. Numerical results of the state variables are compared with those of a typical Skyrme energy density functional with similar properties at nuclear densities, but which differs substantially at supra-nuclear densities. Analytical formulas, which are applicable to non-relativistic potential models such as the equations of state we are considering, are derived for all state variables and their thermodynamic derivatives. A highlight of our work is its focus on thermal response functions in the degenerate and non-degenerate situations, which allow checks of the numerical calculations for arbitrary degeneracy. These functions are sensitive to the density dependent effective masses of neutrons and protons, which determine the thermal properties in all regimes of degeneracy. We develop the ""thermal asymmetry free energy"" and establish its relation to the more commonly used nuclear symmetry energy. We also explore the role of the pion condensate at supra-nuclear densities and temperatures. Tables of matter properties as functions of baryon density, composition (i.e., proton fraction) and temperature are being produced which are suitable for use in astrophysical simulations of supernovae and neutron stars. ",https://doi.org/10.1103/PhysRevC.89.065802,1402.6348v1,Yes,potent(2)
0000-0002-9484-6474,Constantinos Constantinou,"University Of Birmingham, University of Birmingham",Deconfinement Phase Transition under Chemical Equilibrium,1970,"  In this work, we investigate how the assumption of chemical equilibrium with leptons affects the deconfinement phase transition to quark matter. This is done within the framework of the Chiral Mean Field model (CMF) allowing for non-zero net strangeness, corresponding to the conditions found in astrophysical scenarios. We build 3-dimensional QCD phase diagrams with temperature, baryon chemical potential, and either charge or isospin fraction or chemical potential to show how the deconfinement region collapses to a line in the special case of chemical equilibrium, such as the one established the interior of cold catalyzed neutron stars. ",https://doi.org/10.1002/asna.202113932,2011.11686v1,Yes,potent(2)
0000-0001-8500-7401,Sara Hassan,"The University of Birmingham, University of Birmingham",Microservice Transition and its Granularity Problem: A Systematic   Mapping Study,1970,"  Microservices have gained wide recognition and acceptance in software industries as an emerging architectural style for autonomic, scalable, and more reliable computing. The transition to microservices has been highly motivated by the need for better alignment of technical design decisions with improving value potentials of architectures. Despite microservices' popularity, research still lacks disciplined understanding of transition and consensus on the principles and activities underlying ""micro-ing"" architectures. In this paper, we report on a systematic mapping study that consolidates various views, approaches and activities that commonly assist in the transition to microservices. The study aims to provide a better understanding of the transition; it also contributes a working definition of the transition and technical activities underlying it. We term the transition and technical activities leading to microservice architectures as microservitization. We then shed light on a fundamental problem of microservitization: microservice granularity and reasoning about its adaptation as first-class entities. This study reviews state-of-the-art and -practice related to reasoning about microservice granularity; it reviews modelling approaches, aspects considered, guidelines and processes used to reason about microservice granularity. This study identifies opportunities for future research and development related to reasoning about microservice granularity. ",Kein DOI-Link verfügbar,1903.11665v1,Yes,potent(1)
0000-0003-3437-2876,Chris Parker,"The University of Birmingham, University of Birmingham",A note on groups in which the centraliser of every element of order 5 is   a 5-group,1970,"  The main theorem in this article shows that a group of odd order which admits the alternating group of degree 5 with an element of order 5 acting fixed point freely is nilpotent of class at most two. For all odd primes r, other than 5, we give a class two r-group which admits the alternating group of degree 5 in such a way. This theorem corrects an earlier result which asserts that such class two groups do not exist. The result allows us to state a theorem giving precise information about groups in which the centralizer of every element of order 5 has centralizer a 5-group. ",Kein DOI-Link verfügbar,1103.3581v2,Yes,potent(1)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",Gap and screening in Raman scattering of a Bose condensed gas,1970,"  We propose different spectroscopic methods to explore the nature of the thermal excitations of a trapped Bose condensed gas: 1) a four photon process to probe the uniform region in the trap center: 2) a stimulated Raman process in order to analyze the influence of a momentum transfer in the resulting scattered atom momentum distribution. We apply these methods to address specifically the energy spectrum and the scattering amplitude of these excitations in a transition between two hyperfine levels of the gas atoms. In particular, we exemplify the potential offered by these proposed techniques by contrasting the spectrum expected, from the {\it non conserving} Bogoliubov approximation valid for weak depletion, to the spectrum of the finite temperature extensions like the {\it conserving} generalized random phase approximation (GRPA). Both predict the existence of the Bogoliubov collective excitations but the GRPA approximation distinguishes them from the single atom excitations with a gapped and parabolic dispersion relation and accounts for the dynamical screening of any external perturbation applied to the gas. We propose two feasible experiments, one concerns the observation of the gap associated to this second branch of excitations and the other deals with this screening effect. ",https://doi.org/10.1209/0295-5075/88/60008,0903.1009v2,Yes,potent(1)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",Evolution from a Bose-Einstein condensate to a Tonks-Girardeau gas: An   exact diagonalization study,1970,"  We study ground state properties of spinless, quasi one-dimensional bosons which are confined in a harmonic trap and interact via repulsive delta-potentials. We use the exact diagonalization method to analyze the pair correlation function, as well as the density, the momentum distribution, different contributions to the energy and the population of single-particle orbitals in the whole interaction regime. In particular, we are able to trace the fascinating transition from bosonic to fermi-like behavior in characteristic features of the momentum distribution which is accessible to experiments. Our calculations yield quantitative measures for the interaction strength limiting the mean-field regime on one side and the Tonks-Girardeau regime on the other side of an intermediate regime. ",https://doi.org/10.1103/PhysRevA.75.013614,cond-mat/0604673v4,Yes,potent(1)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",Self-Trapping of Bosons and Fermions in Optical Lattices,1970,"  We theoretically investigate the enhanced localization of bosonic atoms by fermionic atoms in three-dimensional optical lattices and find a self-trapping of the bosons for attractive boson-fermion interaction. Because of this mutual interaction, the fermion orbitals are substantially squeezed, which results in a strong deformation of the effective potential for bosons. This effect is enhanced by an increasing bosonic filling factor leading to a large shift of the transition between the superfluid and the Mott-insulator phase. We find a nonlinear dependency of the critical potential depth on the boson-fermion interaction strength. The results, in general, demonstrate the important role of higher Bloch bands for the physics of attractively interacting quantum gas mixtures in optical lattices and are of direct relevance to recent experiments with 87Rb - 40K mixtures, where a large shift of the critical point has been found. ",https://doi.org/10.1103/PhysRevLett.101.050402,0711.2975v2,Yes,potent(2)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",Doppler Compensated Cavity For Atom Interferometry,1970,"  We propose and demonstrate a scheme to enable Doppler compensation within optical cavities for atom interferometry at significantly increased mode diameters. This has the potential to overcome the primary limitations in cavity enhancement for atom interferometry, circumventing the cavity linewidth limit and enabling mode filtering, power enhancement, and a large beam diameter simultaneously. This approach combines a magnified linear cavity with an intracavity Pockels cell. The Pockels cell introduces a voltage tunable birefringence allowing the cavity mode frequencies to track the Raman lasers as they scan to compensate for gravitationally induced Doppler shifts, removing the dominant limitation of current cavity enhanced systems. A cavity is built to this geometry and shown to simultaneously realize the capability required for Doppler compensation, with a 5.04~mm $1/e^{2}$ diameter beam waist and an enhancement factor of $>$5x at a finesse of 35. Furthermore, this has a tunable Gouy phase, allowing the suppression of higher order spatial modes and the avoidance of regions of instability. This approach can therefore enable enhanced contrast and longer atom interferometry times while also enabling the key features of cavity enhanced atom interferometry, power enhancement and the reduction of aberrations. This is relevant to future reductions in the optical power requirement of quantum technology, or in providing enhanced performance for atom interferometers targeting fundamental science. ",Kein DOI-Link verfügbar,2012.07792v1,Yes,potent(1)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",Heteronuclear molecules in an optical lattice: Theory and experiment,1970,"  We study properties of two different atoms at a single optical lattice site at a heteronuclear atomic Feshbach resonance. We calculate the energy spectrum, the efficiency of rf association and the lifetime as a function of magnetic field and compare the results with the experimental data obtained for K-40 and Rb-87 [C. Ospelkaus et al., Phys. Rev. Lett. 97, 120402 (2006)]. We treat the interaction in terms of a regularized delta function pseudopotential and consider the general case of particles with different trap frequencies, where the usual approach of separating center-of-mass and relative motion fails. We develop an exact diagonalization approach to the coupling between center-of-mass and relative motion and numerically determine the spectrum of the system. At the same time, our approach allows us to treat the anharmonicity of the lattice potential exactly. Within the pseudopotential model, the center of the Feshbach resonance can be precisely determined from the experimental data. ",https://doi.org/10.1103/PhysRevA.77.032726,cond-mat/0703322v4,Yes,potent(3)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",A Dielectric Metasurface Optical Chip for the Generation of Cold Atoms,1970,"  Compact and robust cold atom sources are increasingly important for quantum research, especially for transferring cutting-edge quantum science into practical applications. In this letter, we report on a novel scheme that utilizes a metasurface optical chip to replace the conventional bulky optical elements used to produce a cold atomic ensemble with a single incident laser beam, which is split by the metasurface into multiple beams of the desired polarization states. Atom numbers $~10^7$ and temperatures (about 35 ${\mu}$K) of relevance to quantum sensing are achieved in a compact and robust fashion. Our work highlights the substantial progress towards fully integrated cold atom quantum devices by exploiting metasurface optical chips, which may have great potential in quantum sensing, quantum computing and other areas. ",https://doi.org/10.1126/sciadv.abb6667,2008.01356v1,Yes,potent(1)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",Additive manufacturing of magnetic shielding and ultra-high vacuum   flange for cold atom sensors,1970,"  Recent advances in the understanding and control of quantum technologies, such as those based on cold atoms, have resulted in devices with extraordinary metrological sensitivities. To realise this potential outside of a lab environment the size, weight and power consumption need to be reduced. Here we demonstrate the use of laser powder bed fusion, an additive manufacturing technique, as a production technique for the components that make up quantum sensors. As a demonstration we have constructed two key components using additive manufacturing, namely magnetic shielding and vacuum chambers. The initial prototypes for magnetic shields show shielding factors within a factor of 3 of conventional approaches. The vacuum demonstrator device shows that 3D-printed titanium structures are suitable for use as vacuum chambers, with the test system reaching base pressures of $5 \pm 0.5 \times 10^{-10}$ mbar. These demonstrations show considerable promise for the use of additive manufacturing for cold atom based quantum technologies, in future enabling improved integrated structures, allowing for the reduction in size, weight and assembly complexity. ",Kein DOI-Link verfügbar,1710.08279v2,Yes,potent(1)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",Absolute frequency measurement of the $^2$S$_{1/2} \rightarrow   ^2$F$_{7/2}$ optical clock transition in $^{171}$Yb$^+$ with an uncertainty   of $4\times 10^{-16}$ using a frequency link to International Atomic Time,1970,"  The highly forbidden $^2$S$_{1/2} \rightarrow ^2$F$_{7/2}$ electric octupole transition in $^{171}$Yb$^+$ is a potential candidate for a redefinition of the SI second. We present a measurement of the absolute frequency of this optical transition, performed using a frequency link to International Atomic Time to provide traceability to the SI second. The $^{171}$Yb$^+$ optical frequency standard was operated for 76% of a 25-day period, with the absolute frequency measured to be 642 121 496 772 645.14(26) Hz. The fractional uncertainty of $4.0 \times 10 ^{-16}$ is comparable to that of the best previously reported measurement, which was made by a direct comparison to local caesium primary frequency standards. ",https://doi.org/10.1080/09500340.2017.1384514,1707.00646v2,Yes,potent(1)
0000-0002-7334-1848,Kai Bongs,"The University of Birmingham, University of Birmingham",SAI: a compact atom interferometer for future space missions,1970,"  Atom interferometry represents a quantum leap in the technology for the ultra-precise monitoring of accelerations and rotations and, therefore, for all the science that relies on the latter quantities. These sensors evolved from a new kind of optics based on matter-waves rather than light-waves and might result in an advancement of the fundamental detection limits by several orders of magnitude. Matter-wave optics is still a young, but rapidly progressing science. The Space Atom Interferometer project (SAI), funded by the European Space Agency, in a multi-pronged approach aims to investigate both experimentally and theoretically the various aspects of placing atom interferometers in space: the equipment needs, the realistically expected performance limits and potential scientific applications in a micro-gravity environment considering all aspects of quantum, relativistic and metrological sciences. A drop-tower compatible prototype of a single-axis atom interferometry accelerometer is under construction. At the same time the team is studying new schemes, e.g. based on degenerate quantum gases as source for the interferometer. A drop-tower compatible atom interferometry acceleration sensor prototype has been designed, and the manufacturing of its subsystems has been started. A compact modular laser system for cooling and trapping rubidium atoms has been assembled. A compact Raman laser module, featuring outstandingly low phase noise, has been realized. Possible schemes to implement coherent atomic sources in the atom interferometer have been experimentally demonstrated. ",Kein DOI-Link verfügbar,1003.1481v1,Yes,potent(1)
0000-0001-8376-6498,Xiaocheng Shang,"The University of Birmingham, University of Birmingham",Time correlation functions of equilibrium and nonequilibrium Langevin   dynamics: Derivations and numerics using random numbers,1970,"  We study the time correlation functions of coupled linear Langevin dynamics without and with inertia effects, both analytically and numerically. The model equation represents the physical behavior of a harmonic oscillator in two or three dimensions in the presence of friction, noise, and an external field with both rotational and deformational components. This simple model plays pivotal roles in understanding more complicated processes. The presented analytical solution serves as a test of numerical integration schemes, its derivation is presented in a fashion that allows to be repeated directly in a classroom. While the results in the absence of fields (equilibrium) or confinement (free particle) are omnipresent in the literature, we write down, apparently for the first time, the full nonequilibrium results that may correspond, e.g., to a Hookean dumbbell embedded in a macroscopically homogeneous shear or mixed flow field. We demonstrate how the inertia results reduce to their noninertia counterparts in the nontrivial limit of vanishing mass. While the results are derived using basic integrations over Dirac delta distributions, we mention its relationship with alternative approaches involving (i) Fourier transforms, that seems advantageous only if the measured quantities also reside in Fourier space, and (ii) a Fokker--Planck equation and the moments of the probability distribution. The results, verified by numerical experiments, provide additional means of measuring the performance of numerical methods for such systems. It should be emphasized that this manuscript provides specific details regarding the derivations of the time correlation functions as well as the implementations of various numerical methods, so that it can serve as a standalone piece as part of education in the framework of stochastic differential equations and calculus. ",https://doi.org/10.1137/19M1255471,1810.12650v2,Yes,pivotal(1)
0000-0001-5532-3622,Hannah Middleton,"The University of Birmingham, University of Birmingham",Implications of pulsar timing array observations for LISA detections of   massive black hole binaries,1970,"  Pulsar timing arrays (PTAs) and the Laser Interferometer Space Antenna (LISA) will open complementary observational windows on massive black-hole binaries (MBHBs), i.e., with masses in the range $\sim 10^6 - 10^{10}\,$ M$_{\odot}$. While PTAs may detect a stochastic gravitational-wave background from a population of MBHBs, during operation LISA will detect individual merging MBHBs. To demonstrate the profound interplay between LISA and PTAs, we estimate the number of MBHB mergers that one can expect to observe with LISA by extrapolating direct observational constraints on the MBHB merger rate inferred from PTA data. For this, we postulate that the common signal observed by PTAs (and consistent with the increased evidence recently reported) is an astrophysical background sourced by a single MBHB population. We then constrain the LISA detection rate, $\mathcal{R}$, in the mass-redshift space by combining our Bayesian-inferred merger rate with LISA's sensitivity to spin-aligned, inspiral-merger-ringdown waveforms. Using an astrophysically-informed formation model, we predict a 95$\%$ upper limit on the detection rate of $\mathcal{R} < 134\,{\rm yr}^{-1}$ for binaries with total masses in the range $10^7 - 10^8\,$ M$_{\odot}$. For higher masses, i.e., $>10^8\,$ M$_{\odot}$, we find $\mathcal{R} < 2\,(1)\,\mathrm{yr}^{-1}$ using an astrophysically-informed (agnostic) formation model, rising to $11\,(6)\,\mathrm{yr}^{-1}$ if the LISA sensitivity bandwidth extends down to $10^{-5}$ Hz. Forecasts of LISA science potential with PTA background measurements should improve as PTAs continue their search. ",Kein DOI-Link verfügbar,2305.05955v2,Yes,potent(1)
0000-0001-5532-3622,Hannah Middleton,"The University of Birmingham, University of Birmingham",Parameter estimation on gravitational waves from neutron-star binaries   with spinning components,1970,"  Inspiraling binary neutron stars are expected to be one of the most significant sources of gravitational-wave signals for the new generation of advanced ground-based detectors. We investigate how well we could hope to measure properties of these binaries using the Advanced LIGO detectors, which began operation in September 2015. We study an astrophysically motivated population of sources (binary components with masses $1.2~\mathrm{M}_\odot$--$1.6~\mathrm{M}_\odot$ and spins of less than $0.05$) using the full LIGO analysis pipeline. While this simulated population covers the observed range of potential binary neutron-star sources, we do not exclude the possibility of sources with parameters outside these ranges; given the existing uncertainty in distributions of mass and spin, it is critical that analyses account for the full range of possible mass and spin configurations. We find that conservative prior assumptions on neutron-star mass and spin lead to average fractional uncertainties in component masses of $\sim 16\%$, with little constraint on spins (the median $90\%$ upper limit on the spin of the more massive component is $\sim 0.7$). Stronger prior constraints on neutron-star spins can further constrain mass estimates, but only marginally. However, we find that the sky position and luminosity distance for these sources are not influenced by the inclusion of spin; therefore, if LIGO detects a low-spin population of BNS sources, less computationally expensive results calculated neglecting spin will be sufficient for guiding electromagnetic follow-up. ",https://doi.org/10.3847/0004-637X/825/2/116,1508.05336v3,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Application of bond valence method in the non-isovalent semiconductor   alloy (GaN)$_{1-x}$(ZnO)$_x$,1970,"  This paper studies the bond valence method (BVM) and its application in the non-isovalent semiconductor alloy (GaN)$_{\rm{1-x}}$(ZnO)$_{\rm{x}}$. Particular attention is paid to the role of short-range order (SRO). A physical interpretation based on atomic orbital interaction is proposed and examined by density-functional theory (DFT) calculations. Combining BVM with Monte-Carlo simulations and a DFT-based cluster expansion model, bond-length distributions and bond-angle variations are predicted. The correlation between bond valence and bond stiffness is also revealed. Finally the concept of bond valence is extended into the modelling of an atomistic potential. ",https://doi.org/10.1088/0965-0393/24/7/075014,1509.04678v2,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Path Integral Molecular Dynamics for Exact Quantum Statistics of   Multi-Electronic-State Systems,1970,"  An exact approach to compute physical properties for general multi-electronic-state (MES) systems in thermal equilibrium is presented. The approach is extended from our recent progress on path integral molecular dynamics (PIMD) [J. Chem. Phys. 145, 024103 (2016); 147, 034109 (2017)] for quantum statistical mechanics when a single potential energy surface is involved. We first define an effective potential function that is numerically favorable for MES-PIMD, and then derive corresponding estimators in MES-PIMD for evaluating various physical properties. Its application to several representative one-dimensional and multi-dimensional models demonstrates that MES-PIMD in principle offers a practical tool in either of the diabatic and adiabatic representations for studying exact quantum statistics of complex/large MES systems when the Born-Oppenheimer approximation, Condon approximation, and harmonic bath approximation are broken. ",https://doi.org/10.1063/1.5005059,1710.04126v2,Yes,potent(2)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Interaction homotopy and interaction homology,1970,"  Interactions in complex systems are widely observed across various fields, drawing increased attention from researchers. In mathematics, efforts are made to develop various theories and methods for studying the interactions between spaces. In this work, we present an algebraic topology framework to explore interactions between spaces. We introduce the concept of interaction spaces and investigate their homotopy, singular homology, and simplicial homology. Furthermore, we demonstrate that interaction singular homology serves as an invariant under interaction homotopy. We believe that the proposed framework holds potential for practical applications. ",Kein DOI-Link verfügbar,2311.16322v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Mechanisms of pyroelectricity in three- and two-dimensional materials,1970,"  Pyroelectricity is a very promising phenomenon in three- and two-dimensional (2D) materials, but first-principles calculations have not so far been used to elucidate the underlying mechanisms. Here we report density-functional theory (DFT) calculations based on the Born-Szigeti theory of pyroelectricity, by combining fundamental thermodynamics and the modern theory of polarization. We find satisfactory agreement with experimental data in the case of bulk benchmark materials, showing that the so-called electron-phonon renormalization, whose contribution has been viewed as negligible, is important. We predict out-of-plane pyroelectricity in the recently synthesized Janus MoSSe monolayer and in-plane pyroelectricity in the group-IV monochalcogenide GeS monolayer. It is notable that the so-called secondary pyroelectricity is found to be dominant in GeS monolayer. The present work opens a theoretical route to study the pyroelectric effect using DFT and provides a valuable tool in the search for new candidates for pyroelectric applications. ",https://doi.org/10.1103/PhysRevLett.120.207602,1803.04580v1,Yes,notable(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Evolutionary Khovanov homology,1970,"  Knot theory is a study of the embedding of closed circles into three-dimensional Euclidean space, motivated the ubiquity of knots in daily life and human civilization. However, the current knot theory focuses on the topology rather than metric. As such, the application of knot theory remains primitive and qualitative. Motivated by the need of quantitative knot data analysis (KDA), this work implements the metric into knot theory, the evolutionary Khovanov homology (EKH), to facilitate a multiscale KDA of real-world data. It is demonstrated that EKH exhibits non-trivial knot invariants at appropriate scales even if the global topological structure of a knot is simple. The proposed EKH has a great potential for KDA and knot learning. ",Kein DOI-Link verfügbar,2406.02821v2,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham","Photons, phonons, and plasmons with orbital angular momentum in plasmas",1970,"  Exact eigen modes with orbital angular momentum (OAM) in the complex media of unmagnetized homogeneous plasma are studied. Three exact eigen modes with OAM are discovered, i.e., photons, phonons, and plasmons. It is found that an OAM photon can be excited by two familiar Bessel modes without OAM. For the phonons and plasmons, their OAM are carried by the electrons and ions. The OAM modes in plasmas and their characteristics can be explored for various potential applications in plasma physics and accelerator physics. ",https://doi.org/10.1038/srep41731,1608.02186v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Branching actin network remodeling governs the force-velocity   relationship,1970,"  Actin networks, acting as an engine pushing against an external load, are fundamentally important to cell motility. A measure of the effectiveness of an engine is the velocity the engine is able to produce at a given force, the force-velocity curve. One type of force-velocity curve, consisting of a concave region where velocity is insensitive to increasing force followed by a decrease in velocity, is indicative of an adaptive response. In contrast, an engine whose velocity rapidly decays as a convex curve in response to increasing force would indicate a lack of adaptive response. Even taken outside of a cellular context, branching actin networks have been observed to exhibit both concave and convex force-velocity curves. The exact mechanism that can explain both force-velocity curves is not yet known. We carried out an agent-based stochastic simulation to explore such a mechanism. Our results suggest that upon loading, branching actin networks are capable of remodeling by increasing the number filaments growing against the load. Our model provides a mechanism that can account for both convex and concave force-velocity relationships observed in branching actin networks. Finally, our model gives a potential explanation to the experimentally observed force history dependence for actin network velocity. ",Kein DOI-Link verfügbar,1111.6611v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Spin-Orientation Dependent Topological States in Two-Dimensional   Antiferromagnetic NiTl$_2$S$_4$ Monolayers,1970,"  The topological states of matters arising from the nontrivial magnetic configuration provide a better understanding of physical properties and functionalities of solid materials. Such studies benefit from the active control of spin orientation in any solid, which is yet known to rarely take place in the two-dimensional (2D) limit. Here we demonstrate by the first-principles calculations that spin-orientation dependent topological states can appear in the geometrically frustrated monolayer antiferromagnet. Different topological states including quantum anomalous Hall (QAH) effect and time-reversal-symmetry (TRS) broken quantum spin Hall (QSH) effect can be obtained by changing spin orientation in the NiTl2S4 monolayer. Remarkably, the dilated nc-AFM NiTl2S4 monolayer gives birth to the QAH effect with hitherto reported largest number of quantized conducting channels (Chern number C = -4) in 2D materials. Interestingly, under tunable chemical potential, the nc-AFM NiTl2S4 monolayer hosts a novel state supporting the coexistence of QAH and TRS broken QSH effects with a Chern number C = 3 and spin Chern number C_s = 1. This work manifests a promising concept and material realization toward topological spintronics in 2D antiferromagnets by manipulating its spin degree of freedom. ",https://doi.org/10.1021/acs.nanolett.9b00948,1904.11735v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Machine Learning Design of Perovskite Catalytic Properties,1970,"  Discovering new materials that efficiently catalyze the oxygen reduction and evolution reactions is critical for facilitating the widespread adoption of solid oxide fuel cell and electrolyzer (SOFC/SOEC) technologies. Here, we develop machine learning (ML) models to predict perovskite catalytic properties critical for SOFC/SOEC applications, including oxygen surface exchange, oxygen diffusivity, and area specific resistance (ASR). The models are based on trivial-to-calculate elemental features and are more accurate and dramatically faster than the best models based on ab initio-derived features, potentially eliminating the need for ab initio calculations in descriptor-based screening. Our model of ASR enables temperature-dependent predictions, has well calibrated uncertainty estimates and online accessibility. Use of temporal cross-validation reveals our model to be effective at discovering new promising materials prior to their initial discovery, demonstrating our model can make meaningful predictions. Using the SHapley Additive ExPlanations (SHAP) approach, we provide detailed discussion of different approaches of model featurization for ML property prediction. Finally, we use our model to screen more than 19 million perovskites to develop a list of promising cheap, earth-abundant, stable, and high performing materials, and find some top materials contain mixtures of less-explored elements (e.g., K, Bi, Y, Ni, Cu) worth exploring in more detail. ",Kein DOI-Link verfügbar,2311.01401v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Persistent Mayer homology and persistent Mayer Laplacian,1970,"  In algebraic topology, the differential (i.e., boundary operator) typically satisfies $d^{2}=0$. However, the generalized differential $d^{N}=0$ for an integer $N\geq 2$ has been studied in terms of Mayer homology on $N$-chain complexes for more than eighty years. We introduce Mayer Laplacians on $N$-chain complexes. We show that both Mayer homology and Mayer Laplacians offer considerable application potential, providing topological and geometric insights to spaces. We also introduce persistent Mayer homology and persistent Mayer Laplacians at various $N$. The Wasserstein distance and stability of persistence diagrams associated with Mayer homology are investigated. Our computational experiments indicate that the topological features offered by persistent Mayer homology and spectrum given by persistent Mayer Laplacians hold substantial promise for large, complex, and diverse data. We envision that the present work serves as an inaugural step towards integrating Mayer homology and Mayer Laplacians into the realm of topological data analysis. ",Kein DOI-Link verfügbar,2312.01268v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Novel spin-orbit coupling driven emergent states in iridate-based   heterostructures,1970,"  Recent years have seen many examples of how the strong spin-orbit coupling (SOC) present in iridates can stabilize new emergent states that are difficult or impossible to realize in more conventional materials. In this review we outline a representative set of studies detailing how heterostructures based on Ruddlesden-Popper (RP) and perovskite iridates can be used to access yet more novel physics. Beginning with a short synopsis of iridate thin film growth, the effects of the heterostructure morphology on the RP iridates including Sr2IrO4 and SrIrO3 are discussed. Example studies explore the effects of epitaxial strain, laser-excitation to access transient states, topological semimetallicity in SrIrO3, 2D magnetism in artificial RP iridates, and interfacial magnetic coupling between iridate and neighboring layers. Taken together, these works show the fantastic potential for controlled engineering of novel quantum phenomena in iridate heterostructures. ",https://doi.org/10.1016/j.jpcs.2017.11.018,1711.07609v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",VORTEX: Real-Time Off-Chain Payments and Cross-Chain Swaps for   Cryptocurrencies,1970,"  In this paper, we present VERTEX, a TEE-based layer-2 solution that tackles two crucial challenges in the realm of cryptocurrencies: off-chain payments and cross-chain swaps. It offers three notable features: - Channel-free off-chain payments: it allows a payer to make direct payments to anyone without requiring any on-chain relationship or intermediary channels. - Real-time yet decentralized cross-chain swaps: it is the first known solution that enables real-time cross-chain swaps without relying on a central server. This novel feature is made possible through a ground-breaking fair exchange protocol. - TEE crash-tolerance: it offers two solutions to handle TEE crashes, one of which involves an innovative application of time-lock puzzles in this context. We evaluate ECHO on a network consists of 1000 nodes and the evaluation results show that ECHO can achieve 7000 TPS ",Kein DOI-Link verfügbar,2403.15191v3,Yes,"innovative(1), notable(1)"
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Strengthening Layer Interaction via Dynamic Layer Attention,1970,"  In recent years, employing layer attention to enhance interaction among hierarchical layers has proven to be a significant advancement in building network structures. In this paper, we delve into the distinction between layer attention and the general attention mechanism, noting that existing layer attention methods achieve layer interaction on fixed feature maps in a static manner. These static layer attention methods limit the ability for context feature extraction among layers. To restore the dynamic context representation capability of the attention mechanism, we propose a Dynamic Layer Attention (DLA) architecture. The DLA comprises dual paths, where the forward path utilizes an improved recurrent neural network block, named Dynamic Sharing Unit (DSU), for context feature extraction. The backward path updates features using these shared context representations. Finally, the attention mechanism is applied to these dynamically refreshed feature maps among layers. Experimental results demonstrate the effectiveness of the proposed DLA architecture, outperforming other state-of-the-art methods in image recognition and object detection tasks. Additionally, the DSU block has been evaluated as an efficient plugin in the proposed DLA architecture.The code is available at https://github.com/tunantu/Dynamic-Layer-Attention. ",Kein DOI-Link verfügbar,2406.13392v1,Yes,fresh(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",EGR: Equivariant Graph Refinement and Assessment of 3D Protein Complex   Structures,1970,"  Protein complexes are macromolecules essential to the functioning and well-being of all living organisms. As the structure of a protein complex, in particular its region of interaction between multiple protein subunits (i.e., chains), has a notable influence on the biological function of the complex, computational methods that can quickly and effectively be used to refine and assess the quality of a protein complex's 3D structure can directly be used within a drug discovery pipeline to accelerate the development of new therapeutics and improve the efficacy of future vaccines. In this work, we introduce the Equivariant Graph Refiner (EGR), a novel E(3)-equivariant graph neural network (GNN) for multi-task structure refinement and assessment of protein complexes. Our experiments on new, diverse protein complex datasets, all of which we make publicly available in this work, demonstrate the state-of-the-art effectiveness of EGR for atomistic refinement and assessment of protein complexes and outline directions for future work in the field. In doing so, we establish a baseline for future studies in macromolecular refinement and structure analysis. ",Kein DOI-Link verfügbar,2205.10390v2,Yes,notable(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",LookAhead: Preventing DeFi Attacks via Unveiling Adversarial Contracts,1970,"  DeFi incidents stemming from various smart contract vulnerabilities have culminated in financial damages exceeding 3 billion USD. The attacks causing such incidents commonly commence with the deployment of adversarial contracts, subsequently leveraging these contracts to execute adversarial transactions that exploit vulnerabilities in victim contracts. Existing defense mechanisms leverage heuristic or machine learning algorithms to detect adversarial transactions, but they face significant challenges in detecting private adversarial transactions. Namely, attackers can send adversarial transactions directly to miners, evading visibility within the blockchain network and effectively bypassing the detection. In this paper, we propose a new direction for detecting DeFi attacks, i.e., detecting adversarial contracts instead of adversarial transactions, allowing us to proactively identify potential attack intentions, even if they employ private adversarial transactions. Specifically, we observe that most adversarial contracts follow a similar pattern, e.g., anonymous fund source, closed-source, frequent token-related function calls. Based on this observation, we build a machine learning classifier that can effectively distinguish adversarial contracts from benign ones. We build a dataset consists of features extracted from 269 adversarial contracts and 13,000 benign contracts. Based on this dataset, we evaluate different classifiers, the results of which show that our method for identifying DeFi adversarial contracts performs exceptionally well. For example, the F1-Score for LightGBM-based classifier is 0.9541, with a remarkably low false positive rate of only 0.15%. ",Kein DOI-Link verfügbar,2401.07261v2,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Efficient Multi-View Fusion and Flexible Adaptation to View Missing in   Cardiovascular System Signals,1970,"  The progression of deep learning and the widespread adoption of sensors have facilitated automatic multi-view fusion (MVF) about the cardiovascular system (CVS) signals. However, prevalent MVF model architecture often amalgamates CVS signals from the same temporal step but different views into a unified representation, disregarding the asynchronous nature of cardiovascular events and the inherent heterogeneity across views, leading to catastrophic view confusion. Efficient training strategies specifically tailored for MVF models to attain comprehensive representations need simultaneous consideration. Crucially, real-world data frequently arrives with incomplete views, an aspect rarely noticed by researchers. Thus, the View-Centric Transformer (VCT) and Multitask Masked Autoencoder (M2AE) are specifically designed to emphasize the centrality of each view and harness unlabeled data to achieve superior fused representations. Additionally, we systematically define the missing-view problem for the first time and introduce prompt techniques to aid pretrained MVF models in flexibly adapting to various missing-view scenarios. Rigorous experiments involving atrial fibrillation detection, blood pressure estimation, and sleep staging-typical health monitoring tasks-demonstrate the remarkable advantage of our method in MVF compared to prevailing methodologies. Notably, the prompt technique requires finetuning less than 3% of the entire model's data, substantially fortifying the model's resilience to view missing while circumventing the need for complete retraining. The results demonstrate the effectiveness of our approaches, highlighting their potential for practical applications in cardiovascular health monitoring. Codes and models are released at URL. ",Kein DOI-Link verfügbar,2406.08930v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",GLoRE: Evaluating Logical Reasoning of Large Language Models,1970,"  Recently, large language models (LLMs), including notable models such as GPT-4 and burgeoning community models, have showcased significant general language understanding abilities. However, there has been a scarcity of attempts to assess the logical reasoning capacities of these LLMs, an essential facet of natural language understanding. To encourage further investigation in this area, we introduce GLoRE, a meticulously assembled General Logical Reasoning Evaluation benchmark comprised of 12 datasets that span three different types of tasks. Our experimental results show that compared to the performance of human and supervised fine-tuning, the logical reasoning capabilities of open LLM models necessitate additional improvement; ChatGPT and GPT-4 show a strong capability of logical reasoning, with GPT-4 surpassing ChatGPT by a large margin. We propose a self-consistency probing method to enhance the accuracy of ChatGPT and a fine-tuned method to boost the performance of an open LLM. We release the datasets and evaluation programs to facilitate future research. ",Kein DOI-Link verfügbar,2310.09107v1,Yes,"meticulous(1), notable(1), meticulously(1)"
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham","Exploring sensitivity of charge-exchange ($p, n$) reactions to the   neutron density distribution",1970,"  $Background:$ The determination of the nuclear neutron properties suffers from uncontrolled uncertainties, which attracted considerable attention recently, such as in the context of the PREX experiment.   $Purpose:$ Our aim is to analyze the sensitivity of charge-exchange ($p, n$) reactions to the neutron density distribution $\rho_{n}$ and constrain the neutron characteristics in the nuclear structure models.   $Method:$ By combing the folding and the mean-field models, the nucleon-nucleus ($NA$) potential can be obtained from the nuclear density distribution. Further, the ($p, p$) and ($p, n$) cross sections for $^{48}$Ca and $^{208}$Pb are calculated following the distorted-wave Born approximation (DWBA) method.   $Results:$ Compared with the ($p, p$) cross section, the effects of $\rho_{n}$ variation on the ($p, n$) cross section are significant, which is due to the impact of isovector properties. Based on the global folding model analyses of data, it is found that $^{48}$Ca and $^{208}$Pb have relatively large neutron skin thickness $\Delta R_{n p}$.   $Conclusions:$ Results illustrate that the charge-exchange ($p, n$) reaction is a sensitive probe of $\rho_{n}$. The results in this paper can offer useful guides for future experiments of neutron characteristics. ",https://doi.org/10.1103/PhysRevC.106.054605,2210.14554v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Canonical Hamiltonian Guiding Center Dynamics and Its Intrinsic Magnetic   Moment,1970,"  The concept of guiding center is potent in astrophysics, space plasmas, fusion researches, and arc plasmas to solve the multi-scale dynamics of magnetized plasmas. In this letter, we rigorously prove that the guiding center dynamics can generally be described as a constrained canonical Hamiltonian system with two constraints in six dimensional phase space, and that the solution flow of the guiding center lies on a canonical symplectic sub-manifold. The guiding center can thus be modeled as a pseudo-particle with an intrinsic magnetic moment, which properly replaces the charged particle dynamics on time scales larger than the gyro-period. The complete dynamical behaviors, such as the velocity and force, of the guiding center pseudo-particle can be clearly deduced from the model. Furthermore, a series of related theories, such as symplectic numerical methods, the canonical gyro-kinetic theory, and canonical particle-in-cell algorithms can be systematically developed based on the canonical guiding center system. The canonical guiding center theory also provides an enlightenment for the origin of the intrinsic magnetic moment. ",Kein DOI-Link verfügbar,2403.02883v1,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Accurate force field of two-dimensional ferroelectrics from deep   learning,1970,"  The discovery of two-dimensional (2D) ferroelectrics with switchable out-of-plane polarization such as monolayer $\alpha$-In$_2$Se$_3$ offers a new avenue for ultrathin high-density ferroelectric-based nanoelectronics such as ferroelectric field effect transistors and memristors. The functionality of ferroelectrics depends critically on the dynamics of polarization switching in response to an external electric/stress field. Unlike the switching dynamics in bulk ferroelectrics that have been extensively studied, the mechanisms and dynamics of polarization switching in 2D remain largely unexplored. Molecular dynamics (MD) using classical force fields is a reliable and efficient method for large-scale simulations of dynamical processes with atomic resolution. Here we developed a deep neural network-based force field of monolayer In$_2$Se$_3$ using a concurrent learning procedure that efficiently updates the first-principles-based training database. The model potential has accuracy comparable with density functional theory (DFT), capable of predicting a range of thermodynamic properties of In$_2$Se$_3$ polymorphs and lattice dynamics of ferroelectric In$_2$Se$_3$. Pertinent to the switching dynamics, the model potential also reproduces the DFT kinetic pathways of polarization reversal and 180$^\circ$ domain wall motions. Moreover, isobaric-isothermal ensemble MD simulations predict a temperature-driven $\alpha \rightarrow \beta$ phase transition at the single-layer limit, as revealed by both local atomic displacement and Steinhardt's bond orientational order parameter $Q_4$. Our work paves the way for further research on the dynamics of ferroelectric $\alpha$-In$_2$Se$_3$ and related systems. ",https://doi.org/10.1103/PhysRevB.104.174107,2109.07104v2,Yes,potent(2)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",A Self-Healing Magnetic-Array-Type Current Sensor with Data-Driven   Identification of Abnormal Magnetic Measurement Units,1970,"  Magnetic-array-type current sensors have garnered increasing popularity owing to their notable advantages, including broadband functionality, a large dynamic range, cost-effectiveness, and compact dimensions. However, the susceptibility of the measurement error of one or more magnetic measurement units (MMUs) within the current sensor to drift significantly from the nominal value due to environmental factors poses a potential threat to the measurement accuracy of the current sensor. In light of the need to ensure sustained measurement accuracy over the long term, this paper proposes an innovative self-healing approach rooted in cyber-physics correlation. This approach aims to identify MMUs exhibiting abnormal measurement errors, allowing for the exclusive utilization of the remaining unaffected MMUs in the current measurement process. To achieve this, principal component analysis (PCA) is employed to discern the primary component, arising from fluctuations of the measured current, from the residual component, attributed to the drift in measurement error. This analysis is conducted by scrutinizing the measured data obtained from the MMUs. Subsequently, the squared prediction error (SPE) statistic (also called $Q$ statistic) is deployed to individually identify any MMU displaying abnormal behavior. The experimental results demonstrate the successful online identification of abnormal MMUs without the need for a standard magnetic field sensor. By eliminating the contributions from the identified abnormal MMUs, the accuracy of the current measurement is effectively preserved. ",Kein DOI-Link verfügbar,2402.11419v2,Yes,"innovative(1), notable(1), potent(1)"
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",Adaptive Shape Servoing of Elastic Rods using Parameterized Regression   Features and Auto-Tuning Motion Controls,1970,"  The robotic manipulation of deformable linear objects has shown great potential in a wide range of real-world applications. However, it presents many challenges due to the objects' complex nonlinearity and high-dimensional configuration. In this paper, we propose a new shape servoing framework to automatically manipulate elastic rods through visual feedback. Our new method uses parameterized regression features to compute a compact (low-dimensional) feature vector that quantifies the object's shape, thus, enabling to establish an explicit shape servo-loop. To automatically deform the rod into a desired shape, the proposed adaptive controller iteratively estimates the differential transformation between the robot's motion and the relative shape changes; This valuable capability allows to effectively manipulate objects with unknown mechanical models. An auto-tuning algorithm is introduced to adjust the robot's shaping motions in real-time based on optimal performance criteria. To validate the proposed framework, a detailed experimental study with vision-guided robotic manipulators is presented. ",Kein DOI-Link verfügbar,2008.06896v2,Yes,potent(1)
0000-0002-5391-7213,Jian Liu,"The University of Birmingham, University of Birmingham",A Quality-based Syntactic Template Retriever for   Syntactically-controlled Paraphrase Generation,1970,"  Existing syntactically-controlled paraphrase generation (SPG) models perform promisingly with human-annotated or well-chosen syntactic templates. However, the difficulty of obtaining such templates actually hinders the practical application of SPG models. For one thing, the prohibitive cost makes it unfeasible to manually design decent templates for every source sentence. For another, the templates automatically retrieved by current heuristic methods are usually unreliable for SPG models to generate qualified paraphrases. To escape this dilemma, we propose a novel Quality-based Syntactic Template Retriever (QSTR) to retrieve templates based on the quality of the to-be-generated paraphrases. Furthermore, for situations requiring multiple paraphrases for each source sentence, we design a Diverse Templates Search (DTS) algorithm, which can enhance the diversity between paraphrases without sacrificing quality. Experiments demonstrate that QSTR can significantly surpass existing retrieval methods in generating high-quality paraphrases and even perform comparably with human-annotated templates in terms of reference-free metrics. Additionally, human evaluation and the performance on downstream tasks using our generated paraphrases for data augmentation showcase the potential of our QSTR and DTS algorithm in practical scenarios. ",Kein DOI-Link verfügbar,2310.13262v1,Yes,potent(1)
0000-0001-5917-4185,Joshua Williams,"University of Leeds, University of Leeds Faculty of Environment",Dynamic Modeling and Equilibria in Fair Decision Making,1970,"  Recent studies on fairness in automated decision making systems have both investigated the potential future impact of these decisions on the population at large, and emphasized that imposing ''typical'' fairness constraints such as demographic parity or equality of opportunity does not guarantee a benefit to disadvantaged groups. However, these previous studies have focused on either simple one-step cost/benefit criteria, or on discrete underlying state spaces. In this work, we first propose a natural continuous representation of population state, governed by the Beta distribution, using a loan granting setting as a running example. Next, we apply a model of population dynamics under lending decisions, and show that when conditional payback probabilities are estimated correctly 1) ``optimal'' behavior by lenders can lead to ''Matthew Effect'' bifurcations (i.e., ''the rich get richer and the poor get poorer''), but that 2) many common fairness constraints on the allowable policies cause groups to converge to the same equilibrium point. Last, we contrast our results in the case of misspecified conditional probability estimates with prior work, and show that for this model, different levels of group misestimation guarantees that even fair policies lead to bifurcations. We illustrate some of the modeling conclusions on real data from credit scoring. ",Kein DOI-Link verfügbar,1911.06837v1,Yes,potent(1)
0000-0001-5917-4185,Joshua Williams,"University of Leeds, University of Leeds Faculty of Environment",A Bayesian Model of Cash Bail Decisions,1970,"  The use of cash bail as a mechanism for detaining defendants pre-trial is an often-criticized system that many have argued violates the presumption of ""innocent until proven guilty."" Many studies have sought to understand both the long-term effects of cash bail's use and the disparate rate of cash bail assignments along demographic lines (race, gender, etc). However, such work is often susceptible to problems of infra-marginality -- that the data we observe can only describe average outcomes, and not the outcomes associated with the marginal decision.   In this work, we address this problem by creating a hierarchical Bayesian model of cash bail assignments. Specifically, our approach models cash bail decisions as a probabilistic process whereby judges balance the relative costs of assigning cash bail with the cost of defendants potentially skipping court dates, and where these skip probabilities are estimated based upon features of the individual case. We then use Monte Carlo inference to sample the distribution over these costs for different magistrates and across different races. We fit this model to a data set we have collected of over 50,000 court cases in the Allegheny and Philadelphia counties in Pennsylvania. Our analysis of 50 separate judges shows that they are uniformly more likely to assign cash bail to black defendants than to white defendants, even given identical likelihood of skipping a court appearance. This analysis raises further questions about the equity of the practice of cash bail, irrespective of its underlying legal justification. ",Kein DOI-Link verfügbar,2101.12267v1,Yes,potent(1)
0000-0001-9064-6865,Matthias Klein,"Ludwig-Maximilians-Universität München, Ludwig-Maximilians-Universität München Medizinische Fakultät",More Confining N=1 SUSY Gauge Theories From Non-Abelian Duality,1970,  We expand on an idea of Seiberg that an N=1 supersymmetric gauge theory shows confinement without breaking of chiral symmetry when the gauge symmetry of its magnetic dual is completely broken by the Higgs effect. This has recently been applied to some models involving tensor fields and an appropriate tree-level superpotential. We show how the confining spectrum of a supersymmetric gauge theory can easily be derived when a magnetic dual is known and we determine it explicitly for many models containing fields in second rank tensor representations. We also give the form of the confining superpotential for most of these models. ,https://doi.org/10.1016/S0550-3213(99)00229-1,hep-th/9812155v1,Yes,potent(2)
0000-0001-9064-6865,Matthias Klein,"Ludwig-Maximilians-Universität München, Ludwig-Maximilians-Universität München Medizinische Fakultät",Confining N=1 SUSY gauge theories from Seiberg duality,1970,  In this talk I review and generalize an idea of Seiberg that an N=1 supersymmetric gauge theory shows confinement without breaking of chiral symmetry when the gauge symmetry of its magnetic dual is completely broken by the Higgs effect. It is shown how the confining spectrum of a supersymmetric gauge theory can easily be derived when a magnetic dual is known and this method is applied to many models containing fields in second rank tensor representations and an appropriate tree-level superpotential. ,Kein DOI-Link verfügbar,hep-th/9904210v1,Yes,potent(1)
0000-0001-9064-6865,Matthias Klein,"Ludwig-Maximilians-Universität München, Ludwig-Maximilians-Universität München Medizinische Fakultät",Quantum modified moduli spaces and field dependent gauge couplings,1970,"  In this paper we discuss quantum modified moduli spaces in supergravity. We examine a model suggested by Izawa and Yanagida and by Intriligator and Thomas that breaks global supersymmetry by a quantum deformation of the classical moduli space. We determine the minimum of the supergravity potential when the gauge coupling is taken to depend on a dynamical field, typically a modulus of string theory. We find that the only minimum is at the trivial configuration of vanishing coupling constant and unbroken supersymmetry. We also discuss models involving more complicated superpotentials and find that the gauge coupling is only stabilized in a supersymmetric ground state. ",https://doi.org/10.1016/S0550-3213(98)00498-2,hep-th/9803143v1,Yes,potent(2)
0000-0001-9064-6865,Matthias Klein,"Ludwig-Maximilians-Universität München, Ludwig-Maximilians-Universität München Medizinische Fakultät",On effective superpotentials and Kutasov duality,1970,"  We derive the effective superpotential for an N=1 SU(N_c) gauge theory with one massless adjoint field and N_f massless fundamental flavors and cubic tree-level superpotential for the adjoint field. This is a generalization of the Affleck-Dine-Seiberg superpotential to gauge theories with one massless adjoint matter field. Using Kutasov's generalization of Seiberg duality, we then find the effective superpotential for a related theory with massive fundamental flavors. ",https://doi.org/10.1088/1126-6708/2003/10/050,hep-th/0309044v1,Yes,potent(4)
0000-0001-9064-6865,Matthias Klein,"Ludwig-Maximilians-Universität München, Ludwig-Maximilians-Universität München Medizinische Fakultät","Matrix model, Kutasov duality and Factorization of Seiberg-Witten Curves",1970,  We study the duality of $\mathcal{N}=1$ gauge theories in the presence of a massless adjoint field and massive quarks by calculating the superpotential using the Dighkgraaf-Vafa matrix model and by comparing with the previous result coming from Kutasov duality. The Kutasov duality method gives a result in which one instanton term is absent. The matrix model method confirms it and also show that the absence of the one instanton term is related to the masslessness of the adjoint field. ,Kein DOI-Link verfügbar,hep-th/0310078v2,Yes,potent(1)
0000-0001-9064-6865,Matthias Klein,"Ludwig-Maximilians-Universität München, Ludwig-Maximilians-Universität München Medizinische Fakultät",The Dilaton Potential from N= 1*,1970,"  Recent understanding of {\cal N}=1* supersymmetric theory (mass deformed {\cal N}=4) has made it possible to find an exact superpotential which encodes the properties of the different phases of the theory. We consider this superpotential as an illustrative example for the source of a nontrivial scalar potential for the string theory dilaton and study its properties. The superpotential is characterized by the rank of the corresponding gauge group (N) and integers p,q,k labelling the different massive phases of the theory. For generic values of these parameters, we find the expected runaway behaviour of the potential to vanishing string coupling. But there are also supersymmetric minima at weak coupling stabilizing the dilaton field. An interesting property of this potential is that there is a proliferation of supersymmetric vacua in the confining phases, with the number of vacua increasing with N and leading to a kind of staircase potential. For a range of parameters, it is possible to obtain realistic values for the gauge coupling. ",https://doi.org/10.1016/S0550-3213(01)00202-4,hep-th/0101186v3,Yes,potent(7)
0009-0001-5618-4326,Eva Sextl,Ludwig-Maximilians-Universität München,Modified Gravity and the Flux-weighted Gravity-Luminosity Relationship   of Blue Supergiant Stars,1970,"  We calculate models of stellar evolution for very massive stars and include the effects of modified gravity to investigate the influence on the physical properties of blue supergiant stars and their use as extragalactic distance indicators. With shielding and fifth force parameters in a similar range as in previous studies of Cepheid and tip of the red giant branch (TRGB) stars we find clear effects on stellar luminosity and flux-weighted gravity. The relationship between flux weighted gravity, g_F = g/Teff^4, and bolometric magnitude M_bol (FGLR), which has been used successfully for accurate distance determinations, is systematically affected. While the stellar evolution FGLRs show a systematic offset from the observed relation, we can use the differential shifts between models with Newtonian and modified gravity to estimate the influence on FGLR distance determinations. Modified gravity leads to a distance increase of 0.05 to 0.15 magnitudes in distance modulus. These change are comparable to the ones found for Cepheid stars. We compare observed FGLR and TRGB distances of nine galaxies to constrain the free parameters of modified gravity. Not accounting for systematic differences between TRGB and FGLR distances shielding parameters of 5*10^-7 and 10^-6 and fifth force parameters of 1/3 and 1 can be ruled out with about 90% confidence. Allowing for potential systematic offsets between TRGB and FGLR distances no determination is possible for a shielding parameter of 10^-6. For 5*10$^-7 a fifth force parameter of 1 can be ruled out to 92% but 1/3 is unlikely only to 60%. ",https://doi.org/10.3847/1538-4357/abfafa,2104.11174v1,Yes,potent(1)
0000-0001-5193-8574,Timo Kaufmann,Ludwig-Maximilians-Universität München,A Survey of Reinforcement Learning from Human Feedback,1970,"  Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research. ",Kein DOI-Link verfügbar,2312.14925v2,Yes,"intricate(1), potent(1), impressively(1)"
0000-0001-5193-8574,Timo Kaufmann,Ludwig-Maximilians-Universität München,Inverse Constitutional AI: Compressing Preferences into Principles,1970,"  Feedback data plays an important role in fine-tuning and evaluating state-of-the-art AI models. Often pairwise text preferences are used: given two texts, human (or AI) annotators select the ""better"" one. Such feedback data is widely used to align models to human preferences (e.g., reinforcement learning from human feedback), or to rank models according to human preferences (e.g., Chatbot Arena). Despite its wide-spread use, prior work has demonstrated that human-annotated pairwise text preference data often exhibits unintended biases. For example, human annotators have been shown to prefer assertive over truthful texts in certain contexts. Models trained or evaluated on this data may implicitly encode these biases in a manner hard to identify. In this paper, we formulate the interpretation of existing pairwise text preference data as a compression task: the Inverse Constitutional AI (ICAI) problem. In constitutional AI, a set of principles (or constitution) is used to provide feedback and fine-tune AI models. The ICAI problem inverts this process: given a dataset of feedback, we aim to extract a constitution that best enables a large language model (LLM) to reconstruct the original annotations. We propose a corresponding initial ICAI algorithm and validate its generated constitutions quantitatively based on reconstructed annotations. Generated constitutions have many potential use-cases -- they may help identify undesirable biases, scale feedback to unseen data or assist with adapting LLMs to individual user preferences. We demonstrate our approach on a variety of datasets: (a) synthetic feedback datasets with known underlying principles; (b) the AlpacaEval dataset of cross-annotated human feedback; and (c) the crowdsourced Chatbot Arena data set. We release the code for our algorithm and experiments at https://github.com/rdnfn/icai . ",Kein DOI-Link verfügbar,2406.06560v1,Yes,potent(1)
0000-0001-5193-8574,Timo Kaufmann,Ludwig-Maximilians-Universität München,Problem Solving Through Human-AI Preference-Based Cooperation,1970,"  While there is a widespread belief that artificial general intelligence (AGI) -- or even superhuman AI -- is imminent, complex problems in expert domains are far from being solved. We argue that such problems require human-AI cooperation and that the current state of the art in generative AI is unable to play the role of a reliable partner due to a multitude of shortcomings, including inability to keep track of a complex solution artifact (e.g., a software program), limited support for versatile human preference expression and lack of adapting to human preference in an interactive setting. To address these challenges, we propose HAI-Co2, a novel human-AI co-construction framework. We formalize HAI-Co2 and discuss the difficult open research problems that it faces. Finally, we present a case study of HAI-Co2 and demonstrate its efficacy compared to monolithic generative AI models. ",Kein DOI-Link verfügbar,2408.07461v2,Yes,versatile(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Social Networks are Divulging Your Identity behind Crypto Addresses,1970,"  Cryptocurrencies, such as Bitcoin and Ethereum, are becoming increasingly prevalent mainly due to their anonymity, decentralization, transparency, and security. However, the completely public ledger makes the trace and analysis of each account possible as long as the identity behind the public address is revealed. Theoretically, social networks could make that happen when addresses are posted on social network platforms using accounts containing personal information. To verify such a possibility, we have collected public data from two major platforms, i.e. Twitter and Reddit, aiming to find potential privacy leakage behind the ETH public address. In the end, an easy-to-use retrieval application is also built for a better illustration. ",Kein DOI-Link verfügbar,2211.09656v1,Yes,potent(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Covariance Matrix Estimation for High-Throughput Biomedical Data with   Interconnected Communities,1970,"  Estimating a covariance matrix is central to high-dimensional data analysis. Empirical analyses of high-dimensional biomedical data, including genomics, proteomics, microbiome, and neuroimaging, among others, consistently reveal strong modularity in the dependence patterns. In these analyses, intercorrelated high-dimensional biomedical features often form communities or modules that can be interconnected with others. While the interconnected community structure has been extensively studied in biomedical research (e.g., gene co-expression networks), its potential to assist in the estimation of covariance matrices remains largely unexplored. To address this gap, we propose a procedure that leverages the commonly observed interconnected community structure in high-dimensional biomedical data to estimate large covariance and precision matrices. We derive the uniformly minimum variance unbiased estimators for covariance and precision matrices in closed forms and provide theoretical results on their asymptotic properties. Our proposed method enhances the accuracy of covariance- and precision-matrix estimation and demonstrates superior performance compared to the competing methods in both simulations and real data analyses. ",Kein DOI-Link verfügbar,2302.01861v2,Yes,potent(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Minimizing Age of Information for Mobile Edge Computing Systems: A   Nested Index Approach,1970,"  Exploiting the computational heterogeneity of mobile devices and edge nodes, mobile edge computation (MEC) provides an efficient approach to achieving real-time applications that are sensitive to information freshness, by offloading tasks from mobile devices to edge nodes. We use the metric Age-of-Information (AoI) to evaluate information freshness. An efficient solution to minimize the AoI for the MEC system with multiple users is non-trivial to obtain due to the random computing time. In this paper, we consider multiple users offloading tasks to heterogeneous edge servers in a MEC system. We first reformulate the problem as a Restless Multi-Arm-Bandit (RMAB) problem and establish a hierarchical Markov Decision Process (MDP) to characterize the updating of AoI for the MEC system. Based on the hierarchical MDP, we propose a nested index framework and design a nested index policy with provably asymptotic optimality. Finally, the closed form of the nested index is obtained, which enables the performance tradeoffs between computation complexity and accuracy. Our algorithm leads to an optimality gap reduction of up to 40%, compared to benchmarks. Our algorithm asymptotically approximates the lower bound as the system scalar gets large enough. ",Kein DOI-Link verfügbar,2307.01366v1,Yes,fresh(2)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Criticality-Guided Efficient Pruning in Spiking Neural Networks Inspired   by Critical Brain Hypothesis,1970,"  Spiking Neural Networks (SNNs) have gained considerable attention due to the energy-efficient and multiplication-free characteristics. The continuous growth in scale of deep SNNs poses challenges for model deployment. Network pruning reduces hardware resource requirements of model deployment by compressing the network scale. However, existing SNN pruning methods cause high pruning costs and performance loss because the pruning iterations amplify the training difficulty of SNNs. In this paper, inspired by the critical brain hypothesis in neuroscience, we propose a regeneration mechanism based on the neuron criticality for SNN pruning to enhance feature extraction and accelerate the pruning process. Firstly, we propose a low-cost metric for the criticality in SNNs. Then, we re-rank the pruned structures after pruning and regenerate those with higher criticality to obtain the critical network. Our method achieves higher performance than the current state-of-the-art (SOTA) method with up to 95.26% reduction of pruning cost. Moreover, we investigate the underlying mechanism of our method and find that it efficiently selects potential structures and learns the consistent feature representation. ",Kein DOI-Link verfügbar,2311.16141v2,Yes,potent(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Residual spectrum: Brain functional connectivity detection beyond   coherence,1970,"  Coherence is a widely used measure to assess linear relationships between time series. However, it fails to capture nonlinear dependencies. To overcome this limitation, this paper introduces the notion of residual spectral density as a higher-order extension of the squared coherence. The method is based on an orthogonal decomposition of time series regression models. We propose a test for the existence of the residual spectrum and derive its fundamental properties. A numerical study illustrates finite sample performance of the proposed method. An application of the method shows that the residual spectrum can effectively detect brain connectivity. Our study reveals a noteworthy contrast in connectivity patterns between schizophrenia patients and healthy individuals. Specifically, we observed that non-linear connectivity in schizophrenia patients surpasses that of healthy individuals, which stands in stark contrast to the established understanding that linear connectivity tends to be higher in healthy individuals. This finding sheds new light on the intricate dynamics of brain connectivity in schizophrenia. ",Kein DOI-Link verfügbar,2305.19461v2,Yes,"intricate(1), noteworthy(1)"
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,An Internet-wide Penetration Study on NAT Boxes via TCP/IP Side Channel,1970,"  Network Address Translation (NAT) plays an essential role in shielding devices inside an internal local area network from direct malicious accesses from the public Internet. However, recent studies show the possibilities of penetrating NAT boxes in some specific circumstances. The penetrated NAT box can be exploited by attackers as a pivot to abuse the otherwise inaccessible internal network resources, leading to serious security consequences. In this paper, we aim to conduct an Internet-wide penetration testing on NAT boxes. The main difference between our study and the previous ones is that ours is based on the TCP/IP side channels. We explore the TCP/IP side channels in the research literature, and find that the shared-IPID side channel is the most suitable for NAT-penetration testing, as it satisfies the three requirements of our study: generality, ethics, and robustness. Based on this side channel, we develop an adaptive scanner that can accomplish the Internet-wide scanning in 5 days in a very non-aggressive manner. The evaluation shows that our scanner is effective in both the controlled network and the real network. Our measurement results reveal that more than 30,000 network middleboxes are potentially vulnerable to NAT penetration. They are distributed across 154 countries and 4,146 different organizations, showing that NAT-penetration poses a serious security threat. ",Kein DOI-Link verfügbar,2311.17392v1,Yes,potent(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,The Effect of Alcohol Consumption on Brain Ageing: A New Causal   Inference Framework for Incomplete and Massive Phenomic Data,1970,"  Although substance use, such as alcohol consumption, is known to be associated with cognitive decline during ageing, its direct influence on the central nervous system remains unclear. In this study, we aim to investigate the potential influence of alcohol intake frequency on accelerated brain ageing by estimating the mean potential brain-age gap (BAG) index, the difference between brain age and actual age, under different alcohol intake frequencies in a large UK Biobank (UKB) cohort with extensive phenomic data reflecting a comprehensive life-style profile. We face two major challenges: (1) a large number of phenomic variables as potential confounders and (2) a small proportion of participants with complete phenomic data. To address these challenges, we first develop a new ensemble learning framework to establish robust estimation of mean potential outcome in the presence of many confounders. We then construct a data integration step to borrow information from UKB participants with incomplete phenomic data to improve efficiency. Our analysis results reveal that daily intake or even a few times a week may have significant effects on accelerating brain ageing. Moreover, extensive numerical studies demonstrate the superiority of our method over competing methods, in terms of smaller estimation bias and variability. ",Kein DOI-Link verfügbar,2303.03520v2,Yes,potent(4)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Modeling Interconnected Modules in Multivariate Outcomes: Evaluating the   Impact of Alcohol Intake on Plasma Metabolomics,1970,"  Alcohol consumption has been shown to influence cardiovascular mechanisms in humans, leading to observable alterations in the plasma metabolomic profile. Regression models are commonly employed to investigate these effects, treating metabolomics features as the outcomes and alcohol intake as the exposure. Given the latent dependence structure among the numerous metabolomic features (e.g., co-expression networks with interconnected modules), modeling this structure is crucial for accurately identifying metabolomic features associated with alcohol intake. However, integrating dependence structures into regression models remains difficult in both estimation and inference procedures due to their large or high dimensionality. To bridge this gap, we propose an innovative multivariate regression model that accounts for correlations among outcome features by incorporating an interconnected community structure. Furthermore, we derive closed-form and likelihood-based estimators, accompanied by explicit exact and explicit asymptotic covariance matrix estimators, respectively. Simulation analysis demonstrates that our approach provides accurate estimation of both dependence and regression coefficients, and enhances sensitivity while maintaining a well-controlled discovery rate, as evidenced through benchmarking against existing regression models. Finally, we apply our approach to assess the impact of alcohol intake on $249$ metabolomic biomarkers measured using nuclear magnetic resonance spectroscopy. The results indicate that alcohol intake can elevate high-density lipoprotein levels by enhancing the transport rate of Apolipoproteins A1. ",Kein DOI-Link verfügbar,2404.10884v1,Yes,innovative(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Integrative data analysis where partial covariates have complex   non-linear effects by using summary information from an external data,1970,"  A full parametric and linear specification may be insufficient to capture complicated patterns in studies exploring complex features, such as those investigating age-related changes in brain functional abilities. Alternatively, a partially linear model (PLM) consisting of both parametric and non-parametric elements may have a better fit. This model has been widely applied in economics, environmental science, and biomedical studies. In this paper, we introduce a novel statistical inference framework that equips PLM with high estimation efficiency by effectively synthesizing summary information from external data into the main analysis. Such an integrative scheme is versatile in assimilating various types of reduced models from the external study. The proposed method is shown to be theoretically valid and numerically convenient, and it ensures a high-efficiency gain compared to classic methods in PLM. Our method is further validated using two data applications by evaluating the risk factors of brain imaging measures and blood pressure. ",Kein DOI-Link verfügbar,2303.03497v2,Yes,versatile(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Freeplane: Unlocking Free Lunch in Triplane-Based Sparse-View   Reconstruction Models,1970,"  Creating 3D assets from single-view images is a complex task that demands a deep understanding of the world. Recently, feed-forward 3D generative models have made significant progress by training large reconstruction models on extensive 3D datasets, with triplanes being the preferred 3D geometry representation. However, effectively utilizing the geometric priors of triplanes, while minimizing artifacts caused by generated inconsistent multi-view images, remains a challenge. In this work, we present \textbf{Fre}quency modulat\textbf{e}d tri\textbf{plane} (\textbf{Freeplane}), a simple yet effective method to improve the generation quality of feed-forward models without additional training. We first analyze the role of triplanes in feed-forward methods and find that the inconsistent multi-view images introduce high-frequency artifacts on triplanes, leading to low-quality 3D meshes. Based on this observation, we propose strategically filtering triplane features and combining triplanes before and after filtering to produce high-quality textured meshes. These techniques incur no additional cost and can be seamlessly integrated into pre-trained feed-forward models to enhance their robustness against the inconsistency of generated multi-view images. Both qualitative and quantitative results demonstrate that our method improves the performance of feed-forward models by simply modulating triplanes. All you need is to modulate the triplanes during inference. ",Kein DOI-Link verfügbar,2406.00750v1,Yes,strategically(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Higher-order accurate two-sample network inference and network hashing,1970,"  Two-sample hypothesis testing for network comparison presents many significant challenges, including: leveraging repeated network observations and known node registration, but without requiring them to operate; relaxing strong structural assumptions; achieving finite-sample higher-order accuracy; handling different network sizes and sparsity levels; fast computation and memory parsimony; controlling false discovery rate (FDR) in multiple testing; and theoretical understandings, particularly regarding finite-sample accuracy and minimax optimality. In this paper, we develop a comprehensive toolbox, featuring a novel main method and its variants, all accompanied by strong theoretical guarantees, to address these challenges. Our method outperforms existing tools in speed and accuracy, and it is proved power-optimal. Our algorithms are user-friendly and versatile in handling various data structures (single or repeated network observations; known or unknown node registration). We also develop an innovative framework for offline hashing and fast querying as a very useful tool for large network databases. We showcase the effectiveness of our method through comprehensive simulations and applications to two real-world datasets, which revealed intriguing new structures. ",Kein DOI-Link verfügbar,2208.07573v3,Yes,"innovative(1), versatile(1)"
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Evaluating the effects of high-throughput structural neuroimaging   predictors on whole-brain functional connectome outcomes via network-based   vector-on-matrix regression,1970,"  The joint analysis of multimodal neuroimaging data is critical in the field of brain research because it reveals complex interactive relationships between neurobiological structures and functions. In this study, we focus on investigating the effects of structural imaging (SI) features, including white matter micro-structure integrity (WMMI) and cortical thickness, on the whole brain functional connectome (FC) network. To achieve this goal, we propose a network-based vector-on-matrix regression model to characterize the FC-SI association patterns. We have developed a novel multi-level dense bipartite and clique subgraph extraction method to identify which subsets of spatially specific SI features intensively influence organized FC sub-networks. The proposed method can simultaneously identify highly correlated structural-connectomic association patterns and suppress false positive findings while handling millions of potential interactions. We apply our method to a multimodal neuroimaging dataset of 4,242 participants from the UK Biobank to evaluate the effects of whole-brain WMMI and cortical thickness on the resting-state FC. The results reveal that the WMMI on corticospinal tracts and inferior cerebellar peduncle significantly affect functional connections of sensorimotor, salience, and executive sub-networks with an average correlation of 0.81 (p<0.001). ",Kein DOI-Link verfügbar,2310.18533v1,Yes,potent(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,PromptKD: Unsupervised Prompt Distillation for Vision-Language Models,1970,"  Prompt learning has emerged as a valuable technique in enhancing vision-language models (VLMs) such as CLIP for downstream tasks in specific domains. Existing work mainly focuses on designing various learning forms of prompts, neglecting the potential of prompts as effective distillers for learning from larger teacher models. In this paper, we introduce an unsupervised domain prompt distillation framework, which aims to transfer the knowledge of a larger teacher model to a lightweight target model through prompt-driven imitation using unlabeled domain images. Specifically, our framework consists of two distinct stages. In the initial stage, we pre-train a large CLIP teacher model using domain (few-shot) labels. After pre-training, we leverage the unique decoupled-modality characteristics of CLIP by pre-computing and storing the text features as class vectors only once through the teacher text encoder. In the subsequent stage, the stored class vectors are shared across teacher and student image encoders for calculating the predicted logits. Further, we align the logits of both the teacher and student models via KL divergence, encouraging the student image encoder to generate similar probability distributions to the teacher through the learnable prompts. The proposed prompt distillation process eliminates the reliance on labeled data, enabling the algorithm to leverage a vast amount of unlabeled images within the domain. Finally, the well-trained student image encoders and pre-stored text features (class vectors) are utilized for inference. To our best knowledge, we are the first to (1) perform unsupervised domain-specific prompt-driven knowledge distillation for CLIP, and (2) establish a practical pre-storing mechanism of text features as shared class vectors between teacher and student. Extensive experiments on 11 datasets demonstrate the effectiveness of our method. ",Kein DOI-Link verfügbar,2403.02781v5,Yes,potent(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Multi-View Neural 3D Reconstruction of Micro-/Nanostructures with Atomic   Force Microscopy,1970,"  Atomic Force Microscopy (AFM) is a widely employed tool for micro-/nanoscale topographic imaging. However, conventional AFM scanning struggles to reconstruct complex 3D micro-/nanostructures precisely due to limitations such as incomplete sample topography capturing and tip-sample convolution artifacts. Here, we propose a multi-view neural-network-based framework with AFM (MVN-AFM), which accurately reconstructs surface models of intricate micro-/nanostructures. Unlike previous works, MVN-AFM does not depend on any specially shaped probes or costly modifications to the AFM system. To achieve this, MVN-AFM uniquely employs an iterative method to align multi-view data and eliminate AFM artifacts simultaneously. Furthermore, we pioneer the application of neural implicit surface reconstruction in nanotechnology and achieve markedly improved results. Extensive experiments show that MVN-AFM effectively eliminates artifacts present in raw AFM images and reconstructs various micro-/nanostructures including complex geometrical microstructures printed via Two-photon Lithography and nanoparticles such as PMMA nanospheres and ZIF-67 nanocrystals. This work presents a cost-effective tool for micro-/nanoscale 3D analysis. ",Kein DOI-Link verfügbar,2401.11541v1,Yes,intricate(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Extra DoF of Near-Field Holographic MIMOCommunications Leveraging   Evanescent Waves,1970,"  In this letter, we consider transceivers with spatially-constrained antenna apertures of rectangular symmetry, and aim to improve of spatial degrees of freedom (DoF) and channel capacity leveraging evanescent waves for information transmission in near-field scenarios based on the Fourier plane-wave series expansion. The treatment is limited to an isotropic scattering environment but can be extended to the non-isotropic case through the linear-system theoretic interpretation of plane-wave propagation. Numerical results show that evanescent waves have the significant potential to provide additional DoF and capacity in the near-field region. ",Kein DOI-Link verfügbar,2208.13008v1,Yes,potent(1)
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Avalon's Game of Thoughts: Battle Against Deception through Recursive   Contemplation,1970,"  Recent breakthroughs in large language models (LLMs) have brought remarkable success in the field of LLM-as-Agent. Nevertheless, a prevalent assumption is that the information processed by LLMs is consistently honest, neglecting the pervasive deceptive or misleading information in human society and AI-generated content. This oversight makes LLMs susceptible to malicious manipulations, potentially resulting in detrimental outcomes. This study utilizes the intricate Avalon game as a testbed to explore LLMs' potential in deceptive environments. Avalon, full of misinformation and requiring sophisticated logic, manifests as a ""Game-of-Thoughts"". Inspired by the efficacy of humans' recursive thinking and perspective-taking in the Avalon game, we introduce a novel framework, Recursive Contemplation (ReCon), to enhance LLMs' ability to identify and counteract deceptive information. ReCon combines formulation and refinement contemplation processes; formulation contemplation produces initial thoughts and speech, while refinement contemplation further polishes them. Additionally, we incorporate first-order and second-order perspective transitions into these processes respectively. Specifically, the first-order allows an LLM agent to infer others' mental states, and the second-order involves understanding how others perceive the agent's mental state. After integrating ReCon with different LLMs, extensive experiment results from the Avalon game indicate its efficacy in aiding LLMs to discern and maneuver around deceptive information without extra fine-tuning and data. Finally, we offer a possible explanation for the efficacy of ReCon and explore the current limitations of LLMs in terms of safety, reasoning, speaking style, and format, potentially furnishing insights for subsequent research. ",Kein DOI-Link verfügbar,2310.01320v3,Yes,"intricate(1), potent(3)"
0000-0001-7305-3793,Shuo Chen,Ludwig-Maximilians-Universität München,Generation and Detection of Surface Plasmon Polaritons by Transition   Metal Dichalcogenides for Chip-level Electronic-Photonic Integrated Circuits,1970,"  The monolithic integration of electronics and photonics has attracted enormous attention due to its potential applications. However, the realization of such hybrid circuits has remained a challenge because it requires optical communication at nanometer scales. A major challenge to this integration is the identification of a suitable material. After discussing the material aspect of the challenge, we identified atomically thin transition metal dichalcogenides (TMDs) as a perfect material platform to implement the circuit. The selection of TMDs is based on their very distinct property: monolayer TMDs are able to emit and absorb light at the same wavelength determined by direct exciton transitions. To prove the concept, we fabricated simple devices consisting of silver nanowires as plasmonic waveguides and monolayer TMDs as active optoelectronic media. Using photoexcitation, direct optical imaging and spectral analysis, we demonstrated generation and detection of surface plasmon polaritons by monolayer TMDs. Regarded as novel materials for electronics and photonics, transition metal dichalcogenides are expected to find new applications in next generation integrated circuits. ",https://doi.org/10.1021/acsphotonics.6b00101,1507.01974v2,Yes,potent(1)
0000-0002-0186-9475,Philippe Mounaix,Institut Polytechnique de Paris,Wave localization does not affect the breakdown of a Schrödinger-type   amplifier driven by the square of a Gaussian field,1970,"  We study the divergence of the solution to a Schr\""odinger-type amplifier driven by the square of a Gaussian noise in presence of a random potential. We follow the same approach as Mounaix, Collet, and Lebowitz (MCL) in terms of a distributional formulation of the amplified field and the use of the Paley-Wiener theorem [Commun. Math. Phys. {\bf 264}, 741-758 (2006) and {\bf 280}, 281-283 (2008)]. Our results show that the divergence is not affected by the random potential, in the sense that it occurs at exactly the same coupling constant as what was found by MCL without a potential. It follows {\it a fortiori} that the breakdown of the amplifier is not affected by the possible existence of a localized regime in the amplification free limit. ",Kein DOI-Link verfügbar,0911.3744v2,Yes,potent(3)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Statistical inference for pairwise comparison models,1970,"  Pairwise comparison models have been widely used for utility evaluation and ranking across various fields. The increasing scale of problems today underscores the need to understand statistical inference in these models when the number of subjects diverges, a topic currently lacking in the literature except in a few special instances. To partially address this gap, this paper establishes a near-optimal asymptotic normality result for the maximum likelihood estimator in a broad class of pairwise comparison models, as well as a non-asymptotic convergence rate for each individual subject under comparison. The key idea lies in identifying the Fisher information matrix as a weighted graph Laplacian, which can be studied via a meticulous spectral analysis. Our findings provide a unified theory for performing statistical inference in a wide range of pairwise comparison models beyond the Bradley--Terry model, benefiting practitioners with theoretical guarantees for their use. Simulations utilizing synthetic data are conducted to validate the asymptotic normality result, followed by a hypothesis test using a tennis competition dataset. ",Kein DOI-Link verfügbar,2401.08463v2,Yes,meticulous(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Gap Completion in Point Cloud Scene occluded by Vehicles using SGC-Net,1970,"  Recent advances in mobile mapping systems have greatly enhanced the efficiency and convenience of acquiring urban 3D data. These systems utilize LiDAR sensors mounted on vehicles to capture vast cityscapes. However, a significant challenge arises due to occlusions caused by roadside parked vehicles, leading to the loss of scene information, particularly on the roads, sidewalks, curbs, and the lower sections of buildings. In this study, we present a novel approach that leverages deep neural networks to learn a model capable of filling gaps in urban scenes that are obscured by vehicle occlusion. We have developed an innovative technique where we place virtual vehicle models along road boundaries in the gap-free scene and utilize a ray-casting algorithm to create a new scene with occluded gaps. This allows us to generate diverse and realistic urban point cloud scenes with and without vehicle occlusion, surpassing the limitations of real-world training data collection and annotation. Furthermore, we introduce the Scene Gap Completion Network (SGC-Net), an end-to-end model that can generate well-defined shape boundaries and smooth surfaces within occluded gaps. The experiment results reveal that 97.66% of the filled points fall within a range of 5 centimeters relative to the high-density ground truth point cloud scene. These findings underscore the efficacy of our proposed model in gap completion and reconstructing urban scenes affected by vehicle occlusions. ",https://doi.org/10.1016/j.isprsjprs.2024.07.009,2407.08290v1,Yes,innovative(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Sensing-Enhanced Secure Communication: Joint Time Allocation and   Beamforming Design,1970,"  The integration of sensing and communication enables wireless communication systems to serve environment-aware applications. In this paper, we propose to leverage sensing to enhance physical layer security (PLS) in multiuser communication systems in the presence of a suspicious target. To this end, we develop a two-phase framework to first estimate the location of the potential eavesdropper by sensing and then utilize the estimated information to enhance PLS for communication. In particular, in the first phase, a dual-functional radar and communication (DFRC) base station (BS) exploits a sensing signal to mitigate the sensing information uncertainty of the potential eavesdropper. Then, in the second phase, to facilitate joint sensing and secure communication, the DFRC BS employs beamforming and artificial noise to enhance secure communication. The design objective is to maximize the system sum rate while alleviating the information leakage by jointly optimizing the time allocation and beamforming policy. Capitalizing on monotonic optimization theory, we develop a two-layer globally optimal algorithm to reveal the performance upper bound of the considered system. Simulation results show that the proposed scheme achieves a significant sum rate gain over two baseline schemes that adopt existing techniques. Moreover, our results unveil that ISAC is a promising paradigm for enhancing secure communication in wireless networks. ",Kein DOI-Link verfügbar,2312.15231v1,Yes,potent(2)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Interference Mitigation for Network-Level ISAC: An Optimization   Perspective,1970,"  Future wireless networks are envisioned to simultaneously provide high data-rate communication and ubiquitous environment-aware services for numerous users. One promising approach to meet this demand is to employ network-level integrated sensing and communications (ISAC) by jointly designing the signal processing and resource allocation over the entire network. However, to unleash the full potential of network-level ISAC, some critical challenges must be tackled. Among them, interference management is one of the most significant ones. In this article, we build up a bridge between interference mitigation techniques and the corresponding optimization methods, which facilitates efficient interference mitigation in network-level ISAC systems. In particular, we first identify several types of interference in network-level ISAC systems, including self-interference, mutual interference, crosstalk, clutter, and multiuser interference. Then, we present several promising techniques that can be utilized to suppress specific types of interference. For each type of interference, we discuss the corresponding problem formulation and identify the associated optimization methods. Moreover, to illustrate the effectiveness of the proposed interference mitigation techniques, two concrete network-level ISAC systems, namely coordinated cellular network-based and distributed antenna-based ISAC systems, are investigated from interference management perspective. Experiment results indicate that it is beneficial to collaboratively employ different interference mitigation techniques and leverage the network structure to achieve the full potential of network-level ISAC. Finally, we highlight several promising future research directions for the design of ISAC systems. ",Kein DOI-Link verfügbar,2402.09974v1,Yes,potent(2)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,The Effective Field Theory of Dark Matter Direct Detection,1970,"  We extend and explore the general non-relativistic effective theory of dark matter (DM) direct detection. We describe the basic non-relativistic building blocks of operators and discuss their symmetry properties, writing down all Galilean-invariant operators up to quadratic order in momentum transfer arising from exchange of particles of spin 1 or less. Any DM particle theory can be translated into the coefficients of an effective operator and any effective operator can be simply related to most general description of the nuclear response. We find several operators which lead to novel nuclear responses. These responses differ significantly from the standard minimal WIMP cases in their relative coupling strengths to various elements, changing how the results from different experiments should be compared against each other. Response functions are evaluated for common DM targets - F, Na, Ge, I, and Xe - using standard shell model techniques. We point out that each of the nuclear responses is familiar from past studies of semi-leptonic electroweak interactions, and thus potentially testable in weak interaction studies. We provide tables of the full set of required matrix elements at finite momentum transfer for a range of common elements, making a careful and fully model-independent analysis possible. Finally, we discuss embedding non-relativistic effective theory operators into UV models of dark matter. ",https://doi.org/10.1088/1475-7516/2013/02/004,1203.3542v3,Yes,potent(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,Micromobility Trip Origin and Destination Inference Using General   Bikeshare Feed Specification (GBFS) Data,1970,"  Emerging micromobility services (e.g., e-scooters) have a great potential to enhance urban mobility but more knowledge on their usage patterns is needed. The General Bikeshare Feed Specification (GBFS) data are a possible source for examining micromobility trip patterns, but efforts are needed to infer trips from the GBFS data. Existing trip inference methods are usually based upon the assumption that the vehicle ID of a micromobility option (e-scooter or e-bike) does not change, and so they cannot deal with data with vehicle IDs that change over time. In this study, we propose a comprehensive package of algorithms to infer trip origins and destinations from GBFS data with different types of vehicle ID. We implement the algorithms in Washington DC by analyzing one-week (last week of February 2020) of GBFS data published by six vendors, and we evaluate the inference accuracy of the proposed algorithms by R-squared, mean absolute error, and sum absolute error. We find that the R-squared measure is larger than 0.9 and the MAE measure is smaller than 2 when the algorithms are evaluated with a 400m*400m grid, and the absolute errors are relatively larger in the downtown area. The accuracy of the trip-inference algorithms is sufficiently high for most practical applications. ",Kein DOI-Link verfügbar,2010.12006v1,Yes,potent(1)
0000-0002-6579-9808,Yiming Xu,Institut Polytechnique de Paris,A setup for extreme-ultraviolet ultrafast angle-resolved photoelectron   spectroscopy at 50-kHz repetition rate,1970,"  Time- and angle-resolved photoelectron spectroscopy (trARPES) is a powerful method to track the ultrafast dynamics of quasiparticles and electronic bands in energy and momentum space. We present a setup for trARPES with 22.3 eV extreme-ultraviolet (XUV) femtosecond pulses at 50-kHz repetition rate, which enables fast data acquisition and access to dynamics across momentum space with high sensitivity. The design and operation of the XUV beamline, pump-probe setup, and UHV endstation are described in detail. By characterizing the effect of space-charge broadening, we determine an ultimate source-limited energy resolution of 60 meV, with typically 80-100 meV obtained at 1-2e10 photons/s probe flux on the sample. The instrument capabilities are demonstrated via both equilibrium and time-resolved ARPES studies of transition-metal dichalcogenides. The 50-kHz repetition rate enables sensitive measurements of quasiparticles at low excitation fluences in semiconducting MoSe$_2$, with an instrumental time resolution of 65 fs. Moreover, photo-induced phase transitions can be driven with the available pump fluence, as shown by charge density wave melting in 1T-TiSe$_2$. The high repetition-rate setup thus provides a versatile platform for sensitive XUV trARPES, from quenching of electronic phases down to the perturbative limit. ",https://doi.org/10.1063/1.5079677,1811.00715v2,Yes,versatile(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,Towards Autocomplete Strategies for Visualization Construction,1970,"  Constructive visualization uses physical data units - tokens - to enable non-experts to create personalized visualizations engagingly. However, its physical nature limits efficiency and scalability. One potential solution to address this issue is autocomplete. By providing automated suggestions while still allowing for manual intervention, autocomplete can expedite visualization construction while maintaining expressivity. We conduct a speculative design study to examine how people would like to interact with a visualization authoring system that supports autocomplete. Our study identifies three types of autocomplete strategies and gains insights for designing future visualization authoring tools with autocomplete functionality. A free copy of this paper and all supplemental materials are available on our online repository https://osf.io/nu4z3/?view_only=594baee54d114a99ab381886fb32a126 ",Kein DOI-Link verfügbar,2308.02679v2,Yes,potent(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,The Dark Side of Perceptual Manipulations in Virtual Reality,1970,"  ""Virtual-Physical Perceptual Manipulations"" (VPPMs) such as redirected walking and haptics expand the user's capacity to interact with Virtual Reality (VR) beyond what would ordinarily physically be possible. VPPMs leverage knowledge of the limits of human perception to effect changes in the user's physical movements, becoming able to (perceptibly and imperceptibly) nudge their physical actions to enhance interactivity in VR. We explore the risks posed by the malicious use of VPPMs. First, we define, conceptualize and demonstrate the existence of VPPMs. Next, using speculative design workshops, we explore and characterize the threats/risks posed, proposing mitigations and preventative recommendations against the malicious use of VPPMs. Finally, we implement two sample applications to demonstrate how existing VPPMs could be trivially subverted to create the potential for physical harm. This paper aims to raise awareness that the current way we apply and publish VPPMs can lead to malicious exploits of our perceptual vulnerabilities. ",https://doi.org/10.1145/3491102.3517728,2202.13200v1,Yes,potent(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,Design by Immersion: A Transdisciplinary Approach to Problem-Driven   Visualizations,1970,"  While previous work exists on how to conduct and disseminate insights from problem-driven visualization projects and design studies, the literature does not address how to accomplish these goals in transdisciplinary teams in ways that advance all disciplines involved. In this paper we introduce and define a new methodological paradigm we call design by immersion, which provides an alternative perspective on problem-driven visualization work. Design by immersion embeds transdisciplinary experiences at the center of the visualization process by having visualization researchers participate in the work of the target domain (or domain experts participate in visualization research). Based on our own combined experiences of working on cross-disciplinary, problem-driven visualization projects, we present six case studies that expose the opportunities that design by immersion enables, including (1) exploring new domain-inspired visualization design spaces, (2) enriching domain understanding through personal experiences, and (3) building strong transdisciplinary relationships. Furthermore, we illustrate how the process of design by immersion opens up a diverse set of design activities that can be combined in different ways depending on the type of collaboration, project, and goals. Finally, we discuss the challenges and potential pitfalls of design by immersion. ",https://doi.org/10.1109/TVCG.2019.2934790,1908.00559v2,Yes,potent(1)
0000-0002-9319-8559,Samuel Huron,Institut Polytechnique de Paris,Memory Manipulations in Extended Reality,1970,"  Human memory has notable limitations (e.g., forgetting) which have necessitated a variety of memory aids (e.g., calendars). As we grow closer to mass adoption of everyday Extended Reality (XR), which is frequently leveraging perceptual limitations (e.g., redirected walking), it becomes pertinent to consider how XR could leverage memory limitations (forgetting, distorting, persistence) to induce memory manipulations. As memories highly impact our self-perception, social interactions, and behaviors, there is a pressing need to understand XR Memory Manipulations (XRMMs). We ran three speculative design workshops (n=12), with XR and memory researchers creating 48 XRMM scenarios. Through thematic analysis, we define XRMMs, present a framework of their core components and reveal three classes (at encoding, pre-retrieval, at retrieval). Each class differs in terms of technology (AR, VR) and impact on memory (influencing quality of memories, inducing forgetting, distorting memories). We raise ethical concerns and discuss opportunities of perceptual and memory manipulations in XR. ",https://doi.org/10.1145/3544548.3580988,2304.02394v1,Yes,notable(1)
0000-0002-8359-5921,Mireille Sarkiss,"Télécom SudParis, Institut Polytechnique de Paris",Efficient Network Representation for GNN-based Intrusion Detection,1970,"  The last decades have seen a growth in the number of cyber-attacks with severe economic and privacy damages, which reveals the need for network intrusion detection approaches to assist in preventing cyber-attacks and reducing their risks. In this work, we propose a novel network representation as a graph of flows that aims to provide relevant topological information for the intrusion detection task, such as malicious behavior patterns, the relation between phases of multi-step attacks, and the relation between spoofed and pre-spoofed attackers activities. In addition, we present a Graph Neural Network (GNN) based framework responsible for exploiting the proposed graph structure to classify communication flows by assigning them a maliciousness score. The framework comprises three main steps that aim to embed nodes features and learn relevant attack patterns from the network representation. Finally, we highlight a potential data leakage issue with classical evaluation procedures and suggest a solution to ensure a reliable validation of intrusion detection systems performance. We implement the proposed framework and prove that exploiting the flow-based graph structure outperforms the classical machine learning-based and the previous GNN-based solutions. ",https://doi.org/10.1007/978-3-031-33488-7_20,2310.05956v1,Yes,potent(1)
0000-0002-8359-5921,Mireille Sarkiss,"Télécom SudParis, Institut Polytechnique de Paris",Secrecy Capacity-Memory Tradeoff of Erasure Broadcast Channels,1970,"  This paper derives upper and lower bounds on the secrecy capacity-memory tradeoff of a wiretap erasure broadcast channel (BC) with Kw weak receivers and Ks strong receivers, where weak receivers, respectively strong receivers, have same erasure probabilities and cache sizes. The lower bounds are achieved by schemes that meticulously combine joint cache-channel coding with wiretap coding and key-aided one-time pads. The presented upper bound holds more generally for arbitrary degraded BCs and arbitrary cache sizes. When only weak receivers have cache memories, upper and lower bounds coincide for small and large cache memories, thus providing the exact secrecy capacity-memory tradeoff for this setup. The derived bounds allow to further conclude that the secrecy capacity is positive even when the eavesdropper is stronger than all the legitimate receivers with cache memories. Moreover, they show that the secrecy capacity-memory tradeoff can be significantly smaller than its non-secure counterpart, but it grows much faster when cache memories are small. The paper also presents a lower bound on the global secrecy capacity-memory tradeoff where one is allowed to optimize the cache assignment subject to a total cache budget. It is close to the best known lower bound without secrecy constraint. For small total cache budget, the global secrecy capacity-memory tradeoff is achieved by assigning all the available cache memory uniformly over all receivers if the eavesdropper is stronger than all legitimate receivers, and it is achieved by assigning the cache memory uniformly only over the weak receivers if the eavesdropper is weaker than the strong receivers. ",Kein DOI-Link verfügbar,1801.00606v1,Yes,"meticulous(1), meticulously(1)"
0000-0002-1765-1219,Martin Krejca,"Ecole Polytechnique, Institut Polytechnique de Paris",Using random graphs to sample repulsive Gibbs point processes with   arbitrary-range potentials,1970,"  We study computational aspects of repulsive Gibbs point processes, which are probabilistic models of interacting particles in a finite-volume region of space. We introduce an approach for reducing a Gibbs point process to the hard-core model, a well-studied discrete spin system. Given an instance of such a point process, our reduction generates a random graph drawn from a natural geometric model. We show that the partition function of a hard-core model on graphs generated by the geometric model concentrates around the partition function of the Gibbs point process. Our reduction allows us to use a broad range of algorithms developed for the hard-core model to sample from the Gibbs point process and approximate its partition function. This is, to the extend of our knowledge, the first approach that deals with pair potentials of unbounded range. We compare the resulting algorithms with recently established results and study further properties of the random geometric graphs with respect to the hard-core model. ",Kein DOI-Link verfügbar,2204.01793v2,Yes,potent(1)
0000-0003-4641-6944,Shihao Ding,"Telecom Paris, Institut Polytechnique de Paris",Unveiling the dynamical diversity of quantum dot lasers subject to   optoelectronic feedback,1970,"  This paper investigates experimentally and numerically the nonlinear dynamics of an epitaxial quantum dot laser on silicon subjected to optoelectronic feedback. Experimental results showcase a diverse range of dynamics, encompassing square wave patterns, quasi-chaotic states, and mixed waveforms exhibiting fast and slow oscillations. These measurements unequivocally demonstrate that quantum dot lasers on silicon readily and stably generate a more extensive repertoire of nonlinear dynamics compared to quantum well lasers. This pronounced sensitivity of quantum dot lasers to optoelectronic feedback represents a notable departure from their inherent insensitivity to optical feedback arising from reflections. Moreover, based on the Ikeda-like model, our simulations illustrate that the inherent characteristics of quantum dot lasers on silicon enable rapid and diverse dynamic transformations in response to optoelectronic feedback. The emergence of these exotic dynamics paves the way for further applications like integrated optical clocks, optical logic, and optical computing. ",Kein DOI-Link verfügbar,2309.14056v1,Yes,notable(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Rerandomization with Diminishing Covariate Imbalance and Diverging   Number of Covariates,1970,"  Completely randomized experiments have been the gold standard for drawing causal inference because they can balance all potential confounding on average. However, they may suffer from unbalanced covariates for realized treatment assignments. Rerandomization, a design that rerandomizes the treatment assignment until a prespecified covariate balance criterion is met, has recently got attention due to its easy implementation, improved covariate balance and more efficient inference. Researchers have then suggested to use the treatment assignments that minimize the covariate imbalance, namely the optimally balanced design. This has caused again the long-time controversy between two philosophies for designing experiments: randomization versus optimal and thus almost deterministic designs. Existing literature argued that rerandomization with overly balanced observed covariates can lead to highly imbalanced unobserved covariates, making it vulnerable to model misspecification. On the contrary, rerandomization with properly balanced covariates can provide robust inference for treatment effects while sacrificing some efficiency compared to the ideally optimal design. In this paper, we show it is possible that, by making the covariate imbalance diminishing at a proper rate as the sample size increases, rerandomization can achieve its ideally optimal precision that one can expect with perfectly balanced covariates, while still maintaining its robustness. We further investigate conditions on the number of covariates for achieving the desired optimality. Our results rely on a more delicate asymptotic analysis for rerandomization. The derived theory for rerandomization provides a deeper understanding of its large-sample property and can better guide its practical implementation. Furthermore, it also helps reconcile the controversy between randomized and optimal designs in an asymptotic sense. ",Kein DOI-Link verfügbar,2109.02578v2,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Debiased Inverse Propensity Score Weighting for Estimation of Average   Treatment Effects with High-Dimensional Confounders,1970,"  We consider estimation of average treatment effects given observational data with high-dimensional pretreatment variables. Existing methods for this problem typically assume some form of sparsity for the regression functions. In this work, we introduce a debiased inverse propensity score weighting (DIPW) scheme for average treatment effect estimation that delivers $\sqrt{n}$-consistent estimates when the propensity score follows a sparse logistic regression model; the outcome regression functions are permitted to be arbitrarily complex. We further demonstrate how confidence intervals centred on our estimates may be constructed. Our theoretical results quantify the price to pay for permitting the regression functions to be unestimable, which shows up as an inflation of the variance of the estimator compared to the semiparametric efficient variance by a constant factor, under mild conditions. We also show that when outcome regressions can be estimated faster than a slow $1/\sqrt{ \log n}$ rate, our estimator achieves semiparametric efficiency. As our results accommodate arbitrary outcome regression functions, averages of transformed responses under each treatment may also be estimated at the $\sqrt{n}$ rate. Thus, for example, the variances of the potential outcomes may be estimated. We discuss extensions to estimating linear projections of the heterogeneous treatment effect function and explain how propensity score models with more general link functions may be handled within our framework. An R package \texttt{dipw} implementing our methodology is available on CRAN. ",Kein DOI-Link verfügbar,2011.08661v3,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Debiased Regression Adjustment in Completely Randomized Experiments with   Moderately High-dimensional Covariates,1970,"  Completely randomized experiment is the gold standard for causal inference. When the covariate information for each experimental candidate is available, one typical way is to include them in covariate adjustments for more accurate treatment effect estimation. In this paper, we investigate this problem under the randomization-based framework, i.e., that the covariates and potential outcomes of all experimental candidates are assumed as deterministic quantities and the randomness comes solely from the treatment assignment mechanism. Under this framework, to achieve asymptotically valid inference, existing estimators usually require either (i) that the dimension of covariates $p$ grows at a rate no faster than $O(n^{2 / 3})$ as sample size $n \to \infty$; or (ii) certain sparsity constraints on the linear representations of potential outcomes constructed via possibly high-dimensional covariates. In this paper, we consider the moderately high-dimensional regime where $p$ is allowed to be in the same order of magnitude as $n$. We develop a novel debiased estimator with a corresponding inference procedure and establish its asymptotic normality under mild assumptions. Our estimator is model-free and does not require any sparsity constraint on potential outcome's linear representations. We also discuss its asymptotic efficiency improvements over the unadjusted treatment effect estimator under different dimensionality constraints. Numerical analysis confirms that compared to other regression adjustment based treatment effect estimators, our debiased estimator performs well in moderately high dimensions. ",Kein DOI-Link verfügbar,2309.02073v1,Yes,potent(3)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Online fair division with arbitrary entitlements,1970,"  The division of goods in the online realm poses opportunities and challenges. While innovative mechanisms can be developed, uncertainty about the future may hinder effective solutions. This project aims to explore fair distribution models for goods among agents with arbitrary entitlements, specifically addressing food charity challenges in the real world. Building upon prior work in [AAGW15], which focuses on equal entitlements, our project seeks to better understand the proofs of the theorems mentioned in that paper, which currently only provide proof sketches. Our approach employs different proof techniques from those presented in [AAGW15] ",Kein DOI-Link verfügbar,2304.08864v1,Yes,innovative(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Wavelet Transform-assisted Adaptive Generative Modeling for Colorization,1970,"  Unsupervised deep learning has recently demonstrated the promise of producing high-quality samples. While it has tremendous potential to promote the image colorization task, the performance is limited owing to the high-dimension of data manifold and model capability. This study presents a novel scheme that exploits the score-based generative model in wavelet domain to address the issues. By taking advantage of the multi-scale and multi-channel representation via wavelet transform, the proposed model learns the richer priors from stacked coarse and detailed wavelet coefficient components jointly and effectively. This strategy also reduces the dimension of the original manifold and alleviates the curse of dimensionality, which is beneficial for estimation and sampling. Moreover, dual consistency terms in the wavelet domain, namely data-consistency and structure-consistency are devised to leverage colorization task better. Specifically, in the training phase, a set of multi-channel tensors consisting of wavelet coefficients is used as the input to train the network with denoising score matching. In the inference phase, samples are iteratively generated via annealed Langevin dynamics with data and structure consistencies. Experiments demonstrated remarkable improvements of the proposed method on both generation and colorization quality, particularly in colorization robustness and diversity. ",https://doi.org/10.1109/TMM.2022.3177933,2107.04261v2,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Virtual Coil Augmentation Technology for MR Coil Extrapolation via Deep   Learning,1970,"  Magnetic resonance imaging (MRI) is a widely used medical imaging modality. However, due to the limitations in hardware, scan time, and throughput, it is often clinically challenging to obtain high-quality MR images. In this article, we propose a method of using artificial intelligence to expand the channel to achieve the goal of generating the virtual coils. The main characteristic of our work is utilizing dummy variable technology to expand/extrapolate the receive coils in both image and k-space domains. The high-dimensional information formed by channel expansion is used as the prior information to improve the reconstruction effect of parallel imaging. Two main components are incorporated into the network design, namely variable augmentation technology and sum of squares (SOS) objective function. Variable augmentation provides the network with more high-dimensional prior information, which is helpful for the network to extract the deep feature information of the data. The SOS objective function is employed to solve the deficiency of k-space data training while speeding up convergence. Experimental results demonstrated its great potentials in super-resolution of MR images and accelerated parallel imaging reconstruction. ",Kein DOI-Link verfügbar,2201.07540v2,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Automatic Interactive Evaluation for Large Language Models with State   Aware Patient Simulator,1970,"  Large Language Models (LLMs) have demonstrated remarkable proficiency in human interactions, yet their application within the medical field remains insufficiently explored. Previous works mainly focus on the performance of medical knowledge with examinations, which is far from the realistic scenarios, falling short in assessing the abilities of LLMs on clinical tasks. In the quest to enhance the application of Large Language Models (LLMs) in healthcare, this paper introduces the Automated Interactive Evaluation (AIE) framework and the State-Aware Patient Simulator (SAPS), targeting the gap between traditional LLM evaluations and the nuanced demands of clinical practice. Unlike prior methods that rely on static medical knowledge assessments, AIE and SAPS provide a dynamic, realistic platform for assessing LLMs through multi-turn doctor-patient simulations. This approach offers a closer approximation to real clinical scenarios and allows for a detailed analysis of LLM behaviors in response to complex patient interactions. Our extensive experimental validation demonstrates the effectiveness of the AIE framework, with outcomes that align well with human evaluations, underscoring its potential to revolutionize medical LLM testing for improved healthcare delivery. ",Kein DOI-Link verfügbar,2403.08495v4,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Med-PMC: Medical Personalized Multi-modal Consultation with a Proactive   Ask-First-Observe-Next Paradigm,1970,"  The application of the Multi-modal Large Language Models (MLLMs) in medical clinical scenarios remains underexplored. Previous benchmarks only focus on the capacity of the MLLMs in medical visual question-answering (VQA) or report generation and fail to assess the performance of the MLLMs on complex clinical multi-modal tasks. In this paper, we propose a novel Medical Personalized Multi-modal Consultation (Med-PMC) paradigm to evaluate the clinical capacity of the MLLMs. Med-PMC builds a simulated clinical environment where the MLLMs are required to interact with a patient simulator to complete the multi-modal information-gathering and decision-making task. Specifically, the patient simulator is decorated with personalized actors to simulate diverse patients in real scenarios. We conduct extensive experiments to access 12 types of MLLMs, providing a comprehensive view of the MLLMs' clinical performance. We found that current MLLMs fail to gather multimodal information and show potential bias in the decision-making task when consulted with the personalized patient simulators. Further analysis demonstrates the effectiveness of Med-PMC, showing the potential to guide the development of robust and reliable clinical MLLMs. Code and data are available at https://github.com/LiuHC0428/Med-PMC. ",Kein DOI-Link verfügbar,2408.08693v1,Yes,potent(2)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Mimicking the Mavens: Agent-based Opinion Synthesis and Emotion   Prediction for Social Media Influencers,1970,"  Predicting influencers' views and public sentiment on social media is crucial for anticipating societal trends and guiding strategic responses. This study introduces a novel computational framework to predict opinion leaders' perspectives and the emotive reactions of the populace, addressing the inherent challenges posed by the unstructured, context-sensitive, and heterogeneous nature of online communication. Our research introduces an innovative module that starts with the automatic 5W1H (Where, Who, When, What, Why, and How) questions formulation engine, tailored to emerging news stories and trending topics. We then build a total of 60 anonymous opinion leader agents in six domains and realize the views generation based on an enhanced large language model (LLM) coupled with retrieval-augmented generation (RAG). Subsequently, we synthesize the potential views of opinion leaders and predicted the emotional responses to different events. The efficacy of our automated 5W1H module is corroborated by an average GPT-4 score of 8.83/10, indicative of high fidelity. The influencer agents exhibit a consistent performance, achieving an average GPT-4 rating of 6.85/10 across evaluative metrics. Utilizing the 'Russia-Ukraine War' as a case study, our methodology accurately foresees key influencers' perspectives and aligns emotional predictions with real-world sentiment trends in various domains. ",Kein DOI-Link verfügbar,2407.20668v1,Yes,"innovative(1), potent(1)"
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Energy-Efficient NOMA Enabled Heterogeneous Cloud Radio Access Networks,1970,"  Heterogeneous cloud radio access networks (H-CRANs) are envisioned to be promising in the fifth generation (5G) wireless networks. H-CRANs enable users to enjoy diverse services with high energy efficiency, high spectral efficiency, and low-cost operation, which are achieved by using cloud computing and virtualization techniques. However, H-CRANs face many technical challenges due to massive user connectivity, increasingly severe spectrum scarcity and energy-constrained devices. These challenges may significantly decrease the quality of service of users if not properly tackled. Non-orthogonal multiple access (NOMA) schemes exploit non-orthogonal resources to provide services for multiple users and are receiving increasing attention for their potential of improving spectral and energy efficiency in 5G networks. In this article a framework for energy-efficient NOMA H-CRANs is presented. The enabling technologies for NOMA H-CRANs are surveyed. Challenges to implement these technologies and open issues are discussed. This article also presents the performance evaluation on energy efficiency of H-CRANs with NOMA. ",Kein DOI-Link verfügbar,1801.01996v1,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Magic Tokens: Select Diverse Tokens for Multi-modal Object   Re-Identification,1970,"  Single-modal object re-identification (ReID) faces great challenges in maintaining robustness within complex visual scenarios. In contrast, multi-modal object ReID utilizes complementary information from diverse modalities, showing great potentials for practical applications. However, previous methods may be easily affected by irrelevant backgrounds and usually ignore the modality gaps. To address above issues, we propose a novel learning framework named \textbf{EDITOR} to select diverse tokens from vision Transformers for multi-modal object ReID. We begin with a shared vision Transformer to extract tokenized features from different input modalities. Then, we introduce a Spatial-Frequency Token Selection (SFTS) module to adaptively select object-centric tokens with both spatial and frequency information. Afterwards, we employ a Hierarchical Masked Aggregation (HMA) module to facilitate feature interactions within and across modalities. Finally, to further reduce the effect of backgrounds, we propose a Background Consistency Constraint (BCC) and an Object-Centric Feature Refinement (OCFR). They are formulated as two new loss functions, which improve the feature discrimination with background suppression. As a result, our framework can generate more discriminative features for multi-modal object ReID. Extensive experiments on three multi-modal ReID benchmarks verify the effectiveness of our methods. The code is available at https://github.com/924973292/EDITOR. ",Kein DOI-Link verfügbar,2403.10254v1,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",DARB: A Density-Aware Regular-Block Pruning for Deep Neural Networks,1970,"  The rapidly growing parameter volume of deep neural networks (DNNs) hinders the artificial intelligence applications on resource constrained devices, such as mobile and wearable devices. Neural network pruning, as one of the mainstream model compression techniques, is under extensive study to reduce the number of parameters and computations. In contrast to irregular pruning that incurs high index storage and decoding overhead, structured pruning techniques have been proposed as the promising solutions. However, prior studies on structured pruning tackle the problem mainly from the perspective of facilitating hardware implementation, without analyzing the characteristics of sparse neural networks. The neglect on the study of sparse neural networks causes inefficient trade-off between regularity and pruning ratio. Consequently, the potential of structurally pruning neural networks is not sufficiently mined.   In this work, we examine the structural characteristics of the irregularly pruned weight matrices, such as the diverse redundancy of different rows, the sensitivity of different rows to pruning, and the positional characteristics of retained weights. By leveraging the gained insights as a guidance, we first propose the novel block-max weight masking (BMWM) method, which can effectively retain the salient weights while imposing high regularity to the weight matrix. As a further optimization, we propose a density-adaptive regular-block (DARB) pruning that outperforms prior structured pruning work with high pruning ratio and decoding efficiency. Our experimental results show that DARB can achieve 13$\times$ to 25$\times$ pruning ratio, which are 2.8$\times$ to 4.3$\times$ improvements than the state-of-the-art counterparts on multiple neural network models and tasks. Moreover, DARB can achieve 14.3$\times$ decoding efficiency than block pruning with higher pruning ratio. ",Kein DOI-Link verfügbar,1911.08020v2,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China","State of the Art, Taxonomy, and Open Issues on Cognitive Radio Networks   with NOMA",1970,"  The explosive growth of mobile devices and the rapid increase of wideband wireless services call for advanced communication techniques that can achieve high spectral efficiency and meet the massive connectivity requirement. Cognitive radio (CR) and non-orthogonal multiple access (NOMA) are envisioned to be important solutions for the fifth generation wireless networks. Integrating NOMA techniques into CR networks (CRNs) has the tremendous potential to improve spectral efficiency and increase the system capacity. However, there are many technical challenges due to the severe interference caused by using NOMA. Many efforts have been made to facilitate the application of NOMA into CRNs and to investigate the performance of CRNs with NOMA. This article aims to survey the latest research results along this direction. A taxonomy is devised to categorize the literature based on operation paradigms, enabling techniques, design objectives and optimization characteristics. Moreover, the key challenges are outlined to provide guidelines for the domain researchers and designers to realize CRNs with NOMA. Finally, the open issues are discussed. ",Kein DOI-Link verfügbar,1801.01997v1,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",SQUID oscillations in PbTe nanowire networks,1970,"  Network structures by semiconductor nanowires hold great promise for advanced quantum devices, especially for applications in topological quantum computing. In this study, we created networks of PbTe nanowires arranged in loop configurations. Using shadow-wall epitaxy, we defined superconducting quantum interference devices (SQUIDs) using the superconductor Pb. These SQUIDs exhibit oscillations in supercurrent upon the scanning of a magnetic field. Most of the oscillations can be fitted assuming a sinusoidal current-phase relation for each Josephson junction. Under certain conditions, the oscillations are found to be skewed, suggesting possible deviation from a sinusoidal behavior. Our results highlight the potential of PbTe nanowires for building complex quantum devices in the form of networks. ",https://doi.org/10.1103/PhysRevB.110.045405,2404.06899v1,Yes,potent(1)
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Scientific Large Language Models: A Survey on Biological & Chemical   Domains,1970,"  Large Language Models (LLMs) have emerged as a transformative power in enhancing natural language comprehension, representing a significant stride toward artificial general intelligence. The application of LLMs extends beyond conventional linguistic boundaries, encompassing specialized linguistic systems developed within various scientific disciplines. This growing interest has led to the advent of scientific LLMs, a novel subclass specifically engineered for facilitating scientific discovery. As a burgeoning area in the community of AI for Science, scientific LLMs warrant comprehensive exploration. However, a systematic and up-to-date survey introducing them is currently lacking. In this paper, we endeavor to methodically delineate the concept of ""scientific language"", whilst providing a thorough review of the latest advancements in scientific LLMs. Given the expansive realm of scientific disciplines, our analysis adopts a focused lens, concentrating on the biological and chemical domains. This includes an in-depth examination of LLMs for textual knowledge, small molecules, macromolecular proteins, genomic sequences, and their combinations, analyzing them in terms of model architectures, capabilities, datasets, and evaluation. Finally, we critically examine the prevailing challenges and point out promising research directions along with the advances of LLMs. By offering a comprehensive overview of technical developments in this field, this survey aspires to be an invaluable resource for researchers navigating the intricate landscape of scientific LLMs. ",Kein DOI-Link verfügbar,2401.14656v2,Yes,"intricate(1), invaluable(1), methodically(1)"
0000-0001-5650-8186,Yuhao Wang,"University of Nottingham, University of Nottingham - Ningbo China",Conductance Quantization in PbTe Nanowires,1970,"  PbTe nanowires coupled to a superconductor have recently been proposed as a potential Majorana platform. The hallmark of the one-dimensional nature of ballistic nanowires is their quantized conductance. Here, we report the observation of conductance plateaus at multiples of the quantized value $2e^2/h$ in PbTe nanowires at finite magnetic fields. The quantized plateaus, as a function of source-drain bias and magnetic field, allow for the extraction of the Land\'e $g$-factor, sub-band spacing and effective mass. The coefficient of 2 in the plateau conductance indicates the presence of valley degeneracy arising from the crystal orientation of the nanowires, which are grown on a (001) substrate. Occasionally, this degeneracy can be lifted by a gate voltage that breaks the mirror symmetry. Our results demonstrate the one-dimensionality of PbTe nanowires and fulfill one of the necessary conditions for the realization of Majorana zero modes. ",https://doi.org/10.1103/PhysRevB.108.045426,2304.10194v1,Yes,potent(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",The Minimal Seesaw Model at the TeV Scale,1970,"  We point out that the minimal seesaw model can provide a natural framework to accommodate tiny neutrino masses, while its experimental testability and notable predictiveness are still maintained. This possibility is based on the observation that two heavy right-handed Majorana neutrinos in the minimal seesaw model may naturally emerge as a pseudo-Dirac fermion. In a specific scenario, we show that the tri-bimaximal neutrino mixing can be produced, and only the inverted neutrino mass hierarchy is allowed. The low-energy phenomena, including non-unitarity effects in neutrino oscillations, neutrinoless double-beta decays and rare lepton-flavor-violating decays of charged leptons l_alpha to l_beta + gamma, have been explored. The collider signatures of the heavy singlet neutrino are also briefly discussed. ",https://doi.org/10.1016/j.physletb.2010.02.015,0912.2661v1,Yes,notable(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",Exploring Virtual Reality through Ihde's Instrumental Realism,1970,"  Based on Ihde's theory, this paper explores the relationship between virtual reality (VR) as an instrument and phenomenology. It reviews the ""technological revolution"" spurred by the development of VR technology and discusses how VR has been used to study subjective experience, explore perception and embodiment, enhance empathy and perspective, and investigate altered states of consciousness. The paper emphasizes the role of VR as an instrumental technology, particularly its ability to expand human perception and cognition. Reflecting on this in conjunction with the work of Husserl and Ihde, among others, it revisits the potential of VR to provide new avenues for scientific inquiry and experience and to transform our understanding of the world through VR. ",https://doi.org/10.1007/978-3-031-57860-1_6,2401.12521v1,Yes,potent(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",Non-unitarity effects in a realistic low-scale seesaw model,1970,"  We analyze the structure of the non-unitary leptonic mixing matrix in the inverse seesaw model with heavy singlets accessible at the LHC. In this model, unlike in the usual TeV seesaw scenarios, thelow-scale right-handed neutrinos do not suffer from naturalness issues. Underlying correlations among various parameters governing the non-unitarity effects are established, which leads to a considerable improvement of the generic non-unitarity bounds. In view of this, we study the discovery potential of the non-unitarity effects at future experiments, focusing on the sensitivity limits at a neutrino factory. ",https://doi.org/10.1103/PhysRevD.79.073009,0903.1961v2,Yes,potent(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",Systematic search for successful lepton mixing patterns with nonzero   theta_13,1970,"  We perform a systematic search for simple but viable lepton mixing patterns. Our main criterion is that the mixing matrix can be parameterized by three rotation angles, which are simple fractions of pi. These simple rotation angles possess exact expressions for their sines and cosines, and often arise in the flavor symmetry models. All possible parameterizations of the mixing matrix are taken into account. In total, twenty successful mixing patterns are found to be consistent with the latest neutrino oscillation data (including the recent T2K results) in the CP conserving case, whereas fifteen mixing patterns are allowed in the maximal CP violating case. Potential radiative corrections to the constant mixing patterns are also calculated by solving the renormalization group equations. ",https://doi.org/10.1016/j.nuclphysb.2011.10.017,1107.3970v2,Yes,potent(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",Flow to Rare Events: An Application of Normalizing Flow in Temporal   Importance Sampling for Automated Vehicle Validation,1970,"  Automated Vehicle (AV) validation based on simulated testing requires unbiased evaluation and high efficiency. One effective solution is to increase the exposure to risky rare events while reweighting the probability measure. However, characterizing the distribution of risky events is particularly challenging due to the paucity of samples and the temporality of continuous scenario variables. To solve it, we devise a method to represent, generate, and reweight the distribution of risky rare events. We decompose the temporal evolution of continuous variables into distribution components based on conditional probability. By introducing the Risk Indicator Function, the distribution of risky rare events is theoretically precipitated out of naturalistic driving distribution. This targeted distribution is practically generated via Normalizing Flow, which achieves exact and tractable probability evaluation of intricate distribution. The rare event distribution is then demonstrated as the advantageous Importance Sampling distribution. We also promote the technique of temporal Importance Sampling. The combined method, named as TrimFlow, is executed to estimate the collision rate of Car-following scenarios as a tentative practice. The results showed that sampling background vehicle maneuvers from rare event distribution could evolve testing scenarios to hazardous states. TrimFlow reduced 86.1% of tests compared to generating testing scenarios according to their exposure in the naturalistic driving environment. In addition, the TrimFlow method is not limited to one specific type of functional scenario. ",Kein DOI-Link verfügbar,2407.07320v1,Yes,intricate(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",ThreshKnot: Thresholded ProbKnot for Improved RNA Secondary Structure   Prediction,1970,"  RNA structure prediction is a challenging problem, especially with pseudoknots. Recently, there has been a shift from the classical minimum free energy-based methods (MFE) to partition function-based ones that assemble structures using base-pairing probabilities. Two examples of the latter group are the popular maximum expected accuracy (MEA) method and the ProbKnot method. ProbKnot is a fast heuristic that pairs nucleotides that are reciprocally most probable pairing partners, and unlike MEA, can also predict structures with pseudoknots. However, ProbKnot's full potential has been largely overlooked. In particular, when introduced, it did not have an MEA-like hyperparameter that can balance between positive predictive value (PPV) and sensitivity. We show that a simple thresholded version of ProbKnot, which we call ThreshKnot, leads to more accurate overall predictions by filtering out unlikely pairs whose probabilities fall under a given threshold. We also show that on three widely-used folding engines (RNAstructure, Vienna RNAfold, and CONTRAfold), ThreshKnot always outperforms the much more involved MEA algorithm in (1) its higher structure prediction accuracy, (2) its capability to predict pseudoknots, and (3) its faster runtime and easier implementation. This suggests that ThreshKnot should replace MEA as the default partition function-based structure prediction algorithm. ThreshKnot is already available in the widely used RNAstructure software package version 6.2 (released November 27, 2019): https://rna.urmc.rochester.edu/RNAstructure.html ",Kein DOI-Link verfügbar,1912.12796v2,Yes,potent(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",Non-unitary neutrino mixing and CP violation in the minimal inverse   seesaw model,1970,"  We propose a simplified version of the inverse seesaw model, in which only two pairs of the gauge-singlet neutrinos are introduced, to interpret the observed neutrino mass hierarchy and lepton flavor mixing at or below the TeV scale. This minimal inverse seesaw scenario (MISS) is technically natural and experimentally testable. In particular, we show that the effective parameters describing the non-unitary neutrino mixing matrix are strongly correlated in the MISS, and thus, their upper bounds can be constrained by current experimental data in a more restrictive way. The Jarlskog invariants of non-unitary CP violation are calculated, and the discovery potential of such new CP-violating effects in the near detector of a neutrino factory is discussed. ",https://doi.org/10.1016/j.physletb.2009.07.038,0905.2889v2,Yes,potent(1)
0000-0002-1184-014X,He Zhang,"University of Nottingham, University of Nottingham - Ningbo China",The Cloud's Cloudy Moment: A Systematic Survey of Public Cloud Service   Outage,1970,"  Inadequate service availability is the top concern when employing Cloud computing. It has been recognized that zero downtime is impossible for large-scale Internet services. By learning from the previous and others' mistakes, nevertheless, it is possible for Cloud vendors to minimize the risk of future downtime or at least keep the downtime short. To facilitate summarizing lessons for Cloud providers, we performed a systematic survey of public Cloud service outage events. This paper reports the result of this survey. In addition to a set of findings, our work generated a lessons framework by classifying the outage root causes. The framework can in turn be used to arrange outage lessons for reference by Cloud providers. By including potentially new root causes, this lessons framework will be smoothly expanded in our future work. ",https://doi.org/10.11591/closer.v2i5.5125,1312.6485v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China","Reducibility, Lyapunov exponent, pure point spectra property for   quasi-periodic wave operator",1970,"  In the present paper, the reducibility is derived for linear wave equation of finite smooth and time-quasi-periodic potential subject to Dirichlet boundary condition. Moreover, it is proved that the corresponding wave operator possesses the property of pure point spectra and zero Lyapunov exponent. ",Kein DOI-Link verfügbar,1706.06713v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",A reducibility result for Schrödinger operator with finite smooth and   time quasi-periodic potential,1970,"  In the present paper, we establish a reduction theorem for linear Schr\""odinger equation with finite smooth and time-quasi-periodic potential subject to Dirichlet boundary condition by means of KAM technique. Moreover, it is proved that the corresponding Schr\""odinger operator possesses the property of pure point spectra and zero Lyapunov exponent. ",Kein DOI-Link verfügbar,1706.06767v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",The stability analysis based on viscous theory of Faraday waves in   Hele-Shaw cells,1970,"  The linear instability of Faraday waves in Hele-Shaw cells is investigated with consideration of the viscosity of fluids after gap-averaging the governing equations due to the damping from two lateral walls and the dynamic behavior of contact angle. A new hydrodynamic model is thus derived and solved semi-analytically. The contribution of viscosity to critical acceleration amplitude is slight compared to other factors associated with dissipation, and the potential flow theory is sufficient to describe onset based on the present study, but the rotational component of velocity can change the timing of onset largely, which paradoxically comes from the viscosity. The model degenerates into a novel damped Mathieu equation if the viscosity is dropped with two damping terms referring to the gap-averaged damping and dissipation from dynamic contact angle, respectively. The former increases when the gap size decreases, and the latter grows as frequency rises. When it comes to the dispersion relation of Faraday waves, an unusual detuning emerges due to the imaginary part of the gap-averaged damping. ",Kein DOI-Link verfügbar,2402.05505v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",AnglE-optimized Text Embeddings,1970,"  High-quality text embedding is pivotal in improving semantic textual similarity (STS) tasks, which are crucial components in Large Language Model (LLM) applications. However, a common challenge existing text embedding models face is the problem of vanishing gradients, primarily due to their reliance on the cosine function in the optimization objective, which has saturation zones. To address this issue, this paper proposes a novel angle-optimized text embedding model called AnglE. The core idea of AnglE is to introduce angle optimization in a complex space. This novel approach effectively mitigates the adverse effects of the saturation zone in the cosine function, which can impede gradient and hinder optimization processes. To set up a comprehensive STS evaluation, we experimented on existing short-text STS datasets and a newly collected long-text STS dataset from GitHub Issues. Furthermore, we examine domain-specific STS scenarios with limited labeled data and explore how AnglE works with LLM-annotated data. Extensive experiments were conducted on various tasks including short-text STS, long-text STS, and domain-specific STS tasks. The results show that AnglE outperforms the state-of-the-art (SOTA) STS models that ignore the cosine saturation zone. These findings demonstrate the ability of AnglE to generate high-quality text embeddings and the usefulness of angle optimization in STS. ",Kein DOI-Link verfügbar,2309.12871v8,Yes,pivotal(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",HEP ML Lab: An end-to-end framework for applying machine learning into   phenomenology studies,1970,"  Recent years have seen the development and growth of machine learning in high energy physics. There will be more effort to continue exploring its full potential. To make it easier for researchers to apply existing algorithms and neural networks and to advance the reproducibility of the analysis, we develop the \texttt{HEP ML Lab} (\texttt{hml}), a Python-based, end-to-end framework for phenomenology studies. It covers the complete workflow from event generation to performance evaluation, and provides a consistent style of use for different approaches. We propose an observable naming convention to streamline the data extraction and conversion processes. In the \texttt{Keras} style, we provide the traditional cut-and-count and boosted decision trees together with neural networks. We take the $W^+$ tagging as an example and evaluate all built-in approaches with the metrics of significance and background rejection. With its modular design, \texttt{HEP ML Lab} is easy to extend and customize, and can be used as a tool for both beginners and experienced researchers. ",Kein DOI-Link verfügbar,2405.02888v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Students' Difficulties with Equations involving Circuit Elements,1970,"  We discuss an investigation exploring students' difficulties with equations involving resistance, capacitance and inductance. We find that introductory physics students have great difficulty understanding, e.g., how the resistance of an ohmic resistor can be written in terms of the potential difference across it and the current through it, but it does not change when the potential difference across the resistor is varied. Similar confusions arose in problems relating to capacitors and inductors. We discuss these difficulties with equations in the context of introductory physics students' performance on questions about circuit elements both in the free-response and multiple-choice formats. ",https://doi.org/10.1063/1.3680040,1602.04872v1,Yes,potent(2)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",An Attention Based Neural Network for Jet Tagging,1970,"  Convolutional neural networks are basic structures using jet images as input for the jet tagging problems. However, what they have learned during the training process is always difficult to understand just through feature maps. Inspired by the attention mechanism popular in machine learning fields, we propose a novel attention-based neural network (ABNN) to get insight of this problem. The ABNN combines a jet image with average jet images from the signal and the background to generate attention maps which show clearly the relevant importance according to the different origination of jets. Compared with networks in the similar architecture, this network achieves better performance, which indicates the potential of attention mechanism to use in other works. ",Kein DOI-Link verfügbar,2009.00170v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China","Disappearance of Comet C/2010 X1 (Elenin): Gone with a Whimper, not a   Bang",1970,"  We examine the rise and sudden demise of comet C/2010 X1 (Elenin) on its approach to perihelion. Discovered inbound at 4.2 AU, this long-period comet was predicted to become very bright when near perihelion, at 0.48 AU on 2011 September 10. Observations starting 2011 February (heliocentric distance $\sim$3.5 AU) indeed show the comet to brighten by about 11 magnitudes, with most of the increase occurring inside 1 AU from the Sun. The peak brightness reached $m_R$ = 6 on UT 2011 August 12.95$\pm$0.50, when at $\sim$0.83 AU from the Sun. We find that most of the surge in brightness in mid-August resulted from dust particle forward-scattering, not from a sudden increase in the activity. A much smaller ($\sim$3 magnitudes) brightening reached a maximum on UT 2011 August 30$\pm$1 (at 0.56 AU), and reflects the true break-up of the nucleus. This second peak was matched by a change in the morphology from centrally condensed to diffuse. The estimated cross-section of the nucleus when at 1 AU inbound was $\sim$1 km$^2$, corresponding to an equal-area circle of radius 0.6 km. No surviving fragments were found to a limiting red magnitude $r'$ = 24.4, corresponding to radii $\lesssim$40 m (red geometric albedo = 0.04 assumed). Our observations are consistent with disintegration of the nucleus into a power law size distribution of fragments with index $q$ = 3.3$\pm$0.2 combined with the action of radiation pressure. We speculate about physical processes that might cause nucleus disruption in a comet when still 0.7 AU from the Sun. Tidal stresses and devolatilization of the nucleus by sublimation are both negligible at this distance. However, the torque caused by mass loss, even at the very low rates measured in comet Elenin, is potentially large enough to be responsible by driving the nucleus to rotational instability. ",https://doi.org/10.1088/0004-6256/149/4/133,1503.00387v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Hydrogen molecule spectrum by many-body GW and Bethe-Salpeter equation,1970,"  We check the ab initio GW approximation and Bethe-Salpeter equation (BSE) many-body methodology against the exact solution benchmark of the hydrogen molecule H$_2$ ground state and excitation spectrum, and in comparison with the configuration interaction (CI) and time-dependent Hartree-Fock methods. The comparison is made on all the states we could unambiguously identify from the excitonic wave functions' symmetry. At the equilibrium distance $R = 1.4 \, a_0$, the GW+BSE energy levels are in good agreement with the exact results, with an accuracy of 0.1~0.2 eV. GW+BSE potential-energy curves are also in good agreement with the CI and the exact result up to $2.3 \, a_0$. The solution no longer exists beyond $3.0 \, a_0$ for triplets ($4.3 \, a_0$ for singlets) due to instability of the ground state. We tried to improve the GW reference ground state by a renormalized random-phase approximation (r-RPA), but this did not solve the problem. ",https://doi.org/10.1103/PhysRevA.103.012809,2010.07780v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Recursive-Cube-of-Rings (RCR) Revisited: Properties and Enhancement,1970,"  We study recursive-cube-of-rings (RCR), a class of scalable graphs that can potentially provide rich inter-connection network topology for the emerging distributed and parallel computing infrastructure. Through rigorous proof and validating examples, we have corrected previous misunderstandings on the topological properties of these graphs, including node degree, symmetry, diameter and bisection width. To fully harness the potential of structural regularity through RCR construction, new edge connecting rules are proposed. The modified graphs, referred to as {\it Class-II RCR}, are shown to possess uniform node degrees, better connectivity and better network symmetry, and hence will find better application in parallel computing. ",Kein DOI-Link verfügbar,1305.2214v1,Yes,potent(2)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",A Manually Annotated Chinese Corpus for Non-task-oriented Dialogue   Systems,1970,"  This paper presents a large-scale corpus for non-task-oriented dialogue response selection, which contains over 27K distinct prompts more than 82K responses collected from social media. To annotate this corpus, we define a 5-grade rating scheme: bad, mediocre, acceptable, good, and excellent, according to the relevance, coherence, informativeness, interestingness, and the potential to move a conversation forward. To test the validity and usefulness of the produced corpus, we compare various unsupervised and supervised models for response selection. Experimental results confirm that the proposed corpus is helpful in training response selection models. ",Kein DOI-Link verfügbar,1805.05542v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",The Effective Potential Originating from Swampland and the Non-trivial   Brans-Dicke Coupling,1970,"  The effective vacuum energy density contributed by the non-trivial contortion distribution and the bare vacuum energy density can be viewed as the energy density of the auxiliary quintessence field potential. We find that the negative bare vacuum energy density from string landscape leads to a monotonically decreasing quintessence potential while the positive one from swampland leads to the meta stable or stable de Sitter like potential. Moreover, the non-trivial Brans-Dicke like coupling between quintessence field and gravitation field is necessary in the latter case. ",https://doi.org/10.1088/1674-1137/abab8e,2003.09121v1,Yes,potent(3)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Quantum heat engine based on a spin-orbit and Zeeman-coupled   Bose-Einstein condensate,1970,  We explore the potential of a spin-orbit coupled Bose-Einstein condensate for thermodynamic cycles. For this purpose we propose a quantum heat engine based on a condensate with spin-orbit and Zeeman coupling as a working medium. The cooling and heating are simulated by contacts of the condensate with an external magnetized media and demagnetized media. We examine the condensate ground state energy and its dependence on the strength of the synthetic spin-orbit and Zeeman couplings and interatomic interaction. Then we study the efficiency of the proposed engine. The cycle has a critical value of spin-orbit coupling related to the engine maximum efficiency. ,https://doi.org/10.1103/PhysRevA.106.L030201,2206.05041v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Fast transport of Bose-Einstein condensates in anharmonic traps,1970,"  We present a method to transport Bose-Einstein condensates (BECs) in anharmonic traps and in the presence of atom-atom interactions in short times without residual excitation. Using a combination of a variational approach and inverse engineering methods, we derive a set of Ermakov-like equations that take into account the coupling between the center of mass motion and the breathing mode. By an appropriate inverse engineering strategy of those equations, we then design the trap trajectory to achieve the desired boundary conditions. Numerical examples for cubic or quartic anharmonicities are provided for fast and high-fidelity transport of BECs. Potential applications are atom interferometry and quantum information processing. ",https://doi.org/10.1098/rsta.2021.0280,2210.03788v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Non-ohmic spin transport in n-type doped silicon,1970,"  We demonstrate the injection and transport of spin-polarized electrons through n-type doped silicon with in-plane spin-valve and perpendicular magnetic field spin precession and dephasing (""Hanle effect"") measurements. A voltage applied across the transport layer is used to vary the confinement potential caused by conduction band-bending and control the dominant transport mechanism between drift and diffusion. By modeling transport in this device with a Monte-Carlo scheme, we simulate the observed spin polarization and Hanle features, showing that the average transit time across the short Si transport layer can be controlled over 4 orders of magnitude with applied voltage. As a result, this modeling allows inference of a long electron spin lifetime, despite the short transit length. ",https://doi.org/10.1103/PhysRevB.78.165329,0711.4828v3,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Reducibility for wave equations of finitely smooth potential with   periodic boundary conditions,1970,"  In the present paper, the reducibility is derived for the wave equations with finitely smooth and time-quasi-periodic potential subjects to periodic boundary conditions. More exactly, the linear wave equation $u_{tt}-u_{xx}+Mu+\varepsilon (V_0(\omega t)u_{xx}+V(\omega t, x)u)=0,\;x\in \mathbb{R}/2\pi \mathbb{Z}$ can be reduced to a linear Hamiltonian system of a constant coefficient operator which is of pure imaginary point spectrum set, where $V$ is finitely smooth in $(t, x)$, quasi-periodic in time $t$ with Diophantine frequency $\omega\in \mathbb{R}^{n},$ and $V_0$ is finitely smooth and quasi-periodic in time $t$ with Diophantine frequency $\omega\in \mathbb{R}^{n},$ Moreover, it is proved that the corresponding wave operator possesses the property of pure point spectra and zero Lyapunov exponent. ",Kein DOI-Link verfügbar,1802.08133v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Stochastic collocation methods via minimization of Transformed $L_1$   penalty,1970,"  We study the properties of sparse reconstruction of transformed $\ell_1$ (TL1) minimization and present improved theoretical results about the recoverability and the accuracy of this reconstruction from undersampled measurements. We then combine this method with the stochastic collocation approach to identify the coefficients of sparse orthogonal polynomial expansions for uncertainty quantification. In order to implement the TL1 minimization, we use the DCA-TL1 algorithm which was introduced by Zhang and Xin. In particular, when recover non-sparse functions, we adopt an adaptive DCA-TL1 method to guarantee the sparest solutions. Various numerical examples, including sparse polynomial functions recovery and non-sparse analytical functions recovery are presented to demonstrate the recoverability and efficiency of this novel method and its potential for problems of practical interests. ",Kein DOI-Link verfügbar,1805.05416v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Testing the seesaw mechanisms via displaced right-handed neutrinos from   a light scalar at the HL-LHC,1970,"  We investigate the pair production of right-handed neutrinos from the decay of a light $B-L$ scalar in the $U(1)_{B-L}$ model. The $B-L$ scalar mixes to the SM Higgs, and the physical scalar is required to be lighter than the observed Higgs. The produced right-handed neutrinos are predicted to be long-lived according to the type-I seesaw mechanism, and yield potentially distinct signatures such as displaced vertex and time-delayed leptons at the CMS/ATLAS/LHCb, as well as signatures at the far detectors including the CODEX-b, FACET, FASER, MoEDAL-MAPP and MATHUSLA. We analyze the sensitivity reach at the HL-LHC for the right-handed neutrinos with masses of 2.5 $\sim$ 30 GeV, showing that the active-sterile mixing to muons can be probed to $V_{\mu N} \sim 10^{-5}$ at the CMS/ATLAS/LHCb using the displaced vertex searches, and one magnitude lower at the MATHUSLA/CMS using time-delayed leptons searches, reaching the parameter space interesting for type-I seesaw mechanisms. ",https://doi.org/10.1103/PhysRevD.106.015019,2204.03819v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",InfoDiffusion: Information Entropy Aware Diffusion Process for   Non-Autoregressive Text Generation,1970,"  Diffusion models have garnered considerable interest in the field of text generation. Several studies have explored text diffusion models with different structures and applied them to various tasks, including named entity recognition and summarization. However, there exists a notable disparity between the ""easy-first"" text generation process of current diffusion models and the ""keyword-first"" natural text generation process of humans, which has received limited attention. To bridge this gap, we propose InfoDiffusion, a non-autoregressive text diffusion model. Our approach introduces a ""keyinfo-first"" generation strategy and incorporates a noise schedule based on the amount of text information. In addition, InfoDiffusion combines self-conditioning with a newly proposed partially noising model structure. Experimental results show that InfoDiffusion outperforms the baseline model in terms of generation quality and diversity, as well as exhibiting higher sampling efficiency. ",Kein DOI-Link verfügbar,2310.11976v1,Yes,notable(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",GRAPE: Generalizable and Robust Multi-view Facial Capture,1970,"  Deep learning-based multi-view facial capture methods have shown impressive accuracy while being several orders of magnitude faster than a traditional mesh registration pipeline. However, the existing systems (e.g. TEMPEH) are strictly restricted to inference on the data captured by the same camera array used to capture their training data. In this study, we aim to improve the generalization ability so that a trained model can be readily used for inference (i.e. capture new data) on a different camera array. To this end, we propose a more generalizable initialization module to extract the camera array-agnostic 3D feature, including a visual hull-based head localization and a visibility-aware 3D feature aggregation module enabled by the visual hull. In addition, we propose an ``update-by-disagreement'' learning strategy to better handle data noise (e.g. inaccurate registration, scan noise) by discarding potentially inaccurate supervision signals during training. The resultant generalizable and robust topologically consistent multi-view facial capture system (GRAPE) can be readily used to capture data on a different camera array, reducing great effort on data collection and processing. Experiments on the FaMoS and FaceScape datasets demonstrate the effectiveness of the proposed method. ",Kein DOI-Link verfügbar,2407.10193v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Spin-Polarized Transient Electron Trapping in Phosphorus-doped Silicon,1970,"  Experimental evidence of electron spin precession during travel through the phosphorus-doped Si channel of an all-electrical device simultaneously indicates two distinct processes: (i) short timescales (~50ps) due to purely conduction-band transport from injector to detector, and (ii) long timescales (~1ns) originating from delays associated with capture/re-emission in shallow impurity traps. The origin of this phenomenon, examined via temperature, voltage, and electron density dependence measurements, is established by means of comparison to a numerical model and is shown to reveal the participation of metastable excited states in the phosphorus impurity spectrum. This work therefore demonstrates the potential to make the study of macroscopic spin transport relevant to the quantum regime of individual spin interactions with impurities as envisioned for quantum information applications. ",https://doi.org/10.1103/PhysRevLett.106.217202,1101.1944v4,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",On the motion of three-dimensional compressible isentropic flows with   large external potential forces and vacuum,1970,"  We study the global existence and uniqueness of classical solutions to the three-dimensional compressible isentropic Navier-Stokes equations with vacuum and external potential forces which could be arbitrarily large provided the initial data is of small energy and the unique steady state is strictly away from vacuum. In particular, the solution may have large oscillations and contain vacuum states. For the case of discontinuous initial data, we also prove the global existence of weak solutions. The large-time behavior of the solution is obtained simultaneously. It is worthwhile mentioning that the compatibility condition on the initial data and the regularity condition of the external potential forces in the present paper are much weaker than those assumed in the existing literature. ",Kein DOI-Link verfügbar,1111.2114v1,Yes,potent(2)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Probing Heavy Neutrinos at the LHC from Fat-jet using Machine Learning,1970,"  We explore the potential to use machine learning methods to search for heavy neutrinos, from their hadronic final states including a fat-jet signal, via the processes $pp \rightarrow W^{\pm *}\rightarrow \mu^{\pm} N \rightarrow \mu^{\pm} \mu^{\mp} W^{\pm} \rightarrow \mu^{\pm} \mu^{\mp} J$ at hadron colliders. We use either the Gradient Boosted Decision Tree or Multi-Layer Perceptron methods to analyse the observables incorporating the jet substructure information, which is performed at hadron colliders with $\sqrt{s}=$ 13, 27, 100 TeV. It is found that, among the observables, the invariant masses of variable system and the observables from the leptons are the most powerful ones to distinguish the signal from the background. With the help of machine learning techniques, the limits on the active-sterile mixing have been improved by about one magnitude comparing to the cut-based analyses, with $V_{\mu N}^2 \lesssim 10^{-4}$ for the heavy neutrinos with masses, 100 GeV$~<m_N<~$1 TeV. ",Kein DOI-Link verfügbar,2303.15920v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",LLM4Decompile: Decompiling Binary Code with Large Language Models,1970,"  Decompilation aims to convert binary code to high-level source code, but traditional tools like Ghidra often produce results that are difficult to read and execute. Motivated by the advancements in Large Language Models (LLMs), we propose LLM4Decompile, the first and largest open-source LLM series (1.3B to 33B) trained to decompile binary code. We optimize the LLM training process and introduce the LLM4Decompile-End models to decompile binary directly. The resulting models significantly outperform GPT-4o and Ghidra on the HumanEval and ExeBench benchmarks by over 100%. Additionally, we improve the standard refinement approach to fine-tune the LLM4Decompile-Ref models, enabling them to effectively refine the decompiled code from Ghidra and achieve a further 16.2% improvement over the LLM4Decompile-End. LLM4Decompile demonstrates the potential of LLMs to revolutionize binary code decompilation, delivering remarkable improvements in readability and executability while complementing conventional tools for optimal results. Our code, dataset, and models are released at https://github.com/albertan017/LLM4Decompile ",Kein DOI-Link verfügbar,2403.05286v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Domain wall migration-mediated ferroelectric switching and Rashba effect   tuning in GeTe thin films,1970,"  Germanium Telluride (GeTe), identified as a ferroelectric Rashba semiconductor, is a promising candidate for future electronic devices in computing and memory applications. However, its ferroelectric switching on a microscopic scale remains to be understood. Here, we propose that the migration of a domain wall can be the mechanism that mediates the ferroelectric switching. By employing $ab~initio$ methods, such a mechanism is characterized by an energy barrier of $66.8$ meV/nm$^2$, in a suitable range for retention and switchability. In accompanying the domain wall migration, the net Rashba effect is tunable, as it is a result of competition between layers with opposite electric polarization. These results shed light on the ferroelectric switching mechanism in GeTe, paving stones for the design of potential GeTe-based devices. ",https://doi.org/10.1021/acsaelm.4c00392,2404.13293v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Annealed adaptive importance sampling method in PINNs for solving high   dimensional partial differential equations,1970,"  Physics-informed neural networks (PINNs) have emerged as powerful tools for solving a wide range of partial differential equations (PDEs). However, despite their user-friendly interface and broad applicability, PINNs encounter challenges in accurately resolving PDEs, especially when dealing with singular cases that may lead to unsatisfactory local minima. To address these challenges and improve solution accuracy, we propose an innovative approach called Annealed Adaptive Importance Sampling (AAIS) for computing the discretized PDE residuals of the cost functions, inspired by the Expectation Maximization algorithm used in finite mixtures to mimic target density. Our objective is to approximate discretized PDE residuals by strategically sampling additional points in regions with elevated residuals, thus enhancing the effectiveness and accuracy of PINNs. Implemented together with a straightforward resampling strategy within PINNs, our AAIS algorithm demonstrates significant improvements in efficiency across a range of tested PDEs, even with limited training datasets. Moreover, our proposed AAIS-PINN method shows promising capabilities in solving high-dimensional singular PDEs. The adaptive sampling framework introduced here can be integrated into various PINN frameworks. ",Kein DOI-Link verfügbar,2405.03433v1,Yes,"innovative(1), strategically(1)"
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",VIVA: A Benchmark for Vision-Grounded Decision-Making with Human Values,1970,"  This paper introduces VIVA, a benchmark for VIsion-grounded decision-making driven by human VAlues. While most large vision-language models (VLMs) focus on physical-level skills, our work is the first to examine their multimodal capabilities in leveraging human values to make decisions under a vision-depicted situation. VIVA contains 1,062 images depicting diverse real-world situations and the manually annotated decisions grounded in them. Given an image there, the model should select the most appropriate action to address the situation and provide the relevant human values and reason underlying the decision. Extensive experiments based on VIVA show the limitation of VLMs in using human values to make multimodal decisions. Further analyses indicate the potential benefits of exploiting action consequences and predicted human values. ",Kein DOI-Link verfügbar,2407.03000v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China","Activity-Induced Stiffness, Entanglement Network and Dynamic Slowdown in   Unentangled Semidilute Polymer Solutions",1970,"  Active polymers possess numerous unique properties that are quite different from those observed in the system of small active molecule due to the intricate interplay between their activity and topological constraints. This study focuses on the conformational changes induced by activity, impacting effective stiffness and crucially influencing entanglement and dynamics. When the two terminals of a linear chain undergo active modification through coupling to a high-temperature thermal bath, there is a substantial increase in chain size, indicating a notable enhancement in effective stiffness. Unlike in passive semiflexible chains where stiffness predominantly affects local bond angles, activity-induced stiffness manifests at the scale of tens of monomers. While activity raises the ambient temperature, it significantly decreases diffusion by over an order of magnitude. The slowdown of dynamics observed can be attributed to increased entanglement due to chain elongation. ",https://doi.org/10.1039/D4SM00341A,2407.05066v1,Yes,"intricate(1), notable(1)"
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Exploring the Anomalous Top-Higgs FCNC Couplings at the electron proton   colliders,1970,"  We perform an updated analysis on the searches for the anomalous FCNC Yukawa interactions between the top quark, the Higgs boson, and either an up or charm quark ($\rm tqh,\ q=u,\ c$). We probe the observability of the FCNC top-Higgs couplings through the processes $\rm e^- p\rightarrow \nu_e \bar{t} \rightarrow \nu_e h \bar{q}$ (signal.I) and $\rm \ e^- p \to \nu_e h b$ (singal.II) at the proposed electron proton (ep) colliders, where the Higgs boson decays to a $\rm b\bar{b}$ pair. We find that at the high luminosity (1 $\rm ab^{-1}$) ep colliders where the electrons have a polarisation of $\rm 80\%$ and electron energy is typical 60 GeV, the 2$\sigma$ upper limit on $\rm Br(t\to uh)$ are $0.15\times 10^{-2}$($2.9\times 10^{-4}$) at the 7TeV@LHeC(50TeV@FCC-eh) for signal.I and $0.15\times 10^{-2}$($2.2\times 10^{-4}$) for signal.II. We also give an estimate on how the sensitivity (take signal.I as examples) would change when we reduce the electron beam energy from 60 GeV to 50 GeV or even 40 GeV due to the cost reason. The conclusion is that the discovery potential reduce $8.7\%$($29.4\%$) if the electron beam change from 60GeV to 50(40) GeV at the 7TeV LHeC, and $16.8\%$($19.8\%$) at the 50 TeV FCC-eh. ",https://doi.org/10.1140/epjc/s10052-018-5761-9,1602.04670v4,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Efficient implementation of immersed boundary-lattice Boltzmann method   for massive particle-laden flows Part I: Serial computing,1970,"  Immersed boundary-lattice Boltzmann method (IB-LBM) has been widely used for simulation of particle-laden flows recently. However, it was limited to small-scale simulations with no more than O(103) particles. Here, we expand IB-LBM for massive particle-laden flows with more than O(104) particles by two sequential works. First is the Part I: serial computing on a single CPU core and following the Part II: parallel computing on many CPU cores. In this Part I paper, a highly efficient and localized implementation of IB-LBM is proposed for serial computing. We optimize in three main aspects: swap algorithm for incompressible LBM, local grid-to-point algorithm for IBM and improved grid search algorithm for particle pair short-range interaction. In addition, symmetry algorithm is proposed for the half-calculation of LB collision and external force term. The computational performance on a single CPU core is analyzed. Different scales of two dimensional (2D) and three-dimensional (3D) particles settling in closed cavities are used for testing. The solid volume fraction is varied from 0 to 0.40. Simulation results demonstrate that all calculation parts are dramatically decreased by the improved algorithm. For the particle-free flows, the Mega Lattice Site Update per Second (MLUPS) can be achieved up to 36 (2D) and 12 (3D) using the improved algorithm. For the particle-laden flows, MLUPS can be achieved no lower than 15 (2D) and 7 (3D) in the simulations of dense flows. At last, we discuss the potential of the new algorithms for the high-performance computation of the large-scale systems of particle-laden flows with MPI parallel technique. ",Kein DOI-Link verfügbar,2002.08855v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Multiscale modeling of glioma pseudopalisades: contributions from the   tumor microenvironment,1970,"  Gliomas are primary brain tumors with a high invasive potential and infiltrative spread. Among them, glioblastoma multiforme (GBM) exhibits microvascular hyperplasia and pronounced necrosis triggered by hypoxia. Histological samples showing garland-like hypercellular structures (so-called pseudopalisades) centered around the occlusion site of a capillary are typical for GBM and hint on poor prognosis of patient survival. We propose a multiscale modeling approach in the kinetic theory of active particles framework and deduce by an upscaling process a reaction-diffusion model with repellent pH-taxis. We prove existence of a unique global bounded classical solution for a version of the obtained macroscopic system and investigate the asymptotic behavior of the solution. Moreover, we study two different types of scaling and compare the behavior of the obtained macroscopic PDEs by way of simulations. These show that patterns1 (including pseudopalisades) can be formed for some parameter ranges, in accordance with the tumor grade. This is true when the PDEs are obtained via parabolic scaling (undirected tissue), while no such patterns are observed for the PDEs arising by a hyperbolic limit (directed tissue). This suggests that brain tissue might be undirected - at least as far as glioma migration is concerned. We also investigate two different ways of including cell level descriptions of response to hypoxia and the way they are related. ",Kein DOI-Link verfügbar,2007.05297v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Data-Augmented and Retrieval-Augmented Context Enrichment in Chinese   Media Bias Detection,1970,"  With the increasing pursuit of objective reports, automatically understanding media bias has drawn more attention in recent research. However, most of the previous work examines media bias from Western ideology, such as the left and right in the political spectrum, which is not applicable to Chinese outlets. Based on the previous lexical bias and informational bias structure, we refine it from the Chinese perspective and go one step further to craft data with 7 fine-grained labels. To be specific, we first construct a dataset with Chinese news reports about COVID-19 which is annotated by our newly designed system, and then conduct substantial experiments on it to detect media bias. However, the scale of the annotated data is not enough for the latest deep-learning technology, and the cost of human annotation in media bias, which needs a lot of professional knowledge, is too expensive. Thus, we explore some context enrichment methods to automatically improve these problems. In Data-Augmented Context Enrichment (DACE), we enlarge the training data; while in Retrieval-Augmented Context Enrichment (RACE), we improve information retrieval methods to select valuable information and integrate it into our models to better understand bias. Extensive experiments are conducted on both our dataset and an English dataset BASIL. Our results show that both methods outperform our baselines, while the RACE methods are more efficient and have more potential. ",Kein DOI-Link verfügbar,2311.01372v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Chain of Thought with Explicit Evidence Reasoning for Few-shot Relation   Extraction,1970,"  Few-shot relation extraction involves identifying the type of relationship between two specific entities within a text, using a limited number of annotated samples. A variety of solutions to this problem have emerged by applying meta-learning and neural graph techniques which typically necessitate a training process for adaptation. Recently, the strategy of in-context learning has been demonstrating notable results without the need of training. Few studies have already utilized in-context learning for zero-shot information extraction. Unfortunately, the evidence for inference is either not considered or implicitly modeled during the construction of chain-of-thought prompts. In this paper, we propose a novel approach for few-shot relation extraction using large language models, named CoT-ER, chain-of-thought with explicit evidence reasoning. In particular, CoT-ER first induces large language models to generate evidences using task-specific and concept-level knowledge. Then these evidences are explicitly incorporated into chain-of-thought prompting for relation extraction. Experimental results demonstrate that our CoT-ER approach (with 0% training data) achieves competitive performance compared to the fully-supervised (with 100% training data) state-of-the-art approach on the FewRel1.0 and FewRel2.0 datasets. ",Kein DOI-Link verfügbar,2311.05922v3,Yes,notable(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Hole-phonon interactions in quantum dots: Effects of phonon confinement   and encapsulation materials on spin-orbit qubits,1970,"  Spin-phonon interactions are one of the mechanisms limiting the lifetime of spin qubits made in semiconductor quantum dots. At variance with other mechanisms such as charge noise, phonons are intrinsic to the device and can hardly be mitigated. They set, therefore fundamental limits to the relaxation time of the qubits. Here we introduce a general framework for the calculation of the spin (and charge) transition rates induced by bulk (3D) and strongly confined 1D or 2D phonons. We discuss the particular case of hole spin-orbit qubits described by the 6 bands kp model. We next apply this theory to a hole qubit in a silicon-on-insulator device. We show that spin relaxation in this device is dominated by a band mixing term that couples the holes to transverse acoustic phonons through the valence band deformation potential d, and optimize the bias point and magnetic field orientation to maximize the number of Rabi oscillations Q that can be achieved within on relaxation time T1. Despite the strong spin-orbit coupling in the valence band, the phonon-limited Q can reach a few tens of thousands. We next explore the effects of phonon confinement in 1D and 2D structures, and the impact of the encapsulation materials on the relaxation rates. We show that the spin lifetimes can depend on the structure of the device over micrometer-long length scales and that they improve when the materials around the qubit get harder. Phonon engineering in semiconductor qubits may therefore become relevant once the extrinsic sources of relaxation have been reduced. ",https://doi.org/10.1103/PhysRevB.102.075415,2003.07592v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",AutoRepo: A general framework for multi-modal LLM-based automated   construction reporting,1970,"  Ensuring the safety, quality, and timely completion of construction projects is paramount, with construction inspections serving as a vital instrument towards these goals. Nevertheless, the predominantly manual approach of present-day inspections frequently results in inefficiencies and inadequate information management. Such methods often fall short of providing holistic, exhaustive assessments, consequently engendering regulatory oversights and potential safety hazards. To address this issue, this paper presents a novel framework named AutoRepo for automated generation of construction inspection reports. The unmanned vehicles efficiently perform construction inspections and collect scene information, while the multimodal large language models (LLMs) are leveraged to automatically generate the inspection reports. The framework was applied and tested on a real-world construction site, demonstrating its potential to expedite the inspection process, significantly reduce resource allocation, and produce high-quality, regulatory standard-compliant inspection reports. This research thus underscores the immense potential of multimodal large language models in revolutionizing construction inspection practices, signaling a significant leap forward towards a more efficient and safer construction management paradigm. ",Kein DOI-Link verfügbar,2310.07944v2,Yes,potent(3)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",VIALM: A Survey and Benchmark of Visually Impaired Assistance with Large   Models,1970,"  Visually Impaired Assistance (VIA) aims to automatically help the visually impaired (VI) handle daily activities. The advancement of VIA primarily depends on developments in Computer Vision (CV) and Natural Language Processing (NLP), both of which exhibit cutting-edge paradigms with large models (LMs). Furthermore, LMs have shown exceptional multimodal abilities to tackle challenging physically-grounded tasks such as embodied robots. To investigate the potential and limitations of state-of-the-art (SOTA) LMs' capabilities in VIA applications, we present an extensive study for the task of VIA with LMs (VIALM). In this task, given an image illustrating the physical environments and a linguistic request from a VI user, VIALM aims to output step-by-step guidance to assist the VI user in fulfilling the request grounded in the environment. The study consists of a survey reviewing recent LM research and benchmark experiments examining selected LMs' capabilities in VIA. The results indicate that while LMs can potentially benefit VIA, their output cannot be well environment-grounded (i.e., 25.7% GPT-4's responses) and lacks fine-grained guidance (i.e., 32.1% GPT-4's responses). ",Kein DOI-Link verfügbar,2402.01735v2,Yes,potent(2)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Quantum control and quantum speed limits in supersymmetric potentials,1970,"  Supersymmetry allows one to build a hierarchy of Hamiltonians that share the same spectral properties and which are pairwise connected through common superpotentials. The iso-spectral properties of these Hamiltonians imply that the dynamics and therefore control of different eigenstates are connected through supersymmetric intertwining relations. In this work we explore how this enables one to study general dynamics, shortcuts to adiabaticity (STA) and quantum speed limits for distinct states of different supersymmetric partner potentials by using the infinite box as an example. ",https://doi.org/10.1088/1367-2630/ac89a4,2206.13020v1,Yes,potent(2)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Cracking the Code of Juxtaposition: Can AI Models Understand the   Humorous Contradictions,1970,"  Recent advancements in large multimodal language models have demonstrated remarkable proficiency across a wide range of tasks. Yet, these models still struggle with understanding the nuances of human humor through juxtaposition, particularly when it involves nonlinear narratives that underpin many jokes and humor cues. This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction. We introduce the YesBut benchmark, which comprises tasks of varying difficulty aimed at assessing AI's capabilities in recognizing and interpreting these comics, ranging from literal content comprehension to deep narrative reasoning. Through extensive experimentation and analysis of recent commercial or open-sourced large (vision) language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics. Our results show that even state-of-the-art models still lag behind human performance on this task. Our findings offer insights into the current limitations and potential improvements for AI in understanding human creative expressions. ",Kein DOI-Link verfügbar,2405.19088v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Ground-state correlation energy of beryllium dimer by the Bethe-Salpeter   equation,1970,"  Since the '30s the interatomic potential of the beryllium dimer Be$_2$ has been both an experimental and a theoretical challenge. Calculating the ground-state correlation energy of Be$_2$ along its dissociation path is a difficult problem for theory. We present ab initio many-body perturbation theory calculations of the Be$_2$ interatomic potential using the GW approximation and the Bethe-Salpeter equation (BSE). The ground-state correlation energy is calculated by the trace formula with checks against the adiabatic-connection fluctuation-dissipation theorem formula. We show that inclusion of GW corrections already improves the energy even at the level of the random-phase approximation. At the level of the BSE on top of the GW approximation, our calculation is in surprising agreement with the most accurate theories and with experiment. It even reproduces an experimentally observed flattening of the interatomic potential due to a delicate correlations balance from a competition between covalent and van der Waals bonding. ",https://doi.org/10.21468/SciPostPhys.8.2.020,1812.00932v4,Yes,potent(3)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Cross-Media Keyphrase Prediction: A Unified Framework with   Multi-Modality Multi-Head Attention and Image Wordings,1970,"  Social media produces large amounts of contents every day. To help users quickly capture what they need, keyphrase prediction is receiving a growing attention. Nevertheless, most prior efforts focus on text modeling, largely ignoring the rich features embedded in the matching images. In this work, we explore the joint effects of texts and images in predicting the keyphrases for a multimedia post. To better align social media style texts and images, we propose: (1) a novel Multi-Modality Multi-Head Attention (M3H-Att) to capture the intricate cross-media interactions; (2) image wordings, in forms of optical characters and image attributes, to bridge the two modalities. Moreover, we design a unified framework to leverage the outputs of keyphrase classification and generation and couple their advantages. Extensive experiments on a large-scale dataset newly collected from Twitter show that our model significantly outperforms the previous state of the art based on traditional attention networks. Further analyses show that our multi-head attention is able to attend information from various aspects and boost classification or generation in diverse scenarios. ",Kein DOI-Link verfügbar,2011.01565v1,Yes,intricate(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Accurate description of charged excitations in molecular solids from   embedded many-body perturbation theory,1970,"  We present a novel hybrid quantum/classical (QM/MM) approach to the calculation of charged excitations in molecular solids based on the many-body Green's function $GW$ formalism. Molecules described at the $GW$ level are embedded into the crystalline environment modeled with an accurate classical polarizable scheme. This allows the calculation of electron addition and removal energies in the bulk and at crystal surfaces where charged excitations are probed in photoelectron experiments. By considering the paradigmatic case of pentacene and perfluoropentacene crystals, we discuss the different contributions from intermolecular interactions to electronic energy levels, distinguishing between polarization, which is accounted for combining quantum and classical polarizabilities, and crystal field effects, that can impact energy levels by up to $\pm0.6$ eV. After introducing band dispersion, we achieve quantitative agreement (within 0.2 eV) on the ionization potential and electron affinity measured at pentacene and perfluoropentacene crystal surfaces characterized by standing molecules. ",https://doi.org/10.1103/PhysRevB.97.035108,1801.01755v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",A Feshbach engine in the Thomas-Fermi regime,1970,"  Bose-Einstein condensates can be used to produce work by tuning the strength of the interparticle interactions with the help of Feshbach resonances. In inhomogeneous potentials, these interaction ramps change the volume of the trapped gas allowing one to create a thermodynamic cycle known as the Feshbach engine. However, in order to obtain a large power output, the engine strokes must be performed on a short timescale, which is in contrast with the fact that the efficiency of the engine is reduced by irreversible work if the strokes are done in a non-adiabatic fashion. Here we investigate how such an engine can be run in the Thomas-Fermi regime and present a shortcut to adiabaticity that minimizes the irreversible work and allows for efficient engine operation. ",https://doi.org/10.1103/PhysRevResearch.2.033335,2005.06801v2,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Equivariant graph neural network interatomic potential for Green-Kubo   thermal conductivity in phase change materials,1970,"  Thermal conductivity is a fundamental material property that plays an essential role in technology, but its accurate evaluation presents a challenge for theory. In this work, we demonstrate the application of $E(3)$-equivariant neutral network interatomic potentials within Green-Kubo formalism to determine the lattice thermal conductivity in amorphous and crystalline materials. We apply this method to study the thermal conductivity of germanium telluride (GeTe) as a prototypical phase change material. A single deep learning interatomic potential is able to describe the phase transitions between the amorphous, rhombohedral and cubic phases, with critical temperatures in good agreement with experiments. Furthermore, this approach accurately captures the pronounced anharmonicity that is present in GeTe, enabling precise calculations of the thermal conductivity. In contrast, the Boltzmann transport equation including only three-phonon processes tends to overestimate the thermal conductivity by approximately a factor of 2 in the crystalline phases. ",Kein DOI-Link verfügbar,2307.02327v2,Yes,potent(2)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",The Dimorphos Boulder Swarm,1970,"  We present deep Hubble Space Telescope images taken to examine the ejecta from the DART spacecraft impact into asteroid Dimorphos. The images reveal an extensive population of co-moving boulders, the largest of which is about 7 m in diameter (geometric albedo 0.15 assumed). Measurements of 37 boulders show a mean sky-plane velocity dispersion of 0.30+/-0.03 m/s, only slightly larger than the 0.24 m/s gravitational escape velocity from the Didymos/Dimorphos binary system. The total boulder mass, 5e6 kg (density 2200 kg/m3 assumed), corresponds to about 0.1 percent of the mass of Dimorphos and the boulders collectively carry about 3e-5 of the kinetic energy delivered by the DART spacecraft impact. The sky-plane distribution of the boulders is asymmetric, consistent with impact into an inhomogeneous, likely rubble-pile, body. Surface boulder counts on Didymos show that the observed boulder swarm could be ejected from as little as 2 percent of the surface of Dimorphos (for example a circular crater at the impact point about 50 m in diameter). The large, slow-moving boulders are potential targets to be investigated in-situ by the upcoming ESA HERA mission. ",https://doi.org/10.3847/2041-8213/ace1ec,2307.12506v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Cellular uptake of active nonspherical nanoparticles,1970,"  Due to the potential applications in biomedical engineering, it becomes more and more important to understand the process of engulfment and internalization of nanoparticles (NPs) by cell membranes. Despite the fact that the interaction between cell membranes and passive particles has been widely studied, the interaction between cell membranes and self-propelled nonspherical NPs remains to be elucidated. Here we present a theoretical model to systematically investigate the influence of the active force, aspect ratio of NPs, particle size and membrane properties (adhesion energy density and membrane tension) on the cellular uptake of a nonspherical nanoparticle. It is found that the active force generated by an NP can trigger a type of first-order wrapping transition from a small partial wrapping state to a large one. In addition, the phase diagram in the force-aspect ratio (particle size, adhesion energy density and membrane tension) space displays more complex behaviors compared with that for the passive wrapping mediated merely by adhesion. These results may provide a useful guidance to the study of activity-driven cellular entry of active particles into cells. ",Kein DOI-Link verfügbar,2312.05782v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",A Novel Hybrid Ordinal Learning Model with Health Care Application,1970,"  Ordinal learning (OL) is a type of machine learning models with broad utility in health care applications such as diagnosis of different grades of a disease (e.g., mild, modest, severe) and prediction of the speed of disease progression (e.g., very fast, fast, moderate, slow). This paper aims to tackle a situation when precisely labeled samples are limited in the training set due to cost or availability constraints, whereas there could be an abundance of samples with imprecise labels. We focus on imprecise labels that are intervals, i.e., one can know that a sample belongs to an interval of labels but cannot know which unique label it has. This situation is quite common in health care datasets due to limitations of the diagnostic instrument, sparse clinical visits, or/and patient dropout. Limited research has been done to develop OL models with imprecise/interval labels. We propose a new Hybrid Ordinal Learner (HOL) to integrate samples with both precise and interval labels to train a robust OL model. We also develop a tractable and efficient optimization algorithm to solve the HOL formulation. We compare HOL with several recently developed OL methods on four benchmarking datasets, which demonstrate the superior performance of HOL. Finally, we apply HOL to a real-world dataset for predicting the speed of progressing to Alzheimer's Disease (AD) for individuals with Mild Cognitive Impairment (MCI) based on a combination of multi-modality neuroimaging and demographic/clinical datasets. HOL achieves high accuracy in the prediction and outperforms existing methods. The capability of accurately predicting the speed of progression to AD for each individual with MCI has the potential for helping facilitate more individually-optimized interventional strategies. ",Kein DOI-Link verfügbar,2312.09540v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",GPM: A Generic Probabilistic Model to Recover Annotator's Behavior and   Ground Truth Labeling,1970,"  In the big data era, data labeling can be obtained through crowdsourcing. Nevertheless, the obtained labels are generally noisy, unreliable or even adversarial. In this paper, we propose a probabilistic graphical annotation model to infer the underlying ground truth and annotator's behavior. To accommodate both discrete and continuous application scenarios (e.g., classifying scenes vs. rating videos on a Likert scale), the underlying ground truth is considered following a distribution rather than a single value. In this way, the reliable but potentially divergent opinions from ""good"" annotators can be recovered. The proposed model is able to identify whether an annotator has worked diligently towards the task during the labeling procedure, which could be used for further selection of qualified annotators. Our model has been tested on both simulated data and real-world data, where it always shows superior performance than the other state-of-the-art models in terms of accuracy and robustness. ",Kein DOI-Link verfügbar,2003.00475v1,Yes,potent(1)
0000-0002-9598-7420,Jing Li,"University of Nottingham, University of Nottingham Ningbo China",Predictability of road traffic and congestion in urban areas,1970,"  Mitigating traffic congestion on urban roads, with paramount importance in urban development and reduction of energy consumption and air pollution, depends on our ability to foresee road usage and traffic conditions pertaining to the collective behavior of drivers, raising a significant question: to what degree is road traffic predictable in urban areas? Here we rely on the precise records of daily vehicle mobility based on GPS positioning device installed in taxis to uncover the potential daily predictability of urban traffic patterns. Using the mapping from the degree of congestion on roads into a time series of symbols and measuring its entropy, we find a relatively high daily predictability of traffic conditions despite the absence of any a priori knowledge of drivers' origins and destinations and quite different travel patterns between weekdays and weekends. Moreover, we find a counterintuitive dependence of the predictability on travel speed: the road segment associated with intermediate average travel speed is most difficult to be predicted. We also explore the possibility of recovering the traffic condition of an inaccessible segment from its adjacent segments with respect to limited observability. The highly predictable traffic patterns in spite of the heterogeneity of drivers' behaviors and the variability of their origins and destinations enables development of accurate predictive models for eventually devising practical strategies to mitigate urban road congestion. ",https://doi.org/10.1371/journal.pone.0121825,1407.1871v1,Yes,potent(1)
0000-0002-3522-8019,Zhixian Yu,"University of Nottingham, University of Nottingham - Ningbo China",On the existence of natural self-oscillation of a free electron,1970,  The possibility of the existence of natural self-oscillation of a free electron is suggested. This oscillation depends on the interaction of the electron with its own electromagnetic fields. Suitable standing wave solutions of the electromagnetic fields are chosen. A kind of displacement dependent electric potential and mechanism of energy exchange between velocity and acceleration dependent electromagnetic fields are analyzed. Conditions for the existence of natural self-oscillation are given. ,Kein DOI-Link verfügbar,1201.2224v3,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Velocity renormalization of nodal quasiparticles in d-wave   superconductors,1970,"  Gapless nodal quasiparticles emerge at a low-energy regime of high-$T_c$ cuprate superconductors due to the $d_{x^2 - y^2}$ gap symmetry. We study the unusual renormalizations of the Fermi velocity $v_F$ and gap velocity $v_{\Delta}$ of these quasiparticles close to various quantum critical points in a superconducting dome. Special attention is paid to the behavior of the velocity ratio, $v_{\Delta}/v_F$, since it determines a number of observable quantities. We perform a renormalization-group analysis and show that the velocity ratio may vanish, approach unity, or diverge at different quantum critical points. The corresponding superfluid densities and critical temperatures are suppressed, slightly increased, or significantly enhanced. The effects of three types of static disorders, namely, random mass, random gauge potential, and random chemical potential, on the stability of the system are also addressed. An analogous analysis reveals that both random mass and random gauge potential are irrelevant. This implies that these fixed points of the velocity ratio are stable, and hence observable effects ignited by them are unchanged. However, the random chemical potential is marginal. As a result, these fixed points are broken, and thus, the instabilities of quantum phase transitions are triggered. ",https://doi.org/10.1103/PhysRevB.87.054511,1302.6137v1,Yes,potent(4)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Two-loop disorder effects on the nematic quantum criticality in $d$-wave   superconductors,1970,"  The gapless nodal fermions exhibit non-Fermi liquid behaviors at the nematic quantum critical point that is supposed to exist in some $d$-wave cuprate superconductors. This non-Fermi liquid state may be turned into a disorder-dominated diffusive metal if the fermions also couple to a disordered potential that generates a relevant perturbation in the sense of renormalization group theory. It is therefore necessary to examine whether a specific disorder is relevant or not. We study the interplay between critical nematic fluctuation and random chemical potential by performing renormalization group analysis. The parameter that characterizes the strength of random chemical potential is marginal at the one-loop level, but becomes marginally relevant after including the two-loop corrections. Thus even weak random chemical potential leads to diffusive motion of nodal fermions and the significantly critical behaviors of physical implications, since the strength flows eventually to large values at low energies. ",https://doi.org/10.1016/j.physleta.2015.05.028,1505.06533v1,Yes,potent(4)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Quantum Corrections to Gravitational Potential of Scalarized Neutron   Star Binary,1970,"  We investigate the long-distance, low-energy, leading quantum corrections to gravitational potential for scalarized neutron star (NS) binary systems, by treating general relativity as an effective field theory. We neglect the extended scales of two star components and treat them as heavy point particles, which gravitationally interact with each other via the exchanges of both gravitons and scalar particles, because of the settled scalar configurations inside the stars. Accordingly, the gravitational potential includes both Newtonian potential and scalar-modified Newtonian-like part. We, in the non-relativistic limit, calculate the non-analytic corrections to the modified gravitational potential directly from the sum of all exchanges of both gravitons and scalar particles to one-loop order. The appropriate vertex rules are extracted from the effective Lagrangian. Our calculations demonstrate that either the graviton exchanges or the exchanges of scalar particles contribute to both classical relativistic corrections and quantum corrections to the gravitational potential of the scalarized NS binaries. ",https://doi.org/10.1140/epjc/s10052-019-7052-5,1909.01047v1,Yes,potent(5)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Physical Environment of Accreting Neutron Stars,1970,"  Neutron stars (NSs) powered by accretion, which are known as accretion-powered NSs, always locate in binary systems and manifest themselves as X-ray sources. Physical process during accreting material from their companions is a challenging and appealing topic, because of the strong magnetic field of NSs. In this article, we review the physical process of accretion onto magnetized NS in X-ray binary systems. We, firstly, give an introduction to accretion-powered NSs and review the accretion mechanism in X-ray binaries. This review will be mostly focused on accretion-induced evolution of NSs. Finally, we extend the accretion to cosmic scheme and discuss that the NS accretion can be a potential promising tool to constrain dark matter candidates and investigate the nature of dark energy. ",https://doi.org/10.1155/2016/3424565,1909.01057v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Incommensurate magnetic states induced by ordering competition in   $\mathrm{Ba_{1-x}Na_xFe_2As_2}$,1970,"  Quantum criticality nearby a certain magnetic phase transition beneath the superconducting dome of $\mathrm{Ba_{1-x}Na_xFe_2As_2}$ is attentively studied by virtue of a phenomenological theory in conjunction with renormalization group approach. We report that ordering competition between magnetic and superconducting fluctuations is capable of coaxing incommensurate (IC) magnetic states to experience distinct fates depending upon their spin configurations. The $C_2$-symmetry IC magnetic stripe with perpendicular magnetic helix dominates over other $C_2$-symmetry magnetic competitors and hints at a potential candidate for the unknown $C_2$-symmetry magnetic state. Amongst $C_4$-symmetry IC magnetic phases, IC charge spin density wave is substantiated to be superior, shedding light on the significant intertwining of charge and spin degrees of freedom. Meanwhile, ferocious fluctuations render a sharp fall of superfluid density alongside dip of critical temperature as well as intriguing behavior of London penetration depth. ",https://doi.org/10.1088/1361-6668/ac9a86,2007.14981v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Gravitational Higgs mechanism in inspiraling scalarized NS-WD binary,1970,"  We investigate the gravitational Higgs mechanism in the inspiraling scalarized neutron star - white dwarf (NS-WD) binaries, whose dynamics are described by the scalar-tensor theory. Because of the difference in binding energy of NS and WD, the orbital decay of scalarized NS-WD system actually sources an emission of dipolar gravitational scalar radiation, in addition to the tensor gravitational waves, which breaks the Lorentz invariance constructed in the framework of general relativity. The resulted gravitational scalar radiation field obtains a scalar-energy-density-dependent effective mass, arising from a gravitational scalar potential that consists of a monotonically decreasing self-interactions of gravitational scalar field and an increasing exponential coupling between the scalar field and the NS/WD matter. Owing to a thin-ring-orbit effect, the gravitational interactions encoded by the massive scalar field is screened in the region of binary orbit, with high density of stars' scalar energy, which gives us the estimation for scalar masses of about $10^{-21} eV/c^2$ and leads to a Yulkawa-like correction to the Newtonian potential of the binary system. We demonstrate that the radiated gravitational tensor waves, propagating in the Yukawa type of potential, gain a scalar-background-dependent mass term of the order of $\sim 10^{-23} eV/c^2$. ",https://doi.org/10.4236/ijaa.2017.73016,1612.06102v3,Yes,potent(3)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Graviton mass generation in in-spiraling DNS,1970,"  We point out that a spontaneous scalarized inspiring double neutron star (DNS) system can provide us a natural laboratory to investigate the generation mechanism of masses for gravitons. Because of the appearance of a gravitational scalar background field, with small fluctuations, converged by iterative interplay of the mass dimensional external scalar fields, the binary system suffers from a spontaneous Lorentz symmetry breaking. The two scalarized NSs dip in a Higgs-like gravitational scalar potential, where the massless scalar background fluctuation field plays the role of Higgs field. Consequently, the gravitational scalar background field becomes massive. The radiated gravitons, propagating in a Yukawa-corrected potential, acquire a scalar-background-dependent mass term, in a massive-scalar-field-mediated way. We demonstrate that the mass of gravitons depends on intrinsic properties of the sources, which is not a certain value. The background-dependent masses for gravitons from scalarized orbital shrinking DNS is variable with the compactness of two components, as well as the separation of the binary. We get the effective masses for gravitons radiated from 8 detected DNS binaries with more precise mass measurements in our galaxy, whose values appear to be of the order of $10^{-23} ev/c^2$. It is found that more massive gravitons radiate from more closer DNS system, consisting with the higher-frequency gravitational waves from closer binaries. ",https://doi.org/10.21474/IJAR01/5808,1612.06104v2,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Massive scalar counterpart of gravitational waves in scalarized neutron   star binaries,1970,"  In analogy with spontaneous magnetization of ferromagnets below the Curie temperature, a neutron star (NS), with a compactness above a certain critical value, may undergo spontaneous scalarization and exhibit an interior nontrivial scalar configuration. Consequently, the exterior space-time is changed, and an external scalar field appears, which subsequently triggers a scalarization of its companion. The dynamical interplay produces a gravitational scalar counterpart of tensor gravitational waves. In this paper, we resort to scalar-tensor theory and demonstrate that the gravitational scalar counterpart from double neutron star (DNS) and neutron star-white dwarf (NS-WD) become massive. We report that (i) a gravitational scalar background field, arising from convergence of external scalar fields, plays the role of gravitational scalar counterpart in scalarized DNS binary, and the appearance of a mass-dimensional constant in Higgs-like gravitational scalar potential is responsible for a massive gravitational scalar counterpart with mass of order of Planck scale; (ii) a dipolar gravitational scalar radiated field, resulting from different binding energy of NS and WD, plays the role of gravitational scalar counterpart in scalarized orbital shrinking NS-WDs, which oscillates around a local and scalar-energy-density dependent minimum of the gravitational scalar potential and gains a mass of the order of about $10^{-21} ev/c^2$. ",https://doi.org/10.1140/epjc/s10052-017-5214-x,1909.01045v1,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Inspiraling Corrugation-Induced Quantum Effects on Neutron Star Binary   Plane,1970,"  We use the path-integral formula and investigate some dynamical quantum effects induced by the inspiraling lateral corrugation of orbital plane in gravitationally bound neutron star (NS) binaries, with orbital separation of $10^9$ m. Based on Dewitt's approach, we calculate the gravitational Casimir energy cost of the binary plane, which consists of statically gravitational effects and deformation-induced effects. It is found that the static effects include a term coming from the self-gravity of the orbital plane and the contribution of Newtonian gravitational potential of the binary system. While the deformation-induced effect also results from two parts, i.e. the instability of orbital binding energy, scaling as $\frac{1}{(R-r)^2}$, and the dynamically Casimir energy cost of the orbital binding energy, decaying as $\frac{1}{(R-r)^4}$. The dynamically gravitational Casimir phenomena and the corresponding energy cost modify the spiral-in orbital motion of the binary and thus the frequency of released gravitational waves (GWs). We consider the mechanical response of two NS components and qualitatively study the corrections to the orbital motion of the system and the GW frequencies. It is found that the dynamical Casimir effects exert a dissipative force on the binary plane, depending on the frequency of GWs. The resultant dissipation may enhance with the decaying separation and increasing GW frequencies, which subsequently accelerates the orbital decay of the binary. However, the dissipation rate just has an order of $10^{-70}$ eV/s. So the corrections to the dynamics of NS binaries are very marginal, by considering the wide separation, the cosmological coalescence time, and low-frequency GWs of the system. ",https://doi.org/10.1016/j.physletb.2022.136980,2111.07230v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",A Guide to Global Quantum Key Distribution Networks,1970,"  We describe systems and methods for the deployment of global quantum key distribution (QKD) networks covering transoceanic, long-haul, metro, and access segments of the network. A comparative study of the state-of-the-art QKD technologies is carried out, including both terrestrial QKD via optical fibers and free-space optics, as well as spaceborne solutions via satellites. We compare the pros and cons of various existing QKD technologies, including channel loss, potential interference, distance, connection topology, deployment cost and requirements, as well as application scenarios. Technical selection criteria and deployment requirements are developed for various different QKD solutions in each segment of networks. For example, optical fiber-based QKD is suitable for access networks due to its limited distance and compatibility with point-to-multipoint (P2MP) topology; with the help of trusted relays, it can be extended to long-haul and metro networks. Spaceborne QKD on the other hand, has much smaller channel loss and extended transmission distance, which can be used for transoceanic and long-haul networks exploiting satellite-based trusted relays. ",Kein DOI-Link verfügbar,2012.14396v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Exploring Text-based Realistic Building Facades Editing Applicaiton,1970,"  This paper explores the utilization of diffusion models and textual guidance for achieving localized editing of building facades, addressing the escalating demand for sophisticated editing methodologies in architectural design and urban planning. Leveraging the robust generative capabilities of diffusion models, this study presents a promising avenue for realistically synthesizing and modifying architectural facades. Through iterative diffusion and text descriptions, these models adeptly capture both the intricate global and local structures inherent in architectural facades, thus effectively navigating the complexity of such designs. Additionally, the paper examines the expansive potential of diffusion models in various facets, including the generation of novel facade designs, the enhancement of existing facades, and the realization of personalized customization. Despite their promise, diffusion models encounter obstacles such as computational resource constraints and data imbalances. To address these challenges, the study introduces the innovative Blended Latent Diffusion method for architectural facade editing, accompanied by a comprehensive visual analysis of its viability and efficacy. Through these endeavors, we aims to propel forward the field of architectural facade editing, contributing to its advancement and practical application. ",Kein DOI-Link verfügbar,2405.02967v1,Yes,"innovative(1), intricate(1), potent(1)"
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China","Dendrite Net: A White-Box Module for Classification, Regression, and   System Identification",1970,"  The simulation of biological dendrite computations is vital for the development of artificial intelligence (AI). This paper presents a basic machine learning algorithm, named Dendrite Net or DD, just like Support Vector Machine (SVM) or Multilayer Perceptron (MLP). DD's main concept is that the algorithm can recognize this class after learning, if the output's logical expression contains the corresponding class's logical relationship among inputs (and$\backslash$or$\backslash$not). Experiments and main results: DD, a white-box machine learning algorithm, showed excellent system identification performance for the black-box system. Secondly, it was verified by nine real-world applications that DD brought better generalization capability relative to MLP architecture that imitated neurons' cell body (Cell body Net) for regression. Thirdly, by MNIST and FASHION-MNIST datasets, it was verified that DD showed higher testing accuracy under greater training loss than Cell body Net for classification. The number of modules can effectively adjust DD's logical expression capacity, which avoids over-fitting and makes it easy to get a model with outstanding generalization capability. Finally, repeated experiments in MATLAB and PyTorch (Python) demonstrated that DD was faster than Cell body Net both in epoch and forward-propagation. The main contribution of this paper is the basic machine learning algorithm (DD) with a white-box attribute, controllable precision for better generalization capability, and lower computational complexity. Not only can DD be used for generalized engineering, but DD has vast development potential as a module for deep learning. DD code is available at GitHub: https://github.com/liugang1234567/Gang-neuron . ",Kein DOI-Link verfügbar,2004.03955v6,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",nnSAM: Plug-and-play Segment Anything Model Improves nnUNet Performance,1970,"  Automatic segmentation of medical images is crucial in modern clinical workflows. The Segment Anything Model (SAM) has emerged as a versatile tool for image segmentation without specific domain training, but it requires human prompts and may have limitations in specific domains. Traditional models like nnUNet perform automatic segmentation during inference and are effective in specific domains but need extensive domain-specific training. To combine the strengths of foundational and domain-specific models, we propose nnSAM, integrating SAM's robust feature extraction with nnUNet's automatic configuration to enhance segmentation accuracy on small datasets. Our nnSAM model optimizes two main approaches: leveraging SAM's feature extraction and nnUNet's domain-specific adaptation, and incorporating a boundary shape supervision loss function based on level set functions and curvature calculations to learn anatomical shape priors from limited data. We evaluated nnSAM on four segmentation tasks: brain white matter, liver, lung, and heart segmentation. Our method outperformed others, achieving the highest DICE score of 82.77% and the lowest ASD of 1.14 mm in brain white matter segmentation with 20 training samples, compared to nnUNet's DICE score of 79.25% and ASD of 1.36 mm. A sample size study highlighted nnSAM's advantage with fewer training samples. Our results demonstrate significant improvements in segmentation performance with nnSAM, showcasing its potential for small-sample learning in medical image segmentation. ",Kein DOI-Link verfügbar,2309.16967v3,Yes,"versatile(1), potent(1)"
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Very rare events for diffusion processes in short time,1970,"  We study the large deviation estimates for the short time asymptotic behavior of a strongly degenerate diffusion process. Assuming a nilpotent structure of the Lie algebra generated by the driving vector fields, we obtain a graded large deviation principle and prove the existence of those ""very rare events"". In particular the first grade coincides with the classical Large Deviation Principle. ",Kein DOI-Link verfügbar,1901.10025v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Language-guided Few-shot Semantic Segmentation,1970,"  Few-shot learning is a promising way for reducing the label cost in new categories adaptation with the guidance of a small, well labeled support set. But for few-shot semantic segmentation, the pixel-level annotations of support images are still expensive. In this paper, we propose an innovative solution to tackle the challenge of few-shot semantic segmentation using only language information, i.e.image-level text labels. Our approach involves a vision-language-driven mask distillation scheme, which contains a vision-language pretraining (VLP) model and a mask refiner, to generate high quality pseudo-semantic masks from text prompts. We additionally introduce a distributed prototype supervision method and complementary correlation matching module to guide the model in digging precise semantic relations among support and query images. The experiments on two benchmark datasets demonstrate that our method establishes a new baseline for language-guided few-shot semantic segmentation and achieves competitive results to recent vision-guided methods. ",Kein DOI-Link verfügbar,2311.13865v1,Yes,innovative(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Moiré Engineering and Topological Flat Bands in Twisted Orbital-Active   Bilayers,1970,"  Topological flat bands at the Fermi level offer a promising platform to study a variety of intriguing correlated phase of matter. Here we present band engineering in the twisted orbital-active bilayers with spin-orbit coupling. The symmetry constraints on the interlayer coupling that determines the effective potential for low-energy physics of moir\'e electrons are exhaustively derived for two-dimensional point groups. We find the line graph or biparticle sublattice of moir\'e pattern emerge with a minimal $C_3$ symmetry, which exhibit isolated electronic flat bands with nontrivial topology. The band flatness is insensitive to the twist angle since they come from the interference effect. Armed with this guiding principle, we predict that twisted bilayers of 2H-PbS$_2$ and CdS realize the salient physics to engineer two-dimensional topological quantum phases. At small twist angles, PbS$_2$ heterostructures give rise to an emergent moir\'e Kagom\'e lattice, while CdS heterostructures lead to an emergent moir\'e honeycomb lattice, and both of them host moir\'e quantum spin Hall insulators with almost flat topological bands. We further study superconductivity of these two systems with local attractive interactions. The superfluid weight and Berezinskii-Kosterlitz-Thouless temperature are determined by multiband processes and quantum geometry of the band in the flat-band limit when the pairing potential exceeds the band width. Our results demonstrate twisted bilayers with multi-orbitals as a promising tunable platform to realize correlated topological phases. ",Kein DOI-Link verfügbar,2209.06524v1,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Discrete Wilson Lines in N=1 D=4 Type IIB Orientifolds: A Systematic   Exploration for $\IZ_6$ Orientifold,1970,"  We develop techniques to construct general discrete Wilson lines in four-dimensional N=1 Type IIB orientifolds, their T-dual realization corresponds to branes positioned at the orbifold fixed points. The explicit order two and three Wilson lines along with their tadpole consistency conditions are given for D=4 N=1 Z_6 Type IIB orientifold. The systematic search for all models with general order three Wilson lines leads to a small class of inequivalent models. There are only two inequivalent classes of a potentially phenomenologically interesting model that has a possible SU(3)_{color} x SU(2)_L x SU(2)_R x U(1)_{B-L} gauge structure, arising from a set of branes located at the Z_6 orbifold fixed point. We calculate the spectrum and Yukawa couplings for this model. On the other hand, introduction of anti-branes allows for models with three families and realistic gauge group assignment, arising from branes located at the Z_3 orbifold fixed points. ",https://doi.org/10.1016/S0550-3213(00)00669-6,hep-th/0010091v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Magnetic moiré surface states and flat chern band in topological   insulators,1970,"  We theoretically study the effect of magnetic moir\'e superlattice on the topological surface states by introducing a continuum model of Dirac electrons with a single Dirac cone moving in the time-reversal symmetry breaking periodic pontential. The Zeeman-type moir\'e potentials generically gap out the moir\'e surface Dirac cones and give rise to isolated flat Chern minibands with Chern number $\pm1$. This result provides a promising platform for realizing the time-reversal breaking correlated topological phases. In a $C_6$ periodic potential, when the scalar $U_0$ and Zeeman $\Delta_1$ moir\'e potential strengths are equal to each other, we find that energetically the first three bands of $\Gamma$-valley moir\'e surface electrons are non-degenerate and realize i) an $s$-orbital model on a honeycomb lattice, ii) a degenerate $p_x,p_y$-orbitals model on a honeycomb lattice, and iii) a hybridized $sd^2$-orbital model on a kagome lattice, where moir\'e surface Dirac cones in these bands emerge. When $U_0\neq\Delta_1$, the difference between the two moir\'e potential serves as an effective spin-orbit coupling and opens a topological gap in the emergent moir\'e surface Dirac cones. ",https://doi.org/10.1103/PhysRevB.106.035114,2106.01630v1,Yes,potent(4)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Large-Gap Quantum Anomalous Hall Insulators in $A$Ti$X$ Class,1970,"  We theoretically propose that the monolayer $A$Ti$X$ family (KTiSb, KTiBi, RbTiSb, SrTiSn) are potential candidates for large-gap quantum anomalous Hall insulators with high Chern number $\mathcal{C}=2$. Both of the topology and magnetism in these materials are from $3d$-orbitals of Ti. We construct the tight-binding model with symmetry analysis to reveal the origin of topology. Remarkably, quite different from the conventional $s$-$d$ band inversion, here the topological band inversion within $3d$ orbitals is due to the crystal field and electron hopping, while spin-orbit coupling only trivially gaps out the Dirac cone at Fermi level. The general physics from the $3d$ orbitals here applies to a large class of transition metal compounds with the space group $P4/nmm$ or $P$-$42m$ and their subgroups. ",https://doi.org/10.1103/PhysRevB.108.165122,2211.08938v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Effects of Heavy States on the Effective N=1 Supersymmetric Action,1970,"  Using the power of superspace formalism, we investigate the decoupling effects of heavy states in N=1 supersymmetric field theory. We find that ""mixed"" couplings in the superpotential between the heavy and light fields contribute to the effective superpotential at the leading order, and also contribute to the effective K\""{a}hler potential (in the next to leading order). Mixed couplings in the K\""{a}hler potential always contribute to the effective K\""{a}hler potential at the leading order. Several examples are presented which illustrate the effects explicitly. ",https://doi.org/10.1016/S0550-3213(98)00690-7,hep-ph/9807321v1,Yes,potent(5)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Absolutely Continuous Spectrum of Multifrequency Quasiperiodic   Schrödinger operator,1970,"  In this paper, we prove that for any $d$-frequency analytic quasiperiodic Schr\""odinger operator, if the frequency is weak Liouvillean, and the potential is small enough, then the corresponding operator has absolutely continuous spectrum. Moreover, in the case $d=2$, we even establish the existence of ac spectrum under small potential and some super-Liouvillean frequency, and this result is optimal due to a recent counterexample of Avila and Jitomirskaya. ",Kein DOI-Link verfügbar,2004.04409v1,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Absolute continuity of the integrated density of states in the localized   regime,1970,"  We establish the absolute continuity of the integrated density of states (IDS) for quasi-periodic Schr\""odinger operators with a large trigonometric potential and Diophantine frequency. This partially solves Eliasson's open problem in 2002. Furthermore, this result can be extended to a class of quasi-periodic long-range operators on $\ell^2(\Z^d)$. Our proof is based on stratified quantitative almost reducibility results of dual cocycles. Specifically, we prove that a generic analytic one-parameter family of cocycles, sufficiently close to constant coefficients, is reducible except for a zero Hausdorff dimension set of parameters. This result affirms Eliasson's conjecture in 2017. ",Kein DOI-Link verfügbar,2305.00457v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Object Proposal with Kernelized Partial Ranking,1970,"  Object proposals are an ensemble of bounding boxes with high potential to contain objects. In order to determine a small set of proposals with a high recall, a common scheme is extracting multiple features followed by a ranking algorithm which however, incurs two major challenges: {\bf 1)} The ranking model often imposes pairwise constraints between each proposal, rendering the problem away from an efficient training/testing phase; {\bf 2)} Linear kernels are utilized due to the computational and memory bottleneck of training a kernelized model.   In this paper, we remedy these two issues by suggesting a {\em kernelized partial ranking model}. In particular, we demonstrate that {\bf i)} our partial ranking model reduces the number of constraints from $O(n^2)$ to $O(nk)$ where $n$ is the number of all potential proposals for an image but we are only interested in top-$k$ of them that has the largest overlap with the ground truth; {\bf ii)} we permit non-linear kernels in our model which is often superior to the linear classifier in terms of accuracy. For the sake of mitigating the computational and memory issues, we introduce a consistent weighted sampling~(CWS) paradigm that approximates the non-linear kernel as well as facilitates an efficient learning. In fact, as we will show, training a linear CWS model amounts to learning a kernelized model. Extensive experiments demonstrate that equipped with the non-linear kernel and the partial ranking algorithm, recall at top-$k$ proposals can be substantially improved. ",https://doi.org/10.1016/j.patcog.2017.03.022,1502.01526v3,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Fermion-fermion interaction driven instability and criticality of   quadratic band crossing systems with the breaking of time-reversal symmetry,1970,"  We carefully study how the fermion-fermion interactions affect the low-energy states of a two-dimensional spin-$1/2$ fermionic system on the kagom\'{e} lattice with a quadratic band crossing point. With the help of the renormalization group approach, we can treat all kinds of fermionic interactions on the the same footing and then establish the coupled energy-dependent flows of fermionic interaction parameters via collecting one-loop corrections, from which a number of interesting results are extracted in the low-energy regime. At first, various sorts of fermion-fermion interactions furiously compete with each other and are inevitably attracted by certain fixed point in the parameter space, which clusters into three qualitatively distinct regions relying heavily upon the structure parameters of materials. In addition, we notice that an instability accompanied by some symmetry breaking is triggered around different sorts of fixed points. Computing and comparing susceptibilities of twelve potential candidates indicates that charge density wave always dominates over all other instabilities. Incidently, there exist several subleading ones including the $x$-current, bond density, and chiral plus s-wave superconductors. Finally, we realize that strong fluctuations nearby the leading instability prefer to suppress density of states and specific heat as well compressibility of quasiparticles in the lowest-energy limit. ",https://doi.org/10.1016/j.nuclphysb.2021.115371,2010.10963v6,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Physics-Assisted Reduced-Order Modeling for Identifying Dominant   Features of Transonic Buffet,1970,"  Transonic buffet is a flow instability phenomenon that arises from the interaction between the shock wave and the separated boundary layer. This flow phenomenon is considered to be highly detrimental during flight and poses a significant risk to the structural strength and fatigue life of aircraft. Up to now, there has been a lack of an accurate, efficient, and intuitive metric to predict buffet and impose a feasible constraint on aerodynamic design. In this paper, a Physics-Assisted Variational Autoencoder (PAVAE) is proposed to identify dominant features of transonic buffet, which combines unsupervised reduced-order modeling with additional physical information embedded via a buffet classifier. Specifically, four models with various weights adjusting the contribution of the classifier are trained, so as to investigate the impact of buffet information on the latent space. Statistical results reveal that buffet state can be determined exactly with just one latent space when a proper weight of classifier is chosen. The dominant latent space further reveals a strong relevance with the key flow features located in the boundary layers downstream of shock. Based on this identification, the displacement thickness at 80% chordwise location is proposed as a metric for buffet prediction. This metric achieves an accuracy of 98.5% in buffet state classification, which is more reliable than the existing separation metric used in design. The proposed method integrates the benefits of feature extraction, flow reconstruction, and buffet prediction into a unified framework, demonstrating its potential in low-dimensional representations of high-dimensional flow data and interpreting the ""black box"" neural network. ",https://doi.org/10.1063/5.0152127,2305.13644v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Critical behavior around the fixed points driven by fermion-fermion   interactions and disorders in the nodal-line superconductors,1970,"  We systematically investigate the intricate interplay between short-range fermion-fermion interactions and disorder scatterings beneath the superconducting dome of noncentrosymmetric nodal-line superconductors. Employing the renormalization group that unbiasedly treats all kinds of potential degrees of freedom, we establish energy-dependent coupled flows for all associated interaction parameters. Decoding the low-energy information from these coupled evolutions leads to the emergence of several intriguing behavior in the low-energy regime. At first, we identify eight distinct types of fixed points, which are determined by the competition of all interaction parameters and dictate the low-energy properties. Next, we carefully examine and unveil distinct fates of physical implications as approaching such fixed points. The density of states of quasiparticles displays a linear dependence on frequency around the first fixed point, while other fixed points present diverse frequency-dependent behavior. Compressibility and specific heat exhibit unique trends around different fixed points, with the emergence of non-Fermi-liquid behavior nearby the fifth fixed point. Furthermore, after evaluating the susceptibilities of the potential states, we find that a certain phase transition below the critical temperature can be induced when the system approaches the fifth fixed point, transitioning from the nodal-line superconducting state to another superconducting state. This research would enhance our understanding of the unique behavior in the low-energy regime of nodal-line superconductors. ",https://doi.org/10.1140/epjp/s13360-024-05388-5,2402.02040v2,Yes,"intricate(1), potent(2)"
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Fermion-fermion interaction driven phase transitions in the rhombohedral   trilayer graphene,1970,"  The effects of short-range fermion-fermion interactions on the low-energy properties of the rhombohedral trilayer graphene are comprehensively investigated by virtue of the momentum-shell renormalization group method. We take into account all one-loop corrections and establish the energy-dependent coupled evolutions of independent fermionic couplings that carry the physical information stemming from the interplay of various fermion-fermion interactions. With the help of the detailed numerical analysis, we notice that the ferocious competition among all fermion-fermion interactions can drive fermionic couplings to four distinct kinds of fixed points, dubbed $\textrm{FP}_{1}$, $\textrm{FP}_{2}$, $\textrm{FP}_{3}$ and $\textrm{FP}_{4}$, in the interaction-parameter space. Such fixed points principally dictate the fate of the system in the low-energy regime, which are always associated with some instabilities with specific symmetry breakings and thus accompanied by certain phase transitions. In order to judge the favorable states arising from the potential phase transitions, we bring out a number of fermion-bilinear source terms to characterize the underlying candidate states. By comparing their related susceptibilities, it is determined that the dominant states correspond to a spin-singlet superconductivity, a spin-triplet pair-density-wave, and a spin-triplet superconductivity for approaching the fixed points $\textrm{FP}_{1,3}$, $\textrm{FP}_{2}$, and $\textrm{FP}_{4}$, respectively. These results would be helpful to further reveal the low-energy properties of the rhombohedral trilayer graphene and analogous materials. ",Kein DOI-Link verfügbar,2406.01877v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Units and Numerical Values of the Effective Couplings in Perturbative   Heterotic String Vacua,1970,"  We determine the units and numerical values for a class of couplings in the effective theory of perturbative heterotic string vacua, with the emphasis on the correct translation between the canonical gauge coupling g and Planck scale M_Planck ~ 1.2 x 10^19 GeV as used in the effective theory description and the string coupling g_string and string tension alpha' as used in the S-matrix amplitude calculation. In particular, we determine the effective couplings in the superpotential and revisit the Fayet-Iliopoulos (FI) term in a class of models with an anomalous U(1). We derive the values of the effective Yukawa couplings (at the third and fourth order) after the restabilization of vacuum along a particular F- and D-flat direction and show that they are comparable in magnitude. The result corrects results quoted in the literature, and may have implications for the string derived phenomenology, e.g., that of fermion textures. ",https://doi.org/10.1103/PhysRevD.59.107901,hep-ph/9808321v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",A low-energy solution to the mu-problem in gauge mediation,1970,"  In the gauge-mediation framework the soft supersymmetry breaking mass parameters of the supersymmetric standard model are induced by the gauge interactions of some messenger fields. The parameters exhibit flavor universality which is dictated by the gauge interactions and which efficiently eliminates new dangerous contributions to flavor changing neutral currents. However, the Higgs potential in this framework typically contains an unacceptable hierarchy between its dimensionful parameters (the $\mu$-problem of gauge mediation). We show that the problem can be resolved if the Higgs potential arises dynamically once an intermediate U(1)' sector is integrated out rather than arising radiatively from some Yukawa interactions at the messenger scale. As an added benefit, such models may naturally avoid new contribution to CP violating amplitudes. The proposed framework is described, explicit examples are given and its phenomenology is explored. The $\mu$ problem is resolved in this case by the low-energy U(1)' dynamics which could be tested in future collider experiments. ",https://doi.org/10.1103/PhysRevD.60.115005,hep-ph/9905252v2,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Three Family Type IIB Orientifold String Vacua with Non-Abelian Wilson   Lines,1970,"  We address the implementation of non-Abelian Wilson lines in D=4 N=1 Type IIB orientifold constructions. We present an explicit three-family example with the gauge group (U(4)xU(2)xSU(2)xSU(2))^2x(U(6)xSp(4))^2 and give the particle spectrum and the trilinear superpotential. Emphasizing the new subtleties associated with the introduction of non-Abelian Wilson lines, we show that the Abelian gauge anomalies are cancelled by the Green-Schwarz-type mechanism, and calculate the Fayet-Iliopoulos terms and gauge coupling corrections. The analysis thus sets a stage for further investigations of the phenomenological implications of this model. ",https://doi.org/10.1088/1126-6708/2000/04/004,hep-th/9911021v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Dynamical Supersymmetry Breaking in Standard-like Models with   Intersecting D6-branes,1970,"  We address dynamical supersymmetry breaking within a N=1 supersymmetric Standard-like Model based on a Z_2 x Z_2 Type IIA orientifold with intersecting D6-branes. The model possesses an additional, confining gauge sector with the USp(2)_A x USp(2)_B x USp(4) gauge group, where the gaugino condensation mechanism allows for the breaking of supersymmetry and stabilizes moduli. We derive the leading contribution to the non-perturbative effective superpotential and determine numerically the minima of the supergravity potential. These minima break supersymmetry and fix two undetermined moduli, which in turn completely specify the gauge couplings at the string scale. For this specific construction the minima have a negative cosmological constant. We expect that for other supersymmetric Standard-like models with intersecting D6-branes, which also possess confining gauge sectors, the supersymmetry breaking mechanism would have qualitatively similar features. ",https://doi.org/10.1103/PhysRevD.68.046002,hep-th/0303208v1,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Dark Energy Model with Spinor Matter and Its Quintom Scenario,1970,"  A class of dynamical dark energy models, dubbed Spinor Quintom, can be constructed by a spinor field $\psi$ with a nontraditional potential. We find that, if choosing suitable potential, this model is able to allow the equation-of-state to cross the cosmological constant boundary without introducing any ghost fields. In a further investigation, we show that this model is able to mimic a perfect fluid of Chaplygin gas with $p=-c/\rho$ during the evolution, and also realizes the Quintom scenario with its equation-of-state across -1. ",https://doi.org/10.1088/0264-9381/25/16/165014,0806.3890v2,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Magnetic Silicon Fullerene,1970,"  A metal-encapsulating silicon fullerene, Eu@Si20, has been predicted by density functional theory to be by far the most stable fullerene-like silicon structure. The Eu@Si20 structure is a dodecahedron with D2h symmetry in which the europium atom occupies the center site. The calculated results show that the europium atom has a large magnetic moment of nearly 7.0 Bohr magnetons. In addition, it was found that a stable ""pearl necklace"" nanowire, constructed by concatenating a series of Eu@Si20 units, with the central europium atom, retains the high spin moment. The magnetic structure of the nanowire indicates potential applications in the fields of spintronics and high-density magnetic storage. ",https://doi.org/10.1039/B923865D,0908.1494v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Disorder effects at a nematic quantum critical point in d-wave cuprate   superconductor,1970,"  A d-wave high temperature cuprate superconductor exhibits a nematic ordering transition at zero temperature. Near the quantum critical point, the coupling between gapless nodal quasiparticles and nematic order parameter fluctuation can result in unusual behaviors, such as extreme anisotropy of fermion velocities. We study the disorder effect on the nematic quantum critical behavior and especially on the flow of fermion velocities. The disorders that couple to nodal quasiparticles are divided into three types: random mass, random gauge field, and random chemical potential. A renormalization group analysis shows that random mass and random gauge field are both irrelevant and thus do not change the fixed point of extreme velocity anisotropy. However, the marginal interaction due to random chemical potential destroys this fixed point and makes the nematic phase transition unstable. ",https://doi.org/10.1103/PhysRevB.83.214503,1101.1551v3,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Topological insulators for high performance terahertz to infrared   applications,1970,"  Topological insulators in the Bi2Se3 family have an energy gap in the bulk and a gapless surface state consisting of a single Dirac cone. Low frequency optical absorption due to the surface state is universally determined by the fine structure constant. When the thickness of these three dimensional topological insulators is reduced, they become quasi-two dimensional insulators with enhanced absorbance. The two dimensional insulators can be topologically trivial or non-trivial depending on the thickness, and we predict that the optical absorption is larger for topological non-trivial case compared with the trivial case. Since the three dimensional topological insulator surface state is intrinsically gapless, we propose its potential application in wide bandwidth, high performance photo-detection covering a broad spectrum ranging from terahertz to infrared. The performance of photodetection can be dramatically enhanced when the thickness is reduced to several quintuple layers, with a widely tunable band gap depending on the thickness. ",https://doi.org/10.1103/PhysRevB.82.245107,1101.3583v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Unconventional behavior of Dirac fermions in three-dimensional gauge   theory,1970,"  We study the unconventional behavior of massless Dirac fermions due to interaction with a U(1) gauge field in two spatial dimensions. At zero chemical potential, the longitudinal and transverse components of gauge interaction are both long-ranged. There is no fermion velocity renormalization since the system respects Lorentz invariance. At finite chemical potential, the Lorentz invariance is explicitly broken by the finite Fermi surface. The longitudinal gauge interaction is statically screened and becomes unimportant, whereas the transverse gauge interaction remains long-ranged and leads to singular renormalization of fermion velocity. The anomalous dimension of fermion velocity is calculated by means of the renormalization group method. We then examine the influence of singular velocity renormalization on several physical quantities, and show that they exhibit different behavior at zero and finite chemical potential. ",https://doi.org/10.1103/PhysRevD.85.105010,1202.3109v3,Yes,potent(3)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",A new strategy for directly calculating the minimum eigenvector of   matrices without diagonalization,1970,"  The diagonalization of matrices may be the top priority in the application of modern physics. In this paper, we numerically demonstrate that, for real symmetric random matrices with non-positive off-diagonal elements, a universal scaling between the eigenvector and matrix elements exists. Namely, each element of the eigenvector of ground states linearly correlates with the sum of matrix elements in the corresponding row. Although the conclusion is obtained based on the random matrices, the linear relationship still keeps for regular matrices, in which off-diagonal elements are non-positive. The relationship implies a straightforward method to directly calculate the eigenvector of ground states for a kind of matrices. The test on both Hubbard and Ising models shows that, this new method works excellently. ",https://doi.org/10.1038/s41598-020-60103-5,1901.10626v3,Yes,excellently(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Performance evaluation of an integrated photonic convolutional neural   network based on delay buffering and wavelength division multiplexing,1970,"  Photonic technologies have shown a promising way to build high-speed and high-energy-efficiency neural network accelerators. In previously presented photonic neural networks, architectures are mainly designed for fully-connected layers. When convolutional layers are executed in such neural networks, the large-scale electrooptic modulation array heavily increases the energy dissipation on chip. To increase the energy efficiency, here we show an integrated photonic architecture specifically for convolutional layer calculations. Optical delay lines replace electronics to execute data manipulations on optical chip, reducing the scale of electro-optic modulation array. Consequently, the energy dissipation of these parts is mitigated. Powered by wavelength division multiplexing, the footprint of delay lines is significantly reduced compared with previous art, thus being practical to fabricate. We evaluate the potential performance of the proposed architecture with respect to component flaws in practical fabrications. According to the results, with well-controlled system insertion loss, energy efficiency of the proposed architecture would surpass previously presented works and the state-of-art electronic processors. We anticipate the proposed architecture is beneficial for future fast and energy-efficient convolutional neural network accelerators. ",Kein DOI-Link verfügbar,1910.12635v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Dissipative Edge Transport in Disordered Axion Insulator Films,1970,"  We investigate the role of disorder in the edge transport of axion insulator films. We predict by first-principles calculations that even-number-layer MnBi$_2$Te$_4$ have gapped helical edge states. The random potential will dramatically modify the edge spectral function to become gapless. However, such gapless helical state here is fundamentally different from that in quantum spin Hall insulator or topological Anderson insulator. We further study the edge transport in this system by Landauer-B\""{u}ttiker formalism, and find such gapless edge state is dissipative and not immune to backscattering, which would explain the dissipative nonlocal transport in the axion insulator state observed in six septuple layer MnBi$_2$Te$_4$ experimentally. Several transport experiments are proposed to verify our theory on the dissipative helical edge channels. In particular, the longitudinal resistance can be greatly reduced by adding an extra floating probe even if it is not used. These results will facilitate the observsation of long-sought topological magnetoelectric effect in axion insulators. ",https://doi.org/10.1103/PhysRevB.108.245116,2109.06178v1,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Exploring Active Learning in Meta-Learning: Enhancing Context Set   Labeling,1970,"  Most meta-learning methods assume that the (very small) context set used to establish a new task at test time is passively provided. In some settings, however, it is feasible to actively select which points to label; the potential gain from a careful choice is substantial, but the setting requires major differences from typical active learning setups. We clarify the ways in which active meta-learning can be used to label a context set, depending on which parts of the meta-learning process use active learning. Within this framework, we propose a natural algorithm based on fitting Gaussian mixtures for selecting which points to label; though simple, the algorithm also has theoretical motivation. The proposed algorithm outperforms state-of-the-art active learning methods when used with various meta-learning algorithms across several benchmark datasets. ",Kein DOI-Link verfügbar,2311.02879v3,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Predicting Many Properties of Crystals by a Single Deep Learning Model,1970,"  The use of machine learning methods for predicting the properties of crystalline materials encounters significant challenges, primarily related to input encoding, output versatility, and interpretability. Here, we introduce CrystalBERT, an adaptable transformer-based framework with novel structure that integrates space group, elemental, and unit cell information. The method's adaptability lies not only in its ability to seamlessly combine diverse features but also in its capability to accurately predict a wide range of physically important properties, including topological properties, superconducting transition temperatures, dielectric constants, and more. CrystalBERT also provides insightful physical interpretations regarding the features that most significantly influence the target properties. Our findings indicate that space group and elemental information are more important for predicting topological and superconducting properties, in contrast to some properties that primarily depend on the unit cell information. This underscores the intricate nature of topological and superconducting properties. By incorporating all these features, we achieve a high accuracy of 91% in topological classification, surpassing prior studies and identifying previously misclassified topological materials, further demonstrating the effectiveness of our model. ",Kein DOI-Link verfügbar,2405.18944v1,Yes,intricate(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Confinement induced by fermion damping in three-dimensional QED,1970,"  The three-dimensional non-compact QED is known to exhibit weak confinement when fermions acquire a finite mass via the mechanism of dynamical chiral symmetry breaking. In this paper, we study the effect of fermion damping caused by elastic scattering on the classical potential between fermions. By calculating the vacuum polarization function that incorporates the fermion damping effect, we show that fermion damping can induce a weak confinement even when the fermions are massless and the chiral symmetry is not broken. ",https://doi.org/10.1103/PhysRevD.82.067701,1008.0736v2,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Cooper instability generated by attractive fermion-fermion interaction   in the two-dimensional semi-Dirac semimetals,1970,"  Cooper instability associated with superconductivity in the two-dimensional semi-Dirac semimetals is attentively studied in the presence of attractive Cooper-pairing interaction, which is the projection of an attractive fermion-fermion interaction. Performing the standard renormalization group analysis shows that the Cooper theorem is violated at zero chemical potential but instead Cooper instability can be generated only if the absolute strength of fermion-fermion coupling exceeds certain critical value and transfer momentum is restricted to a confined region, which is determined by the initial conditions. Rather, the Cooper theorem would be instantly restored once a finite chemical potential is introduced and thus a chemical potential-tuned phase transition is expected. Additionally, we briefly examine the effects of impurity scatterings on the Cooper instability at zero chemical potential, which in principle are harmful to Cooper instability although they can enhance the density of states of systems. Furthermore, the influence of competition between a finite chemical potential and impurities upon the Cooper instability is also simply investigated. These results are expected to provide instructive clues for exploring unconventional superconductors in the kinds of semimetals. ",https://doi.org/10.1088/1361-648X/ab142d,1806.03410v6,Yes,potent(5)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Singular low-energy states of tilted Dirac semimetals induced by the   fermion-fermion interactions,1970,"  We attentively investigate the effects of short-range fermion-fermion interactions on the low-energy properties of both two-dimensional type-I and type-II tilted Dirac semimetals by means of the renormalization group framework. Practicing the standard renormalization group procedures via taking into account all one-loop corrections gives rise to the coupled energy-dependent evolutions of all interaction parameters, which are adopted to carefully examine whether and how the fermion-fermion interactions influence the low-energy physical behaviors of tilted Dirac fermions. After carrying out the detailed analysis of coupled flows, we figure out the tilting parameter dictates the low-energy states of tilted Dirac fermions in conjunction with starting values of fermion-fermion couplings. With proper variations of these two kinds of parameters, the tilted Dirac fermions can either flow towards the Gaussian fixed point or undergo certain instability that is conventionally accompanied by a phase transition in the low-energy regime. In addition, all potential instabilities can be clustered into five distinct classes owing to the competitions between the tilting parameter and initial fermionic interactions. Moreover, the dominant phases accompanied by the instabilities are determined via computing and comparing the susceptibilities of eight potential phases. ",https://doi.org/10.1140/epjb/e2019-100373-9,1907.13341v3,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",FashionLOGO: Prompting Multimodal Large Language Models for Fashion Logo   Embeddings,1970,"  Logo embedding plays a crucial role in various e-commerce applications by facilitating image retrieval or recognition, such as intellectual property protection and product search. However, current methods treat logo embedding as a purely visual problem, which may limit their performance in real-world scenarios. A notable issue is that the textual knowledge embedded in logo images has not been adequately explored. Therefore, we propose a novel approach that leverages textual knowledge as an auxiliary to improve the robustness of logo embedding. The emerging Multimodal Large Language Models (MLLMs) have demonstrated remarkable capabilities in both visual and textual understanding and could become valuable visual assistants in understanding logo images. Inspired by this observation, our proposed method, FashionLOGO, aims to utilize MLLMs to enhance fashion logo embedding. We explore how MLLMs can improve logo embedding by prompting them to generate explicit textual knowledge through three types of prompts, including image OCR, brief captions, and detailed descriptions prompts, in a zero-shot setting. We adopt a cross-attention transformer to enable image embedding queries to learn supplementary knowledge from textual embeddings automatically. To reduce computational costs, we only use the image embedding model in the inference stage, similar to traditional inference pipelines. Our extensive experiments on three real-world datasets demonstrate that FashionLOGO learns generalized and robust logo embeddings, achieving state-of-the-art performance in all benchmark datasets. Furthermore, we conduct comprehensive ablation studies to demonstrate the performance improvements resulting from the introduction of MLLMs. ",Kein DOI-Link verfügbar,2308.09012v1,Yes,notable(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Effects of the interplay between fermionic interactions and disorders in   the nodal-line superconductors,1970,"  We study the interplay between fermion-fermion interactions and disorder scatterings beneath the superconducting dome of noncentrosymmetric nodal-line superconductors. With the application of renormalization group, several interesting low-energy behaviors are extracted from the coupled equations of all interaction parameters. At the clean limit, fermion-fermion interactions decrease with lowering the energy scales but conversely fermion velocities climb up and approach certain saturated values. This yields a slight decrease or increase of the anisotropy of fermion velocities depending upon their initial ratio. After bringing out four kinds of disorders designated by the random charge ($\Delta_{1}$), random mass ($\Delta_{2}$), random axial chemical potential ($\Delta_{3}$), and spin-orbit scatterers ($\Delta_{4}$) based on their own unique features, we begin with presenting the distinct low-energy fates of these disorders. For the presence of sole disorder, its strength becomes either relevant ($\Delta_{1,4}$) or irrelevant($\Delta_{2,3}$) in the low-energy regime. However, the competition for multiple sorts of disorders is capable of qualitatively reshaping the low-energy properties of disorders $\Delta_{2,3,4}$. Besides, it can generate an initially absent disorder as long as two of $\Delta_{1,2,3}$ are present. In addition, the fermion-fermion couplings are insensitive to the presence of $\Delta_4$ but rather substantially modified by $\Delta_1$, $\Delta_2$, or $\Delta_3$, and evolve towards zero or certain finite non-zero values under the coexistence of distinct disorders. Furthermore, the fermion velocities flow towards certain finite saturated value for the only presence of $\Delta_{2,3}$ and vanish for all other situations. As to their ratio, it acquires a little increase once the disorder is subordinate to fermionic interactions, otherwise keeps some fixed constant. ",Kein DOI-Link verfügbar,2212.02356v3,Yes,potent(1)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",Edge State Induced Andreev Oscillation in Quantum Anomalous Hall   Insulator-Superconductor Junctions,1970,"  We study the quantum Andreev oscillation induced by interference of the edge chiral Majorana fermions in junctions made of quantum anomalous Hall (QAH) insulators and superconductors (SCs). We show two chiral Majorana fermions on a QAH edge with SC proximity generically have a momentum difference $\Delta k$, which depends on the chemical potentials of both the QAH insulator and the SC. Due to the spatial interference induced by $\Delta k$, the longitudinal conductance of QAH-SC junctions oscillates with respect to the edge lengths and the chemical potentials, which can be probed via charge transport. Furthermore, we show the dynamical SC phase fluctuation will give rise to a geometrical correction to the longitudinal conductance of the junctions. ",https://doi.org/10.1103/PhysRevB.93.161401,1512.05856v2,Yes,potent(2)
0000-0001-7735-8682,Jing Wang,"University of Nottingham, University of Nottingham - Ningbo China",The Fokas-Lenells equations: Bilinear approach,1970,"  In this paper, the Fokas-Lenells equations are investigated via bilinear approach. We bilinearize the unreduced Fokas-Lenells system, derive double Wronskian solutions, and then, by means of a reduction technique we obtain variety of solutions of the reduced equations. This enables us to have a full profile of solutions of the classical and nonlocal Fokas-Lenells equations. Some obtained solutions are illustrated based on asymptotic analysis. As a notable new result, we obtain solutions to the Fokas-Lenells equation, which are related to real discrete eigenvalues and not reported before in the analytic approaches. These solutions behave like (multi-)periodic waves or solitary waves with algebraic decay. In addition, we also obtain solutions to the two-dimensional massive Thirring model from those of the Fokas-Lenells equation. ",https://doi.org/10.1111/sapm.12454,2104.04938v3,Yes,notable(1)
0000-0002-3825-6891,Ana Vukovic,"University of Nottingham, University of Nottingham Faculty of Engineering",Theory and Numerical Modelling of Parity-Time Symmetric Structures in   Photonics: Introduction and Grating Structures in One Dimension,1970,  A class of structures based on PT PT-symmetric Bragg gratings in the presence of both gain and loss is studied. The basic concepts and properties of parity and time reversal in one-dimensional structures that possess idealised material properties are given. The impact of realistic material properties on the behaviour of these devices is then investigated. Further extension to include material non-linearity is used to study an innovative all-optical memory device. ,https://doi.org/10.1007/978-3-319-55438-9_6,1801.10191v1,Yes,innovative(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Mechanism Design for Large Scale Network Utility Maximization,1970,"  Network utility maximization (NUM) is a general framework for designing distributed optimization algorithms for large-scale networks. An economic challenge arises in the presence of strategic agents' private information. Existing studies proposed (economic) mechanisms but largely neglected the issue of large-scale implementation. Specifically, they require certain modifications to the deployed algorithms, which may bring the significant cost. To tackle this challenge, we present the large-scale Vickery-Clark-Grove (VCG) Mechanism for NUM, with a simpler payment rule characterized by the shadow prices. The Large-Scale VCG Mechanism maximizes the network utility and achieves individual rationality and budget balance. With infinitely many agents, agents' truthful reports of their types are their dominant strategies; for the finite case, each agent's incentive to misreport converges quadratically to zero. For practical implementation, we introduce a modified mechanism that possesses an additional important technical property, superimposability, which makes it able to be built upon any (potentially distributed) algorithm that optimally solves the NUM Problem and ensures all agents to obey the algorithm. We then extend this idea to the dynamic case, when agents' types are dynamically evolving as a controlled Markov process. In this case, the mechanism leads to incentive compatible actions of agent for each time slot. ",Kein DOI-Link verfügbar,2003.04263v2,Yes,potent(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Wireless Power Transfer with Information Asymmetry: A Public Goods   Perspective,1970,"  Wireless power transfer (WPT) technology enables a cost-effective and sustainable energy supply in wireless networks. However, the broadcast nature of wireless signals makes them non-excludable public goods, which leads to potential free-riders among energy receivers. In this study, we formulate the wireless power provision problem as a public goods provision problem, aiming to maximize the social welfare of a system of an energy transmitter (ET) and all the energy users (EUs), while considering their private information and self-interested behaviors. We propose a two-phase all-or-none scheme involving a low-complexity Power And Taxation (PAT) mechanism, which ensures voluntary participation, truthfulness, budget balance, and social optimality at every Nash equilibrium (NE). We propose a distributed PAT (D-PAT) algorithm to reach an NE, and prove its convergence by connecting the structure of NEs and that of the optimal solution to a related optimization problem. We further extend the analysis to a multi-channel system, which brings a further challenge due to the non-strict concavity of the agents' payoffs. We propose a Multi-Channel PAT (M-PAT) mechanism and a distributed M-PAT (D-MPAT) algorithm to address the challenge. Simulation results show that our design is most beneficial when there are more EUs with more homogeneous channel gains. ",Kein DOI-Link verfügbar,1904.06907v2,Yes,potent(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Index Modulation for 5G: Striving to Do More with Less,1970,"  The fifth generation (5G) wireless communications brag both high spectrum efficiency and high energy efficiency. To meet the requirements, various new techniques have been proposed. Among these, the recently-emerging index modulation has attracted significant interests. By judiciously activating a subset of certain communication {building blocks, such as} antenna, subcarrier and time slot, index modulation is claimed to have the potential to meet the challenging 5G needs. In this article, we will discuss index modulation and its general and specific representations, enhancements, and potential applications in various 5G scenarios. The objective is to reveal whether, and how, index modulation may strive for more performance gains with less medium resource occupation. ",Kein DOI-Link verfügbar,1712.06235v1,Yes,potent(2)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Age-Dependent Differential Privacy,1970,"  The proliferation of real-time applications has motivated extensive research on analyzing and optimizing data freshness in the context of \textit{age of information}. However, classical frameworks of privacy (e.g., differential privacy (DP)) have overlooked the impact of data freshness on privacy guarantees, which may lead to unnecessary accuracy loss when trying to achieve meaningful privacy guarantees in time-varying databases. In this work, we introduce \textit{age-dependent DP}, taking into account the underlying stochastic nature of a time-varying database. In this new framework, we establish a connection between classical DP and age-dependent DP, based on which we characterize the impact of data staleness and temporal correlation on privacy guarantees. Our characterization demonstrates that \textit{aging}, i.e., using stale data inputs and/or postponing the release of outputs, can be a new strategy to protect data privacy in addition to noise injection in the traditional DP framework. Furthermore, to generalize our results to a multi-query scenario, we present a sequential composition result for age-dependent DP under any publishing and aging policies. We then characterize the optimal tradeoffs between privacy risk and utility and show how this can be achieved. Finally, case studies show that to achieve a target of an arbitrarily small privacy risk in a single-query case, combing aging and noise injection only leads to a bounded accuracy loss, whereas using noise injection only (as in the benchmark case of DP) will lead to an unbounded accuracy loss. ",Kein DOI-Link verfügbar,2209.01466v1,Yes,fresh(2)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Multipacting saturation in parallel plate and micro-pulse electron gun,1970,"  A novel parallel plate model is proposed that divided the electron cloud into three parts at saturation, and it is studied in detail using both an analytical approach and PIC (Particle In Cell) code simulations. As one part of the electron cloud, ribbons modes are suggested by tracking the trajectory of individual particle, and the aim of this mode form is to simplify the progress of multipacting effect in the parallel plate so as to be eliminated by optimizing RF parameters. The micro-pulse electron gun (MPG) has demonstrated the potential to address the need for high peak and average current electron beams, hence studying the multipacting in MPG is essential. On the basis of multipacting studying in the parallel plate, it is clear that increasing the cavity voltage is of interest in yielding high quality beams in the gun. ",https://doi.org/10.1088/1674-1137/39/2/027003,1403.6360v2,Yes,potent(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Minimizing Age of Information for Mobile Edge Computing Systems: A   Nested Index Approach,1970,"  Exploiting the computational heterogeneity of mobile devices and edge nodes, mobile edge computation (MEC) provides an efficient approach to achieving real-time applications that are sensitive to information freshness, by offloading tasks from mobile devices to edge nodes. We use the metric Age-of-Information (AoI) to evaluate information freshness. An efficient solution to minimize the AoI for the MEC system with multiple users is non-trivial to obtain due to the random computing time. In this paper, we consider multiple users offloading tasks to heterogeneous edge servers in a MEC system. We first reformulate the problem as a Restless Multi-Arm-Bandit (RMAB) problem and establish a hierarchical Markov Decision Process (MDP) to characterize the updating of AoI for the MEC system. Based on the hierarchical MDP, we propose a nested index framework and design a nested index policy with provably asymptotic optimality. Finally, the closed form of the nested index is obtained, which enables the performance tradeoffs between computation complexity and accuracy. Our algorithm leads to an optimality gap reduction of up to 40%, compared to benchmarks. Our algorithm asymptotically approximates the lower bound as the system scalar gets large enough. ",Kein DOI-Link verfügbar,2307.01366v1,Yes,fresh(2)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Age-minimal Multicast by Graph Attention Reinforcement Learning,1970,"  Age of Information (AoI) is an emerging metric used to assess the timeliness of information, gaining research interest in real-time multicast applications such as video streaming and metaverse platforms. In this paper, we consider a dynamic multicast network with energy constraints, where our objective is to minimize the expected time-average AoI through energy-constrained multicast routing and scheduling. The inherent complexity of the problem, given the NP-hardness and intertwined scheduling and routing decisions, makes existing approaches inapplicable. To address these challenges, we decompose the original problem into two subtasks, each amenable to reinforcement learning (RL) methods. Subsequently, we propose an innovative framework based on graph attention networks (GATs) to effectively capture graph information with superior generalization capabilities. To validate our framework, we conduct experiments on three datasets including a real-world dataset called AS-733, and show that our proposed scheme reduces the average weighted AoI by 62.9% and reduces the energy consumption by at most 72.5% compared to baselines. ",Kein DOI-Link verfügbar,2404.18084v3,Yes,innovative(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Beyond Boundaries: efficient Projected Entangled Pair States methods for   periodic quantum systems,1970,"  Projected Entangled Pair States (PEPS) are recognized as a potent tool for exploring two-dimensional quantum many-body systems. However, a significant challenge emerges when applying conventional PEPS methodologies to systems with periodic boundary conditions (PBC), attributed to the prohibitive computational scaling with the bond dimension. This has notably restricted the study of systems with complex boundary conditions. To address this challenge, we have developed a strategy that involves the superposition of PEPS with open boundary conditions (OBC) to treat systems with PBC. This approach significantly reduces the computational complexity of such systems while maintaining their translational invariance and the PBC. We benchmark this method against the Heisenberg model and the $J_1$-$J_2$ model, demonstrating its capability to yield highly accurate results at low computational costs, even for large system sizes. The techniques are adaptable to other boundary conditions, including cylindrical and twisted boundary conditions, and therefore significantly expands the application scope of the PEPS approach, shining new light on numerous applications. ",Kein DOI-Link verfügbar,2407.15333v1,Yes,potent(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Hybrid Pricing for Mobile Collaborative Internet Access,1970,"  Mobile Collaborative Internet Access (MCA) enables mobile users to share their Internet through flexible tethering arrangements. This can potentially make better use of network resources. However, from a mobile network operator's (MNO's) viewpoint, it can either reduce revenue or increase congestion, and thus has been blocked by some MNOs in practice. We propose a hybrid pricing framework for MNOs who charge users separately for access and tethering. This scheme serves to coordinate the tethering decisions of mobile users with MNO network management objectives. We analyze the MNOs' equilibrium pricing strategies in both cooperative and competitive scenarios. In the cooperative scenario, at the equilibrium, each user's cost is independent of any chosen tethering links. We then characterize the optimal hybrid pricing strategies of MNOs in this scenario. For the competitive scenario, we formulate the MNOs' competitive interactions as a pricing game, and we show that MNO competition leads to equalized prices for users if an equilibrium exists but does not guarantee its existence. Both insights motivate a quantity competition game, which is shown to guarantee an equilibrium. Simulation results show that in scenarios of interest the proposed hybrid pricing schemes can double both MNOs' profit and users' payoff and such improvements increase with the degree of network heterogeneity. ",Kein DOI-Link verfügbar,1809.02917v2,Yes,potent(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",How to Price Fresh Data,1970,"  We introduce the concept of a fresh data market, in which a destination user requests, and pays for, fresh data updates from a source provider. Data freshness is captured by the {\it age of information} (AoI) metric, defined as the time elapsed since the latest update has reached the destination. The source incurs an operational cost, modeled as an increasing convex function of the number of updates. The destination incurs an age-related cost, modeled as an increasing convex function of the AoI. The source charges the destination for each update and designs a pricing mechanism to maximize its profit; the destination on the other hand chooses a data update schedule to minimize the summation of its payments to the source and its age-related cost. The interaction among the source and destination is hence game-theoretic. Motivated by the existing pricing literature, we first study a time-dependent pricing scheme, in which the price for each update depends on when it is requested. We show in this case that the game equilibrium leads to only one data update, which does not yield the maximum profit to the source. This motivates us to consider a quantity-based pricing scheme, in which the price of each update depends on how many updates have been previously requested. We show that among all pricing schemes in which the price of an update may vary according to both time and quantity, the quantity-based pricing scheme performs best: it maximizes the source's profit and minimizes the social cost of the system, defined as the aggregate source's operational cost and the destination's age-related cost. Numerical results show that the optimal quantity-based pricing can be 27% more profitable for the source and incurs 54% less social cost, compared with the optimal time-dependent pricing. ",Kein DOI-Link verfügbar,1904.06899v2,Yes,fresh(3)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Optimal and Quantized Mechanism Design for Fresh Data Acquisition,1970,"  The proliferation of real-time applications has spurred much interest in data freshness, captured by the {\it age-of-information} (AoI) metric. When strategic data sources have private market information, a fundamental economic challenge is how to incentivize them to acquire fresh data and optimize the age-related performance. In this work, we consider an information update system in which a destination acquires, and pays for, fresh data updates from multiple sources. The destination incurs an age-related cost, modeled as a general increasing function of the AoI. Each source is strategic and incurs a sampling cost, which is its private information and may not be truthfully reported to the destination. The destination decides on the price of updates, when to get them, and who should generate them, based on the sources' reported sampling costs. We show that a benchmark that naively trusts the sources' reports can lead to an arbitrarily bad outcome compared to the case where sources truthfully report. To tackle this issue, we design an optimal (economic) mechanism for timely information acquisition following Myerson's seminal work. To this end, our proposed optimal mechanism minimizes the sum of the destination's age-related cost and its payment to the sources, while ensuring that the sources truthfully report their private information and will voluntarily participate in the mechanism. However, finding the optimal mechanisms may suffer from \textit{prohibitively expensive computational overheads} as it involves solving a nonlinear infinite-dimensional optimization problem. We further propose a quantized version of the optimal mechanism that achieves asymptotic optimality, maintains the other economic properties, and enables one to tradeoff between optimality and computational overheads. ",Kein DOI-Link verfügbar,2006.15751v4,Yes,fresh(3)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Pricing Fresh Data,1970,"  We introduce the concept of {\it fresh data trading}, in which a destination user requests, and pays for, fresh data updates from a source provider, and data freshness is captured by the {\it age of information} (AoI) metric. Keeping data fresh relies on frequent data updates by the source, which motivates the source to {\it price fresh data}. In this work, the destination incurs an age-related cost, modeled as a general increasing function of the AoI. The source designs a pricing mechanism to maximize its profit; the destination chooses a data update schedule to trade off its payments to the source and its age-related cost. Depending on different real-time applications and scenarios, we study both a predictable-deadline and an unpredictable-deadline models. The key challenge of designing the optimal pricing scheme lies in the destination's time-interdependent valuations, due to the nature of AoI and the infinite-dimensional and dynamic optimization. To this end, we consider three pricing schemes that exploit and understand the profitability of three different dimensions in designing pricing: a {\it time-dependent} pricing scheme, in which the price for each update depends on when it is requested; a {\it quantity-based} pricing scheme, in which the price of each update depends on how many updates have been previously requested; a {\it subscription-based} pricing scheme, in which the price for each update is flat-rate but the source charges an additional subscription fee. Our analysis reveals that the optimal subscription-based pricing maximizes the source's profit among all possible pricing schemes under both predictable deadline and unpredictable deadline models; the optimal quantity-based pricing scheme is only optimal with a predictable deadline; the time-dependent pricing scheme, under the unpredictable deadline, is asymptotically optimal under significant time discounting. ",Kein DOI-Link verfügbar,2006.16805v4,Yes,fresh(5)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology",Universal Conditional Masked Language Pre-training for Neural Machine   Translation,1970,"  Pre-trained sequence-to-sequence models have significantly improved Neural Machine Translation (NMT). Different from prior works where pre-trained models usually adopt an unidirectional decoder, this paper demonstrates that pre-training a sequence-to-sequence model but with a bidirectional decoder can produce notable performance gains for both Autoregressive and Non-autoregressive NMT. Specifically, we propose CeMAT, a conditional masked language model pre-trained on large-scale bilingual and monolingual corpora in many languages. We also introduce two simple but effective methods to enhance the CeMAT, aligned code-switching & masking and dynamic dual-masking. We conduct extensive experiments and show that our CeMAT can achieve significant performance improvement for all scenarios from low- to extremely high-resource languages, i.e., up to +14.4 BLEU on low resource and +7.9 BLEU improvements on average for Autoregressive NMT. For Non-autoregressive NMT, we demonstrate it can also produce consistent performance gains, i.e., up to +5.3 BLEU. To the best of our knowledge, this is the first work to pre-train a unified model for fine-tuning on both NMT tasks. Code, data, and pre-trained models are available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/CeMAT. ",Kein DOI-Link verfügbar,2203.09210v3,Yes,notable(1)
0000-0002-1743-0686,Meng Zhang,"KTH Royal Institute of Technology, KTH Royal Institute of Technology Gene technology","SCATTER: Algorithm-Circuit Co-Sparse Photonic Accelerator with   Thermal-Tolerant, Power-Efficient In-situ Light Redistribution",1970,"  Photonic computing has emerged as a promising solution for accelerating computation-intensive artificial intelligence (AI) workloads. However, limited reconfigurability, high electrical-optical conversion cost, and thermal sensitivity limit the deployment of current optical analog computing engines to support power-restricted, performance-sensitive AI workloads at scale. Sparsity provides a great opportunity for hardware-efficient AI accelerators. However, current dense photonic accelerators fail to fully exploit the power-saving potential of algorithmic sparsity. It requires sparsity-aware hardware specialization with a fundamental re-design of photonic tensor core topology and cross-layer device-circuit-architecture-algorithm co-optimization aware of hardware non-ideality and power bottleneck. To trim down the redundant power consumption while maximizing robustness to thermal variations, we propose SCATTER, a novel algorithm-circuit co-sparse photonic accelerator featuring dynamically reconfigurable signal path via thermal-tolerant, power-efficient in-situ light redistribution and power gating. A power-optimized, crosstalk-aware dynamic sparse training framework is introduced to explore row-column structured sparsity and ensure marginal accuracy loss and maximum power efficiency. The extensive evaluation shows that our cross-stacked optimized accelerator SCATTER achieves a 511X area reduction and 12.4X power saving with superior crosstalk tolerance that enables unprecedented circuit layout compactness and on-chip power efficiency. ",Kein DOI-Link verfügbar,2407.05510v1,Yes,potent(1)
0000-0002-0359-6474,Yilin Liu,KTH Royal Institute of Technology,RelicVR: A Virtual Reality Game for Active Exploration of Archaeological   Relics,1970,"  Digitalization is changing how people visit museums and explore the artifacts they house. Museums, as important educational venues outside classrooms, need to actively explore the application of digital interactive media, including games that can balance entertainment and knowledge acquisition. In this paper, we introduce RelicVR, a virtual reality (VR) game that encourages players to discover artifacts through physical interaction in a game-based approach. Players need to unearth artifacts hidden in a clod enclosure by using available tools and physical movements. The game relies on the dynamic voxel deformation technique to allow players to chip away earth covering the artifacts. We added uncertainty in the exploration process to bring it closer to how archaeological discovery happens in real life. Players do not know the shape or features of the hidden artifact and have to take away the earth gradually but strategically without hitting the artifact itself. From playtesting sessions with eight participants, we found that the uncertainty elements are conducive to their engagement and exploration experience. Overall, RelicVR is an innovative game that can improve players' learning motivation and outcomes of ancient artifacts. ",Kein DOI-Link verfügbar,2109.14185v1,Yes,"innovative(1), strategically(1)"
0000-0002-0359-6474,Yilin Liu,KTH Royal Institute of Technology,Deep Learning-Based Open Source Toolkit for Eosinophil Detection in   Pediatric Eosinophilic Esophagitis,1970,"  Eosinophilic Esophagitis (EoE) is a chronic, immune/antigen-mediated esophageal disease, characterized by symptoms related to esophageal dysfunction and histological evidence of eosinophil-dominant inflammation. Owing to the intricate microscopic representation of EoE in imaging, current methodologies which depend on manual identification are not only labor-intensive but also prone to inaccuracies. In this study, we develop an open-source toolkit, named Open-EoE, to perform end-to-end whole slide image (WSI) level eosinophil (Eos) detection using one line of command via Docker. Specifically, the toolkit supports three state-of-the-art deep learning-based object detection models. Furthermore, Open-EoE further optimizes the performance by implementing an ensemble learning strategy, and enhancing the precision and reliability of our results. The experimental results demonstrated that the Open-EoE toolkit can efficiently detect Eos on a testing set with 289 WSIs. At the widely accepted threshold of >= 15 Eos per high power field (HPF) for diagnosing EoE, the Open-EoE achieved an accuracy of 91%, showing decent consistency with pathologist evaluations. This suggests a promising avenue for integrating machine learning methodologies into the diagnostic process for EoE. The docker and source code has been made publicly available at https://github.com/hrlblab/Open-EoE. ",Kein DOI-Link verfügbar,2308.06333v1,Yes,intricate(1)
0000-0002-0359-6474,Yilin Liu,KTH Royal Institute of Technology,Spatial Pathomics Toolkit for Quantitative Analysis of Podocyte Nuclei   with Histology and Spatial Transcriptomics Data in Renal Pathology,1970,"  Podocytes, specialized epithelial cells that envelop the glomerular capillaries, play a pivotal role in maintaining renal health. The current description and quantification of features on pathology slides are limited, prompting the need for innovative solutions to comprehensively assess diverse phenotypic attributes within Whole Slide Images (WSIs). In particular, understanding the morphological characteristics of podocytes, terminally differentiated glomerular epithelial cells, is crucial for studying glomerular injury. This paper introduces the Spatial Pathomics Toolkit (SPT) and applies it to podocyte pathomics. The SPT consists of three main components: (1) instance object segmentation, enabling precise identification of podocyte nuclei; (2) pathomics feature generation, extracting a comprehensive array of quantitative features from the identified nuclei; and (3) robust statistical analyses, facilitating a comprehensive exploration of spatial relationships between morphological and spatial transcriptomics features.The SPT successfully extracted and analyzed morphological and textural features from podocyte nuclei, revealing a multitude of podocyte morphomic features through statistical analysis. Additionally, we demonstrated the SPT's ability to unravel spatial information inherent to podocyte distribution, shedding light on spatial patterns associated with glomerular injury. By disseminating the SPT, our goal is to provide the research community with a powerful and user-friendly resource that advances cellular spatial pathomics in renal pathology. The implementation and its complete source code of the toolkit are made openly accessible at https://github.com/hrlblab/spatial_pathomics. ",Kein DOI-Link verfügbar,2308.06288v1,Yes,"innovative(1), pivotal(1)"
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Incorporating Covariates into Integrated Factor Analysis of Multi-View   Data,1970,"  In modern biomedical research, it is ubiquitous to have multiple data sets measured on the same set of samples from different views (i.e., multi-view data). For example, in genetic studies, multiple genomic data sets at different molecular levels or from different cell types are measured for a common set of individuals to investigate genetic regulation. Integration and reduction of multi-view data have the potential to leverage information in different data sets, and to reduce the magnitude and complexity of data for further statistical analysis and interpretation. In this paper, we develop a novel statistical model, called supervised integrated factor analysis (SIFA), for integrative dimension reduction of multi-view data while incorporating auxiliary covariates. The model decomposes data into joint and individual factors, capturing the joint variation across multiple data sets and the individual variation specific to each set respectively. Moreover, both joint and individual factors are partially informed by auxiliary covariates via nonparametric models. We devise a computationally efficient Expectation-Maximization (EM) algorithm to fit the model under some identifiability conditions. We apply the method to the Genotype-Tissue Expression (GTEx) data, and provide new insights into the variation decomposition of gene expression in multiple tissues. Extensive simulation studies and an additional application to a pediatric growth study demonstrate the advantage of the proposed method over competing methods. ",Kein DOI-Link verfügbar,1703.05794v1,Yes,potent(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",One-Shot Open Affordance Learning with Foundation Models,1970,"  We introduce One-shot Open Affordance Learning (OOAL), where a model is trained with just one example per base object category, but is expected to identify novel objects and affordances. While vision-language models excel at recognizing novel objects and scenes, they often struggle to understand finer levels of granularity such as affordances. To handle this issue, we conduct a comprehensive analysis of existing foundation models, to explore their inherent understanding of affordances and assess the potential for data-limited affordance learning. We then propose a vision-language framework with simple and effective designs that boost the alignment between visual features and affordance text embeddings. Experiments on two affordance segmentation benchmarks show that the proposed method outperforms state-of-the-art models with less than 1% of the full training data, and exhibits reasonable generalization capability on unseen objects and affordances. ",Kein DOI-Link verfügbar,2311.17776v1,Yes,potent(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Lower Bound for RIP Constants and Concentration of Sum of Top Order   Statistics,1970,"  Restricted Isometry Property (RIP) is of fundamental importance in the theory of compressed sensing and forms the base of many exact and robust recovery guarantees in this field. A quantitative description of RIP involves bounding the so-called RIP constants of measurement matrices. In this respect, it is noteworthy that most results in the literature concerning RIP are upper bounds of RIP constants, which can be interpreted as a theoretical guarantee of successful sparse recovery. On the contrary, the land of lower bounds for RIP constants remains uncultivated. Lower bounds of RIP constants, if exist, can be interpreted as the fundamental limit aspect of successful sparse recovery. In this paper, the lower bound of RIP constants Gaussian random matrices are derived, along with a guide for generalization to sub-Gaussian random matrices. This provides a new proof of the fundamental limit that the minimal number of measurements needed to enforce the RIP of order $s$ is $\Omega(s\log({\rm e}N/s))$, which is more straight-forward than the classical Gelfand width argument. Furthermore, in the proof, we propose a useful technical tool featuring the concentration phenomenon for top-$k$ sum of a sequence of i.i.d. random variables, which is closely related to mainstream problems in statistics and is of independent interest. ",https://doi.org/10.1109/TSP.2020.2985848,1907.06054v1,Yes,noteworthy(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Towards Faster Non-Asymptotic Convergence for Diffusion-Based Generative   Models,1970,"  Diffusion models, which convert noise into new data instances by learning to reverse a Markov diffusion process, have become a cornerstone in contemporary generative modeling. While their practical power has now been widely recognized, the theoretical underpinnings remain far from mature. In this work, we develop a suite of non-asymptotic theory towards understanding the data generation process of diffusion models in discrete time, assuming access to $\ell_2$-accurate estimates of the (Stein) score functions. For a popular deterministic sampler (based on the probability flow ODE), we establish a convergence rate proportional to $1/T$ (with $T$ the total number of steps), improving upon past results; for another mainstream stochastic sampler (i.e., a type of the denoising diffusion probabilistic model), we derive a convergence rate proportional to $1/\sqrt{T}$, matching the state-of-the-art theory. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results characterize how $\ell_2$ score estimation errors affect the quality of the data generation processes. In contrast to prior works, our theory is developed based on an elementary yet versatile non-asymptotic approach without resorting to toolboxes for SDEs and ODEs. Further, we design two accelerated variants, improving the convergence to $1/T^2$ for the ODE-based sampler and $1/T$ for the DDPM-type sampler, which might be of independent theoretical and empirical interest. ",Kein DOI-Link verfügbar,2306.09251v3,Yes,versatile(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Breaking the Sample Complexity Barrier to Regret-Optimal Model-Free   Reinforcement Learning,1970,"  Achieving sample efficiency in online episodic reinforcement learning (RL) requires optimally balancing exploration and exploitation. When it comes to a finite-horizon episodic Markov decision process with $S$ states, $A$ actions and horizon length $H$, substantial progress has been achieved towards characterizing the minimax-optimal regret, which scales on the order of $\sqrt{H^2SAT}$ (modulo log factors) with $T$ the total number of samples. While several competing solution paradigms have been proposed to minimize regret, they are either memory-inefficient, or fall short of optimality unless the sample size exceeds an enormous threshold (e.g., $S^6A^4 \,\mathrm{poly}(H)$ for existing model-free methods).   To overcome such a large sample size barrier to efficient RL, we design a novel model-free algorithm, with space complexity $O(SAH)$, that achieves near-optimal regret as soon as the sample size exceeds the order of $SA\,\mathrm{poly}(H)$. In terms of this sample size requirement (also referred to the initial burn-in cost), our method improves -- by at least a factor of $S^5A^3$ -- upon any prior memory-efficient algorithm that is asymptotically regret-optimal. Leveraging the recently introduced variance reduction strategy (also called {\em reference-advantage decomposition}), the proposed algorithm employs an {\em early-settled} reference update rule, with the aid of two Q-learning sequences with upper and lower confidence bounds. The design principle of our early-settled variance reduction method might be of independent interest to other RL settings that involve intricate exploration-exploitation trade-offs. ",Kein DOI-Link verfügbar,2110.04645v2,Yes,intricate(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Removing Interference and Recovering Content Imaginatively for Visible   Watermark Removal,1970,"  Visible watermarks, while instrumental in protecting image copyrights, frequently distort the underlying content, complicating tasks like scene interpretation and image editing. Visible watermark removal aims to eliminate the interference of watermarks and restore the background content. However, existing methods often implement watermark component removal and background restoration tasks within a singular branch, leading to residual watermarks in the predictions and ignoring cases where watermarks heavily obscure the background. To address these limitations, this study introduces the Removing Interference and Recovering Content Imaginatively (RIRCI) framework. RIRCI embodies a two-stage approach: the initial phase centers on discerning and segregating the watermark component, while the subsequent phase focuses on background content restoration. To achieve meticulous background restoration, our proposed model employs a dual-path network capable of fully exploring the intrinsic background information beneath semi-transparent watermarks and peripheral contextual information from unaffected regions. Moreover, a Global and Local Context Interaction module is built upon multi-layer perceptrons and bidirectional feature transformation for comprehensive representation modeling in the background restoration phase. The efficacy of our approach is empirically validated across two large-scale datasets, and our findings reveal a marked enhancement over existing watermark removal techniques. ",Kein DOI-Link verfügbar,2312.14383v1,Yes,meticulous(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Unleashing the Denoising Capability of Diffusion Prior for Solving   Inverse Problems,1970,"  The recent emergence of diffusion models has significantly advanced the precision of learnable priors, presenting innovative avenues for addressing inverse problems. Since inverse problems inherently entail maximum a posteriori estimation, previous works have endeavored to integrate diffusion priors into the optimization frameworks. However, prevailing optimization-based inverse algorithms primarily exploit the prior information within the diffusion models while neglecting their denoising capability. To bridge this gap, this work leverages the diffusion process to reframe noisy inverse problems as a two-variable constrained optimization task by introducing an auxiliary optimization variable. By employing gradient truncation, the projection gradient descent method is efficiently utilized to solve the corresponding optimization problem. The proposed algorithm, termed ProjDiff, effectively harnesses the prior information and the denoising capability of a pre-trained diffusion model within the optimization framework. Extensive experiments on the image restoration tasks and source separation and partial generation tasks demonstrate that ProjDiff exhibits superior performance across various linear and nonlinear inverse problems, highlighting its potential for practical applications. Code is available at https://github.com/weigerzan/ProjDiff/. ",Kein DOI-Link verfügbar,2406.06959v1,Yes,"innovative(1), potent(1)"
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",A Sharp Convergence Theory for The Probability Flow ODEs of Diffusion   Models,1970,"  Diffusion models, which convert noise into new data instances by learning to reverse a diffusion process, have become a cornerstone in contemporary generative modeling. In this work, we develop non-asymptotic convergence theory for a popular diffusion-based sampler (i.e., the probability flow ODE sampler) in discrete time, assuming access to $\ell_2$-accurate estimates of the (Stein) score functions. For distributions in $\mathbb{R}^d$, we prove that $d/\varepsilon$ iterations -- modulo some logarithmic and lower-order terms -- are sufficient to approximate the target distribution to within $\varepsilon$ total-variation distance. This is the first result establishing nearly linear dimension-dependency (in $d$) for the probability flow ODE sampler. Imposing only minimal assumptions on the target data distribution (e.g., no smoothness assumption is imposed), our results also characterize how $\ell_2$ score estimation errors affect the quality of the data generation processes. In contrast to prior works, our theory is developed based on an elementary yet versatile non-asymptotic approach without the need of resorting to SDE and ODE toolboxes. ",Kein DOI-Link verfügbar,2408.02320v1,Yes,versatile(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Sample-Efficient Reinforcement Learning Is Feasible for Linearly   Realizable MDPs with Limited Revisiting,1970,"  Low-complexity models such as linear function representation play a pivotal role in enabling sample-efficient reinforcement learning (RL). The current paper pertains to a scenario with value-based linear representation, which postulates the linear realizability of the optimal Q-function (also called the ""linear $Q^{\star}$ problem""). While linear realizability alone does not allow for sample-efficient solutions in general, the presence of a large sub-optimality gap is a potential game changer, depending on the sampling mechanism in use. Informally, sample efficiency is achievable with a large sub-optimality gap when a generative model is available but is unfortunately infeasible when we turn to standard online RL settings.   In this paper, we make progress towards understanding this linear $Q^{\star}$ problem by investigating a new sampling protocol, which draws samples in an online/exploratory fashion but allows one to backtrack and revisit previous states in a controlled and infrequent manner. This protocol is more flexible than the standard online RL setting, while being practically relevant and far more restrictive than the generative model. We develop an algorithm tailored to this setting, achieving a sample complexity that scales polynomially with the feature dimension, the horizon, and the inverse sub-optimality gap, but not the size of the state/action space. Our findings underscore the fundamental interplay between sampling protocols and low-complexity structural representation in RL. ",Kein DOI-Link verfügbar,2105.08024v2,Yes,"pivotal(1), potent(1)"
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",HT-eQTL: Integrative Expression Quantitative Trait Loci Analysis in a   Large Number of Human Tissues,1970,"  Expression quantitative trait loci (eQTL) analysis identifies genetic markers associated with the expression of a gene. Most existing eQTL analyses and methods investigate association in a single, readily available tissue, such as blood. Joint analysis of eQTL in multiple tissues has the potential to improve, and expand the scope of, single-tissue analyses. Large-scale collaborative efforts such as the Genotype-Tissue Expression (GTEx) program are currently generating high quality data in a large number of tissues. However, computational constraints limit genome-wide multi-tissue eQTL analysis. We develop an integrative method under a hierarchical Bayesian framework for eQTL analysis in a large number of tissues. The model fitting procedure is highly scalable, and the computing time is a polynomial function of the number of tissues. Multi-tissue eQTLs are identified through a local false discovery rate approach, which rigorously controls the false discovery rate. Using simulation and GTEx real data studies, we show that the proposed method has superior performance to existing methods in terms of computing time and the power of eQTL discovery. We provide a scalable method for eQTL analysis in a large number of tissues. The method enables the identification of eQTL with different configurations and facilitates the characterization of tissue specificity. ",Kein DOI-Link verfügbar,1701.05426v3,Yes,potent(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Learning Precise Affordances from Egocentric Videos for Robotic   Manipulation,1970,"  Affordance, defined as the potential actions that an object offers, is crucial for robotic manipulation tasks. A deep understanding of affordance can lead to more intelligent AI systems. For example, such knowledge directs an agent to grasp a knife by the handle for cutting and by the blade when passing it to someone. In this paper, we present a streamlined affordance learning system that encompasses data collection, effective model training, and robot deployment. First, we collect training data from egocentric videos in an automatic manner. Different from previous methods that focus only on the object graspable affordance and represent it as coarse heatmaps, we cover both graspable (e.g., object handles) and functional affordances (e.g., knife blades, hammer heads) and extract data with precise segmentation masks. We then propose an effective model, termed Geometry-guided Affordance Transformer (GKT), to train on the collected data. GKT integrates an innovative Depth Feature Injector (DFI) to incorporate 3D shape and geometric priors, enhancing the model's understanding of affordances. To enable affordance-oriented manipulation, we further introduce Aff-Grasp, a framework that combines GKT with a grasp generation model. For comprehensive evaluation, we create an affordance evaluation dataset with pixel-wise annotations, and design real-world tasks for robot experiments. The results show that GKT surpasses the state-of-the-art by 15.9% in mIoU, and Aff-Grasp achieves high success rates of 95.5% in affordance prediction and 77.1% in successful grasping among 179 trials, including evaluations with seen, unseen objects, and cluttered scenes. ",Kein DOI-Link verfügbar,2408.10123v1,Yes,"innovative(1), potent(1)"
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",High-probability sample complexities for policy evaluation with linear   function approximation,1970,"  This paper is concerned with the problem of policy evaluation with linear function approximation in discounted infinite horizon Markov decision processes. We investigate the sample complexities required to guarantee a predefined estimation error of the best linear coefficients for two widely-used policy evaluation algorithms: the temporal difference (TD) learning algorithm and the two-timescale linear TD with gradient correction (TDC) algorithm. In both the on-policy setting, where observations are generated from the target policy, and the off-policy setting, where samples are drawn from a behavior policy potentially different from the target policy, we establish the first sample complexity bound with high-probability convergence guarantee that attains the optimal dependence on the tolerance level. We also exhihit an explicit dependence on problem-related quantities, and show in the on-policy setting that our upper bound matches the minimax lower bound on crucial problem parameters, including the choice of the feature maps and the problem dimension. ",Kein DOI-Link verfügbar,2305.19001v2,Yes,potent(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Watt For What: Rethinking Deep Learning's Energy-Performance   Relationship,1970,"  Deep learning models have revolutionized various fields, from image recognition to natural language processing, by achieving unprecedented levels of accuracy. However, their increasing energy consumption has raised concerns about their environmental impact, disadvantaging smaller entities in research and exacerbating global energy consumption. In this paper, we explore the trade-off between model accuracy and electricity consumption, proposing a metric that penalizes large consumption of electricity. We conduct a comprehensive study on the electricity consumption of various deep learning models across different GPUs, presenting a detailed analysis of their accuracy-efficiency trade-offs. By evaluating accuracy per unit of electricity consumed, we demonstrate how smaller, more energy-efficient models can significantly expedite research while mitigating environmental concerns. Our results highlight the potential for a more sustainable approach to deep learning, emphasizing the importance of optimizing models for efficiency. This research also contributes to a more equitable research landscape, where smaller entities can compete effectively with larger counterparts. This advocates for the adoption of efficient deep learning practices to reduce electricity consumption, safeguarding the environment for future generations whilst also helping ensure a fairer competitive landscape. ",Kein DOI-Link verfügbar,2310.06522v1,Yes,potent(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Log-Contrast Regression with Functional Compositional Predictors:   Linking Preterm Infant's Gut Microbiome Trajectories to Neurobehavioral   Outcome,1970,"  The neonatal intensive care unit (NICU) experience is known to be one of the most crucial factors that drive preterm infant's neurodevelopmental and health outcomes. It is hypothesized that stressful early life experience of very preterm neonate is imprinting gut microbiome by the regulation of the so-called brain-gut axis, and consequently, certain microbiome markers are predictive of later infant neurodevelopment. To investigate, a preterm infant study was conducted; infant fecal samples were collected during the infants' first month of postnatal age, resulting in functional compositional microbiome data, and neurobehavioral outcomes were measured when infants reached 36-38 weeks of post-menstrual age. To identify potential microbiome markers and estimate how the trajectories of gut microbiome compositions during early postnatal stage impact later neurobehavioral outcomes of the preterm infants, we innovate a sparse log-contrast regression with functional compositional predictors. The functional simplex structure is strictly preserved, and the functional compositional predictors are allowed to have sparse, smoothly varying, and accumulating effects on the outcome through time. Through a pragmatic basis expansion step, the problem boils down to a linearly constrained sparse group regression, for which we develop an efficient algorithm and obtain theoretical performance guarantees. Our approach yields insightful results in the preterm infant study. The identified microbiome markers and the estimated time dynamics of their impact on the neurobehavioral outcome shed light on the linkage between stress accumulation in early postnatal stage and neurodevelopmental process of infants. ",Kein DOI-Link verfügbar,1808.02403v2,Yes,potent(1)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Boosting current-induced molecular dynamics with machine-learning   potential,1970,"  In a current-carrying single-molecular junction (SMJ), a hierarchy of hybrid energy transport processes takes place under a highly nonequilibrium situation, including energy transfer from electrons to molecular vibrations via electron-vibration interaction, energy redistribution within different vibrational modes via anharmonic coupling, and eventual energy transport to surrounding electrodes. A comprehensive understanding of such processes is a prerequisite for their potential applications as single-molecular devices. $Ab$ $initio$ current-induced molecular dynamics (MD) is an ideal approach to address this complicated problem. But the computational cost hinders its usage in systematic study of realistic SMJs. Here, we achieve orders of magnitude improvement in the speed of MD simulation by employing machine-learning potential with accuracy comparable to density functional theory. Using this approach, we show that SMJs with graphene electrodes generate order of magnitude less heating than those with gold electrodes. Our work illustrates the superior heat transport property of graphene as electrodes for SMJs, thanks to its better phonon spectral overlap with molecular vibrations. ",https://doi.org/10.1063/5.0118952,2206.05068v1,Yes,potent(2)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",An Empirical Bayes Approach for Multiple Tissue eQTL Analysis,1970,"  Expression quantitative trait loci (eQTL) analyses, which identify genetic markers associated with the expression of a gene, are an important tool in the understanding of diseases in human and other populations. While most eQTL studies to date consider the connection between genetic variation and expression in a single tissue, complex, multi-tissue data sets are now being generated by the GTEx initiative. These data sets have the potential to improve the findings of single tissue analyses by borrowing strength across tissues, and the potential to elucidate the genotypic basis of differences between tissues.   In this paper we introduce and study a multivariate hierarchical Bayesian model (MT-eQTL) for multi-tissue eQTL analysis. MT-eQTL directly models the vector of correlations between expression and genotype across tissues. It explicitly captures patterns of variation in the presence or absence of eQTLs, as well as the heterogeneity of effect sizes across tissues. Moreover, the model is applicable to complex designs in which the set of donors can (i) vary from tissue to tissue, and (ii) exhibit incomplete overlap between tissues. The MT-eQTL model is marginally consistent, in the sense that the model for a subset of tissues can be obtained from the full model via marginalization. Fitting of the MT-eQTL model is carried out via empirical Bayes, using an approximate EM algorithm. Inferences concerning eQTL detection and the configuration of eQTLs across tissues are derived from adaptive thresholding of local false discovery rates, and maximum a-posteriori estimation, respectively. We investigate the MT-eQTL model through a simulation study, and rigorously establish the FDR control of the local FDR testing procedure under mild assumptions appropriate for dependent data. ",Kein DOI-Link verfügbar,1311.2948v5,Yes,potent(2)
0009-0000-2151-595X,Gen Li,"KU Leuven, KU Leuven Association",Dynamic Sparsity Is Channel-Level Sparsity Learner,1970,"  Sparse training has received an upsurging interest in machine learning due to its tantalizing saving potential for the entire training process as well as inference. Dynamic sparse training (DST), as a leading sparse training approach, can train deep neural networks at high sparsity from scratch to match the performance of their dense counterparts. However, most if not all DST prior arts demonstrate their effectiveness on unstructured sparsity with highly irregular sparse patterns, which receives limited support in common hardware. This limitation hinders the usage of DST in practice. In this paper, we propose Channel-aware dynamic sparse (Chase), which for the first time seamlessly translates the promise of unstructured dynamic sparsity to GPU-friendly channel-level sparsity (not fine-grained N:M or group sparsity) during one end-to-end training process, without any ad-hoc operations. The resulting small sparse networks can be directly accelerated by commodity hardware, without using any particularly sparsity-aware hardware accelerators. This appealing outcome is partially motivated by a hidden phenomenon of dynamic sparsity: off-the-shelf unstructured DST implicitly involves biased parameter reallocation across channels, with a large fraction of channels (up to 60%) being sparser than others. By progressively identifying and removing these channels during training, our approach translates unstructured sparsity to channel-wise sparsity. Our experimental results demonstrate that Chase achieves 1.7 X inference throughput speedup on common GPU devices without compromising accuracy with ResNet-50 on ImageNet. We release our codes in https://github.com/luuyin/chase. ",Kein DOI-Link verfügbar,2305.19454v2,Yes,potent(1)
0009-0009-6519-0291,Gorjan Radevski,"KU Leuven, KU Leuven Association",Estimating calibration error under label shift without labels,1970,"  In the face of dataset shift, model calibration plays a pivotal role in ensuring the reliability of machine learning systems. Calibration error (CE) is an indicator of the alignment between the predicted probabilities and the classifier accuracy. While prior works have delved into the implications of dataset shift on calibration, existing CE estimators assume access to labels from the target domain, which are often unavailable in practice, i.e., when the model is deployed and used. This work addresses such challenging scenario, and proposes a novel CE estimator under label shift, which is characterized by changes in the marginal label distribution $p(Y)$, while keeping the conditional $p(X|Y)$ constant between the source and target distributions. Our contribution is an approach, which, by leveraging importance re-weighting of the labeled source distribution, provides consistent and asymptotically unbiased CE estimation with respect to the shifted target distribution. Empirical results across diverse real-world datasets, under various conditions and label-shift intensities, demonstrate the effectiveness and reliability of the proposed estimator. ",Kein DOI-Link verfügbar,2312.08586v1,Yes,pivotal(1)
0000-0001-8844-2126,Miryam de Lhoneux,"KU Leuven, KU Leuven Association",Nightmare at test time: How punctuation prevents parsers from   generalizing,1970,"  Punctuation is a strong indicator of syntactic structure, and parsers trained on text with punctuation often rely heavily on this signal. Punctuation is a diversion, however, since human language processing does not rely on punctuation to the same extent, and in informal texts, we therefore often leave out punctuation. We also use punctuation ungrammatically for emphatic or creative purposes, or simply by mistake. We show that (a) dependency parsers are sensitive to both absence of punctuation and to alternative uses; (b) neural parsers tend to be more sensitive than vintage parsers; (c) training neural parsers without punctuation outperforms all out-of-the-box parsers across all scenarios where punctuation departs from standard punctuation. Our main experiments are on synthetically corrupted data to study the effect of punctuation in isolation and avoid potential confounds, but we also show effects on out-of-domain data. ",Kein DOI-Link verfügbar,1809.00070v1,Yes,potent(1)
0000-0001-8844-2126,Miryam de Lhoneux,"KU Leuven, KU Leuven Association","Parsing with Pretrained Language Models, Multiple Datasets, and Dataset   Embeddings",1970,"  With an increase of dataset availability, the potential for learning from a variety of data sources has increased. One particular method to improve learning from multiple data sources is to embed the data source during training. This allows the model to learn generalizable features as well as distinguishing features between datasets. However, these dataset embeddings have mostly been used before contextualized transformer-based embeddings were introduced in the field of Natural Language Processing. In this work, we compare two methods to embed datasets in a transformer-based multilingual dependency parser, and perform an extensive evaluation. We show that: 1) embedding the dataset is still beneficial with these models 2) performance increases are highest when embedding the dataset at the encoder level 3) unsurprisingly, we confirm that performance increases are highest for small datasets and datasets with a low baseline score. 4) we show that training on the combination of all datasets performs similarly to designing smaller clusters based on language-relatedness. ",Kein DOI-Link verfügbar,2112.03625v1,Yes,potent(1)
0000-0001-8844-2126,Miryam de Lhoneux,"KU Leuven, KU Leuven Association",Challenges and Strategies in Cross-Cultural NLP,1970,"  Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies. ",Kein DOI-Link verfügbar,2203.10020v1,Yes,potent(1)
0000-0001-8844-2126,Miryam de Lhoneux,"KU Leuven, KU Leuven Association",CreoleVal: Multilingual Multitask Benchmarks for Creoles,1970,"  Creoles represent an under-explored and marginalized group of languages, with few available resources for NLP research.While the genealogical ties between Creoles and a number of highly-resourced languages imply a significant potential for transfer learning, this potential is hampered due to this lack of annotated data. In this work we present CreoleVal, a collection of benchmark datasets spanning 8 different NLP tasks, covering up to 28 Creole languages; it is an aggregate of novel development datasets for reading comprehension, relation classification, and machine translation for Creoles, in addition to a practical gateway to a handful of preexisting benchmarks. For each benchmark, we conduct baseline experiments in a zero-shot setting in order to further ascertain the capabilities and limitations of transfer learning for Creoles. Ultimately, we see CreoleVal as an opportunity to empower research on Creoles in NLP and computational linguistics, and in general, a step towards more equitable language technology around the globe. ",Kein DOI-Link verfügbar,2310.19567v3,Yes,potent(2)
0000-0003-2995-8303,Matthew Jones,The University of Sheffield,Cross-Frequency Coupling of Neuronal Oscillations During Cognition,1970,"  How the brain co-ordinates the actions of distant regions in an efficient manner is an open problem. Many believe that cross-frequency coupling between the amplitude of high frequency local field potential oscillations in one region and the phase of lower frequency signals in another may form a possible mechanism. This work provides a preliminary study from both an experimental and theoretical viewpoint, concentrating on possible coupling between the hippocampus and pre-frontal cortex in rats during tasks involving working memory, spatial navigation and decision making processes. Attempts to search for such coupling events are made using newly developed MATLAB scripts. This leads to the discovery of increased envelope-to-signal correlation (ESC) between the 1-10 Hz hippocampus theta and 30-40 Hz pre-fontal cortex gamma bands when a choice turn is approached during a T-maze task. From a theoretical perspective, a standard connectionist modelling approach is extended to allow for the formation of oscillations. Although detrimental to overall average task performance, this did lead to a reduced increase in performance variation as noise was increased, when compared to a standard non-oscillating model. ",Kein DOI-Link verfügbar,1604.00069v1,Yes,potent(1)
0000-0003-2995-8303,Matthew Jones,The University of Sheffield,"Developing, Analyzing, and Evaluating Vehicular Lane Keeping Algorithms   Under Dynamic Lighting and Weather Conditions Using Electric Vehicles",1970,"  Self-driving vehicles have the potential to reduce accidents and fatalities on the road. Many production vehicles already come equipped with basic self-driving capabilities, but have trouble following lanes in adverse lighting and weather conditions. Therefore, we develop, analyze, and evaluate two vehicular lane-keeping algorithms under dynamic weather conditions using a combined deep learning- and hand-crafted approach and an end-to-end deep learning approach. We use image segmentation- and linear-regression based deep learning to drive the vehicle toward the center of the lane, measuring the amount of laps completed, average speed, and average steering error per lap. Our hybrid model completes more laps than our end-to-end deep learning model. In the future, we are interested in combining our algorithms to form one cohesive approach to lane-following. ",Kein DOI-Link verfügbar,2406.06899v1,Yes,potent(1)
0000-0003-2995-8303,Matthew Jones,The University of Sheffield,Controlled Single-Photon Emission from a Single Trapped Two-Level Atom,1970,"  By illuminating an individual rubidium atom stored in a tight optical tweezer with short resonant light pulses, we create an efficient triggered source of single photons with a well-defined polarization. The measured intensity correlation of the emitted light pulses exhibits almost perfect antibunching. Such a source of high rate, fully controlled single photon pulses has many potential applications for quantum information processing. ",https://doi.org/10.1126/science.1113394,quant-ph/0610087v1,Yes,potent(1)
0000-0003-3419-1813,Huayang Li,The University of Sheffield,A Survey on Retrieval-Augmented Text Generation,1970,"  Recently, retrieval-augmented text generation attracted increasing attention of the computational linguistics community. Compared with conventional generation models, retrieval-augmented text generation has remarkable advantages and particularly has achieved state-of-the-art performance in many NLP tasks. This paper aims to conduct a survey about retrieval-augmented text generation. It firstly highlights the generic paradigm of retrieval-augmented generation, and then it reviews notable approaches according to different tasks including dialogue response generation, machine translation, and other generation tasks. Finally, it points out some important directions on top of recent methods to facilitate future research. ",Kein DOI-Link verfügbar,2202.01110v2,Yes,notable(1)
0000-0003-3419-1813,Huayang Li,The University of Sheffield,Investigating Data Variance in Evaluations of Automatic Machine   Translation Metrics,1970,"  Current practices in metric evaluation focus on one single dataset, e.g., Newstest dataset in each year's WMT Metrics Shared Task. However, in this paper, we qualitatively and quantitatively show that the performances of metrics are sensitive to data. The ranking of metrics varies when the evaluation is conducted on different datasets. Then this paper further investigates two potential hypotheses, i.e., insignificant data points and the deviation of Independent and Identically Distributed (i.i.d) assumption, which may take responsibility for the issue of data variance. In conclusion, our findings suggest that when evaluating automatic translation metrics, researchers should take data variance into account and be cautious to claim the result on a single dataset, because it may leads to inconsistent results with most of other datasets. ",Kein DOI-Link verfügbar,2203.15858v2,Yes,potent(1)
0000-0003-3419-1813,Huayang Li,The University of Sheffield,TranSmart: A Practical Interactive Machine Translation System,1970,"  Automatic machine translation is super efficient to produce translations yet their quality is not guaranteed. This technique report introduces TranSmart, a practical human-machine interactive translation system that is able to trade off translation quality and efficiency. Compared to existing publicly available interactive translation systems, TranSmart supports three key features, word-level autocompletion, sentence-level autocompletion and translation memory. By word-level and sentence-level autocompletion, TranSmart allows users to interactively translate words in their own manners rather than the strict manner from left to right. In addition, TranSmart has the potential to avoid similar translation mistakes by using translated sentences in history as its memory. This report presents major functions of TranSmart, algorithms for achieving these functions, how to use the TranSmart APIs, and evaluation results of some key functions. TranSmart is publicly available at its homepage (https://transmart.qq.com). ",Kein DOI-Link verfügbar,2105.13072v1,Yes,potent(1)
0000-0003-3419-1813,Huayang Li,The University of Sheffield,M3T: A New Benchmark Dataset for Multi-Modal Document-Level Machine   Translation,1970,"  Document translation poses a challenge for Neural Machine Translation (NMT) systems. Most document-level NMT systems rely on meticulously curated sentence-level parallel data, assuming flawless extraction of text from documents along with their precise reading order. These systems also tend to disregard additional visual cues such as the document layout, deeming it irrelevant. However, real-world documents often possess intricate text layouts that defy these assumptions. Extracting information from Optical Character Recognition (OCR) or heuristic rules can result in errors, and the layout (e.g., paragraphs, headers) may convey relationships between distant sections of text. This complexity is particularly evident in widely used PDF documents, which represent information visually. This paper addresses this gap by introducing M3T, a novel benchmark dataset tailored to evaluate NMT systems on the comprehensive task of translating semi-structured documents. This dataset aims to bridge the evaluation gap in document-level NMT systems, acknowledging the challenges posed by rich text layouts in real-world applications. ",Kein DOI-Link verfügbar,2406.08255v1,Yes,"meticulous(1), intricate(1), meticulously(1)"
0000-0003-3419-1813,Huayang Li,The University of Sheffield,GPT4Video: A Unified Multimodal Large Language Model for   lnstruction-Followed Understanding and Safety-Aware Generation,1970,"  While the recent advances in Multimodal Large Language Models (MLLMs) constitute a significant leap forward in the field, these models are predominantly confined to the realm of input-side multimodal comprehension, lacking the capacity for multimodal content generation. To fill this gap, we present GPT4Video, a unified multi-model framework that empowers Large Language Models (LLMs) with the capability of both video understanding and generation. Specifically, we develop an instruction-following-based approach integrated with the stable diffusion generative model, which has demonstrated to effectively and securely handle video generation scenarios. GPT4Video offers the following benefits: 1) It exhibits impressive capabilities in both video understanding and generation scenarios. For example, GPT4Video outperforms Valley by 11.8\% on the Video Question Answering task, and surpasses NExt-GPT by 2.3\% on the Text to Video generation task. 2) it endows the LLM/MLLM with video generation capabilities without requiring additional training parameters and can flexibly interface with a wide range of models to perform video generation. 3) it maintains a safe and healthy conversation not only in output-side but also the input side in an end-to-end manner. Qualitative and qualitative experiments demonstrate that GPT4Video holds the potential to function as a effective, safe and Humanoid-like video assistant that can handle both video understanding and generation scenarios. ",Kein DOI-Link verfügbar,2311.16511v1,Yes,potent(1)
0000-0003-3419-1813,Huayang Li,The University of Sheffield,"On the Transformations across Reward Model, Parameter Update, and   In-Context Prompt",1970,"  Despite the general capabilities of pre-trained large language models (LLMs), they still need further adaptation to better serve practical applications. In this paper, we demonstrate the interchangeability of three popular and distinct adaptation tools: parameter updating, reward modeling, and in-context prompting. This interchangeability establishes a triangular framework with six transformation directions, each of which facilitates a variety of applications. Our work offers a holistic view that unifies numerous existing studies and suggests potential research directions. We envision our work as a useful roadmap for future research on LLMs. ",Kein DOI-Link verfügbar,2406.16377v1,Yes,potent(1)
0000-0002-3007-1673,Stephen Potter,The University of Sheffield,New short period stellar pulsators at large Galactocentric distances,1970,"  We report the discovery of 31 blue, short period, pulsators made using data taken as part of the Rapid Temporal Survey (RATS). We find they have periods between 51-83 mins and full-amplitudes between 0.05-0.65 mag. Using the period-luminosity relationship for short period pulsating stars we determine their distance. Assuming they are pulsating in either the fundamental or first over-tone radial mode the majority are located at a distance greater than 3kpc, with several being more than 20 kpc distant. Most stars are at least 1 kpc from the Galactic plane, with three being more than 10 kpc. One is located in the direction of the Galactic anti-center and has Galactocentric distance of ~30 kpc and is ~20 kpc below the plane: they are therefore potential tracers of Galactic structure. We have obtained low-resolution spectra for a small number our targets and find they have temperatures between 7200--7900K and a metal content less than Solar. The colours of the pulsators and the spectral fits to those stars for which we have spectra indicate that they are either SX Phe or delta Scuti stars. We estimate the number of SX Phe stars in our Galaxy and find significantly fewer per unit mass than reported in massive globular clusters or dwarf spheroidal galaxies. ",https://doi.org/10.1111/j.1365-2966.2011.19275.x,1106.3232v1,Yes,potent(1)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Equational theories of idempotent semifields,1970,"  This paper provides answers to several open problems about equational theories of idempotent semifields. In particular, it is proved that (i) no equational theory of a non-trivial class of idempotent semifields has a finite basis; (ii) there are continuum-many equational theories of classes of idempotent semifields; and (iii) the equational theory of the class of idempotent semifields is co-NP-complete. ",Kein DOI-Link verfügbar,2402.09876v1,Yes,potent(4)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Exact Unification,1970,"  A new hierarchy of ""exact"" unification types is introduced, motivated by the study of admissibility for equational classes and non-classical logics. In this setting, unifiers of identities in an equational class are preordered, not by instantiation, but rather by inclusion over the corresponding sets of unified identities. Minimal complete sets of unifiers under this new preordering always have a smaller or equal cardinality than those provided by the standard instantiation preordering, and in significant cases a dramatic reduction may be observed. In particular, the classes of distributive lattices, idempotent semigroups, and MV-algebras, which all have nullary unification type, have unitary or finitary exact type. These results are obtained via an algebraic interpretation of exact unification, inspired by Ghilardi's algebraic approach to equational unification. ",Kein DOI-Link verfügbar,1410.5583v1,Yes,potent(1)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Exact Unification and Admissibility,1970,"  A new hierarchy of ""exact"" unification types is introduced, motivated by the study of admissible rules for equational classes and non-classical logics. In this setting, unifiers of identities in an equational class are preordered, not by instantiation, but rather by inclusion over the corresponding sets of unified identities. Minimal complete sets of unifiers under this new preordering always have a smaller or equal cardinality than those provided by the standard instantiation preordering, and in significant cases a dramatic reduction may be observed. In particular, the classes of distributive lattices, idempotent semigroups, and MV-algebras, which all have nullary unification type, have unitary or finitary exact type. These results are obtained via an algebraic interpretation of exact unification, inspired by Ghilardi's algebraic approach to equational unification. ",https://doi.org/10.2168/LMCS-11(3:23)2015,1508.04360v2,Yes,potent(1)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Structure theorems for idempotent residuated lattices,1970,"  In this paper we study structural properties of residuated lattices that are idempotent as monoids. We provide descriptions of the totally ordered members of this class and obtain counting theorems for the number of finite algebras in various subclasses. We also establish the finite embeddability property for certain varieties generated by classes of residuated lattices that are conservative in the sense that monoid multiplication always yields one of its arguments. We then make use of a more symmetric version of Raftery's characterization theorem for totally ordered commutative idempotent residuated lattices to prove that the variety generated by this class has the amalgamation property. Finally, we address an open problem in the literature by giving an example of a noncommutative variety of idempotent residuated lattices that has the amalgamation property. ",Kein DOI-Link verfügbar,2004.09553v1,Yes,potent(3)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Transfer theorems for finitely subdirectly irreducible algebras,1970,"  We show that under certain conditions, well-studied algebraic properties transfer from the class $\mathcal{Q}_{_\text{RFSI}}$ of the relatively finitely subdirectly irreducible members of a quasivariety $\mathcal{Q}$ to the whole quasivariety, and, in certain cases, back again. First, we prove that if $\mathcal{Q}$ is relatively congruence-distributive, then it has the $\mathcal{Q}$-congruence extension property if and only if $\mathcal{Q}_{_\text{RFSI}}$ has this property. We then prove that if $\mathcal{Q}$ has the $\mathcal{Q}$-congruence extension property and $\mathcal{Q}_{_\text{RFSI}}$ is closed under subalgebras, then $\mathcal{Q}$ has a one-sided amalgamation property (equivalently, for $\mathcal{Q}$, the amalgamation property) if and only if $\mathcal{Q}_{_\text{RFSI}}$ has this property. We also establish similar results for the transferable injections property and strong amalgamation property. For each property considered, we specialize our results to the case where $\mathcal{Q}$ is a variety -- so that $\mathcal{Q}_{_\text{RFSI}}$ is the class of finitely subdirectly irreducible members of $\mathcal{Q}$ and the $\mathcal{Q}$-congruence extension property is the usual congruence extension property -- and prove that when $\mathcal{Q}$ is finitely generated and congruence-distributive, and $\mathcal{Q}_{_\text{RFSI}}$ is closed under subalgebras, possession of the property is decidable. Finally, as a case study, we provide a complete description of the subvarieties of a notable variety of BL-algebras that have the amalgamation property. ",Kein DOI-Link verfügbar,2205.05148v3,Yes,notable(1)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Deciding Equations in the Time Warp Algebra,1970,"  Join-preserving maps on the discrete time scale $\omega^+$, referred to as time warps, have been proposed as graded modalities that can be used to quantify the growth of information in the course of program execution. The set of time warps forms a simple distributive involutive residuated lattice -- called the time warp algebra -- that is equipped with residual operations relevant to potential applications. In this paper, we show that although the time warp algebra generates a variety that lacks the finite model property, it nevertheless has a decidable equational theory. We also describe an implementation of a procedure for deciding equations in this algebra, written in the OCaml programming language, that makes use of the Z3 theorem prover. ",https://doi.org/10.46298/lmcs-20(1:8)2024,2302.04668v4,Yes,potent(1)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Theorems of Alternatives for Substructural Logics,1970,"  A theorem of alternatives provides a reduction of validity in a substructural logic to validity in its multiplicative fragment. Notable examples include a theorem of Arnon Avron that reduces the validity of a disjunction of multiplicative formulas in the R-mingle logic RM to the validity of a linear combination of these formulas, and Gordan's theorem for solutions of linear systems over the real numbers, that yields an analogous reduction for validity in Abelian logic A. In this paper, general conditions are provided for axiomatic extensions of involutive uninorm logic without additive constants to admit a theorem of alternatives. It is also shown that a theorem of alternatives for a logic can be used to establish (uniform) deductive interpolation and completeness with respect to a class of dense totally ordered residuated lattices. ",Kein DOI-Link verfügbar,2002.11419v1,Yes,notable(1)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,Interpolation and the Exchange Rule,1970,"  It was proved by Maksimova in 1977 that exactly eight varieties of Heyting algebras have the amalgamation property, and hence exactly eight axiomatic extensions of intuitionistic propositional logic have the deductive interpolation property. The prevalence of the deductive interpolation property for axiomatic extensions of substructural logics and the amalgamation property for varieties of pointed residuated lattices, their equivalent algebraic semantics, is far less well understood, however. Taking as our starting point a formulation of intuitionistic propositional logic as the full Lambek calculus with exchange, weakening, and contraction, we investigate the role of the exchange rule--algebraically, the commutativity law--in determining the scope of these properties. First, we show that there are continuum-many varieties of idempotent semilinear residuated lattices that have the amalgamation property and contain non-commutative members, and hence continuum-many axiomatic extensions of the corresponding logic that have the deductive interpolation property in which exchange is not derivable. We then show that, in contrast, exactly sixty varieties of commutative idempotent semilinear residuated lattices have the amalgamation property, and hence exactly sixty axiomatic extensions of the corresponding logic with exchange have the deductive interpolation property. From this latter result, it follows also that there are exactly sixty varieties of commutative idempotent semilinear residuated lattices whose first-order theories have a model completion. ",Kein DOI-Link verfügbar,2310.14953v1,Yes,potent(3)
0000-0001-7444-7137,George Metcalfe,The University of Sheffield,"Time Warps, from Algebra to Algorithms",1970,"  Graded modalities have been proposed in recent work on programming languages as a general framework for refining type systems with intensional properties. In particular, continuous endomaps of the discrete time scale, or time warps, can be used to quantify the growth of information in the course of program execution. Time warps form a complete residuated lattice, with the residuals playing an important role in potential programming applications. In this paper, we study the algebraic structure of time warps, and prove that their equational theory is decidable, a necessary condition for their use in real-world compilers. We also describe how our universal-algebraic proof technique lends itself to a constraint-based implementation, establishing a new link between universal algebra and verification technology. ",Kein DOI-Link verfügbar,2106.06205v2,Yes,potent(1)
0000-0002-6608-330X,Pieter Kok,The University of Sheffield,Quantum Imaging and Metrology,1970,"  The manipulation of quantum entanglement has found enormous potential for improving performances of devices such as gyroscopes, clocks, and even computers. Similar improvements have been demonstrated for lithography and microscopy. We present an overview of some aspects of enhancement by quantum entanglement in imaging and metrology. ",Kein DOI-Link verfügbar,quant-ph/0306113v1,Yes,potent(1)
0000-0002-6608-330X,Pieter Kok,The University of Sheffield,Quantum interferometric optical lithography:towards arbitrary   two-dimensional patterns,1970,"  As demonstrated by Boto et al. [Phys. Rev. Lett. 85, 2733 (2000)], quantum lithography offers an increase in resolution below the diffraction limit. Here, we generalize this procedure in order to create patterns in one and two dimensions. This renders quantum lithography a potentially useful tool in nanotechnology. ",https://doi.org/10.1103/PhysRevA.63.063407,quant-ph/0011088v2,Yes,potent(1)
0000-0002-6608-330X,Pieter Kok,The University of Sheffield,Quantum Interferometric Sensors,1970,"  Quantum entanglement has the potential to revolutionize the entire field of interferometric sensing by providing many orders of magnitude improvement in interferometer sensitivity. The quantum-entangled particle interferometer approach is very general and applies to many types of interferometers. In particular, without nonlocal entanglement, a generic classical interferometer has a statistical-sampling shot-noise limited sensitivity that scales like $1/\sqrt{N}$, where $N$ is the number of particles passing through the interferometer per unit time. However, if carefully prepared quantum correlations are engineered between the particles, then the interferometer sensitivity improves by a factor of $\sqrt{N}$ to scale like 1/N, which is the limit imposed by the Heisenberg Uncertainty Principle. For optical interferometers operating at milliwatts of optical power, this quantum sensitivity boost corresponds to an eight-order-of-magnitude improvement of signal to noise. This effect can translate into a tremendous science pay-off for space missions. For example, one application of this new effect is to fiber optical gyroscopes for deep-space inertial guidance and tests of General Relativity (Gravity Probe B). Another application is to ground and orbiting optical interferometers for gravity wave detection, Laser Interferometer Gravity Observatory (LIGO) and the European Laser Interferometer Space Antenna (LISA), respectively. Other applications are to Satellite-to-Satellite laser Interferometry (SSI) proposed for the next generation Gravity Recovery And Climate Experiment (GRACE II). ",Kein DOI-Link verfügbar,quant-ph/0507150v1,Yes,potent(1)
0000-0003-1783-7176,Michel Rumin,Université Paris-Saclay,Around heat decay on forms and relations of nilpotent Lie groups,1970,  One knows that the large time heat decay exponent on a nilpotent group is given by half the growing rate of the volume of its large balls. This work deals with the similar problem of trying to interpret geometrically the heat decay on (one) forms. We will show how it is (partially) related to the depth of the relations required to define the group. The tools used apply in general on Carnot-Caratheodory manifolds. ,Kein DOI-Link verfügbar,math/0112057v2,Yes,potent(1)
0000-0003-1692-0488,Guillaume Sandou,Université Paris-Saclay,Multi-Head Attention for Multi-Modal Joint Vehicle Motion Forecasting,1970,"  This paper presents a novel vehicle motion forecasting method based on multi-head attention. It produces joint forecasts for all vehicles on a road scene as sequences of multi-modal probability density functions of their positions. Its architecture uses multi-head attention to account for complete interactions between all vehicles, and long short-term memory layers for encoding and forecasting. It relies solely on vehicle position tracks, does not need maneuver definitions, and does not represent the scene with a spatial grid. This allows it to be more versatile than similar model while combining any forecasting capabilities, namely joint forecast with interactions, uncertainty estimation, and multi-modality. The resulting prediction likelihood outperforms state-of-the-art models on the same dataset. ",Kein DOI-Link verfügbar,1910.03650v3,Yes,versatile(1)
0000-0001-7158-2261,Athmane Bakhta,Université Paris-Saclay,Numerical reconstruction of the first band(s) in an inverse Hill's   problem,1970,"  This paper concerns an inverse band structure problem for one dimensional periodic Schr\""odinger operators (Hill's operators). Our goal is to find a potential for the Hill's operator in order to reproduce as best as possible some given target bands, which may not be realisable. We recast the problem as an optimisation problem, and prove that this problem is well-posed when considering singular potentials (Borel measures). We then propose different algorithms to tackle the problem numerically. ",Kein DOI-Link verfügbar,1709.07023v1,Yes,potent(2)
0000-0001-7158-2261,Athmane Bakhta,Université Paris-Saclay,Epidemiological Forecasting with Model Reduction of Compartmental   Models. Application to the COVID-19 pandemic,1970,"  We propose a forecasting method for predicting epidemiological health series on a two-week horizon at the regional and interregional resolution. The approach is based on model order reduction of parametric compartmental models, and is designed to accommodate small amount of sanitary data. The efficiency of the method is shown in the case of the prediction of the number of infected and removed people during the two pandemic waves of COVID-19 in France, which have taken place approximately between February and November 2020. Numerical results illustrate the promising potential of the approach. ",Kein DOI-Link verfügbar,2009.09200v3,Yes,potent(1)
0000-0002-6287-2627,Benjamin Graille,Université Paris-Saclay,Does the multiresolution lattice Boltzmann method allow to deal with   waves passing through mesh jumps?,1970,"  We consider an adaptive multiresolution-based lattice Boltzmann scheme, which we have recently introduced and studied from the perspective of the error control and the theory of the equivalent equations. This numerical strategy leads to high compression rates, error control and its high accuracy has been explained on uniform and dynamically adaptive grids. However, one key issue with non-uniform meshes within the framework of lattice Boltzmann schemes is to properly handle acoustic waves passing through a level jump of the grid. It usually yields spurious effects, in particular reflected waves. In this paper, we propose a simple mono-dimensional test-case for the linear wave equation with a fixed adapted mesh characterized by a potentially large level jump. We investigate this configuration with our original strategy and prove that we can handle and control the amplitude of the reflected wave, which is of fourth order in the space step of the finest mesh. Numerical illustrations show that the proposed strategy outperforms the existing methods in the literature and allow to assess the ability of the method to handle the mesh jump properly. ",Kein DOI-Link verfügbar,2105.12609v2,Yes,potent(1)
0009-0005-2319-5255,Bartjan van Tent,Université Paris-Saclay,Non-Gaussianity in two-field inflation beyond the slow-roll   approximation,1970,"  We use the long-wavelength formalism to investigate the level of bispectral non-Gaussianity produced in two-field inflation models with standard kinetic terms. Even though the Planck satellite has so far not detected any primordial non-Gaussianity, it has tightened the constraints significantly, and it is important to better understand what regions of inflation model space have been ruled out, as well as prepare for the next generation of experiments that might reach the important milestone of Delta f_NL(local) = 1. We derive an alternative formulation of the previously derived integral expression for f_NL, which makes it easier to physically interpret the result and see which types of potentials can produce large non-Gaussianity. We apply this to the case of a sum potential and show that it is very difficult to satisfy simultaneously the conditions for a large f_NL and the observational constraints on the spectral index n_s. In the case of the sum of two monomial potentials and a constant we explicitly show in which small region of parameter space this is possible, and we show how to construct such a model. Finally, the new general expression for f_NL also allows us to prove that for the sum potential the explicit expressions derived within the slow-roll approximation remain valid even when the slow-roll approximation is broken during the turn of the field trajectory (as long as only the epsilon slow-roll parameter remains small). ",https://doi.org/10.1088/1475-7516/2017/05/019,1611.09233v2,Yes,potent(4)
0009-0005-2319-5255,Bartjan van Tent,Université Paris-Saclay,Bispectra from two-field inflation using the long-wavelength formalism,1970,"  We use the long-wavelength formalism to compute the bispectral non-Gaussianity produced in two-field inflation. We find an exact result that is used as the basis of numerical studies, and an explicit analytical slow-roll expression for several classes of potentials that gives insight into the origin and importance of the various contributions to fNL. We also discuss the momentum dependence of fNL. Based on these results we find a simple model that produces a relatively large non-Gaussianity. We show that the long-wavelength formalism is a viable alternative to the standard delta-N formalism, and can be preferable to it in certain situations. ",https://doi.org/10.1088/1475-7516/2011/06/026,1012.6027v3,Yes,potent(1)
0009-0005-2319-5255,Bartjan van Tent,Université Paris-Saclay,The bispectra of galactic CMB foregrounds and their impact on primordial   non-Gaussianity estimation,1970,"  We use the binned bispectrum estimator to determine the bispectra of the dust, free-free, synchrotron, and AME galactic foregrounds using maps produced by the Commander component separation method from Planck 2015 data. We find that all of these peak in the squeezed configuration, allowing for potential confusion with in particular the local primordial shape. Applying an additional functionality implemented in the binned bispectrum estimator code, we then use these galactic bispectra as templates in an $f_\mathrm{NL}$ analysis of other maps. After testing and validating the method and code with simulations, we show that we detect the dust in the raw 143 GHz map with the expected amplitude (the other galactic foregrounds are too weak at 143 GHz to be detected) and that no galactic residuals are detected in the cleaned CMB map. We also investigate the effect of the mask on the templates and the effect of the choice of binning on a joint dust-primordial $f_\mathrm{NL}$ analysis. ",https://doi.org/10.1088/1475-7516/2018/11/047,1810.01727v1,Yes,potent(1)
0000-0002-0706-0220,Francesco Saverio Pezzicoli,Université Paris-Saclay,Roadmap on machine learning glassy liquids,1970,"  Unraveling the connections between microscopic structure, emergent physical properties, and slow dynamics has long been a challenge in the field of the glass transition. The absence of clear visible structural order in amorphous configurations complicates the identification of the key features related to structural relaxation and transport properties. The difficulty in sampling equilibrated configurations at low temperatures hampers thorough numerical and theoretical investigations. This roadmap article explores the potential of machine learning (ML) techniques to face these challenges, building on the algorithms that have revolutionized computer vision and image recognition. We present successful ML applications, as well as many open problems for the future, such as transferability and interpretability of ML approaches. We highlight new ideas and directions in which ML could provide breakthroughs to better understand glassy liquids. To foster a collaborative community effort, the article introduces the ""GlassBench"" dataset, providing simulation data and benchmarks for both two-dimensional and three-dimensional glass-formers. Emphasizing the importance of benchmarks, we identify critical metrics for comparing the performance of emerging ML methodologies, in line with benchmarking practices in image and text recognition. The goal of this roadmap is to provide guidelines for the development of ML techniques in systems displaying slow dynamics, while inspiring new directions to improve our understanding of glassy liquids. ",Kein DOI-Link verfügbar,2311.14752v1,Yes,potent(1)
0009-0003-3065-4267,Evgenii Chzhen,Université Paris-Saclay,Classification with abstention but without disparities,1970,"  Classification with abstention has gained a lot of attention in recent years as it allows to incorporate human decision-makers in the process. Yet, abstention can potentially amplify disparities and lead to discriminatory predictions. The goal of this work is to build a general purpose classification algorithm, which is able to abstain from prediction, while avoiding disparate impact. We formalize this problem as risk minimization under fairness and abstention constraints for which we derive the form of the optimal classifier. Building on this result, we propose a post-processing classification algorithm, which is able to modify any off-the-shelf score-based classifier using only unlabeled sample. We establish finite sample risk, fairness, and abstention guarantees for the proposed algorithm. In particular, it is shown that fairness and abstention constraints can be achieved independently from the initial classifier as long as sufficiently many unlabeled data is available. The risk guarantee is established in terms of the quality of the initial classifier. Our post-processing scheme reduces to a sparse linear program allowing for an efficient implementation, which we provide. Finally, we validate our method empirically showing that moderate abstention rates allow to bypass the risk-fairness trade-off. ",Kein DOI-Link verfügbar,2102.12258v1,Yes,potent(1)
0009-0003-3065-4267,Evgenii Chzhen,Université Paris-Saclay,Addressing bias in online selection with limited budget of comparisons,1970,"  Consider a hiring process with candidates coming from different universities. It is easy to order candidates with the same background, yet it can be challenging to compare them otherwise. The latter case requires additional costly assessments, leading to a potentially high total cost for the hiring organization. Given an assigned budget, what would be an optimal strategy to select the most qualified candidate? We model the above problem as a multicolor secretary problem, allowing comparisons between candidates from distinct groups at a fixed cost. Our study explores how the allocated budget enhances the success probability of online selection algorithms. ",Kein DOI-Link verfügbar,2303.09205v3,Yes,potent(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,A simple proof of three properties on Simpson's 4-slot Algorithm,1970,"  In this paper we present an invariance proof of three properties on Simpson's 4-slot algorithm, i.e. data-race freedom, data coherence and data freshness, which together implies linearisability of the algorithm. It is an extension of previous works whose proof focuses mostly on data-race freedom. In addition, our proof uses simply inductive invariants and transition invariants, whereas previous work uses more sophisticated machinery like separation logics, rely-guarantee or ownership transfer. ",Kein DOI-Link verfügbar,2112.06233v1,Yes,fresh(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Causal Discovery in High-Dimensional Point Process Networks with Hidden   Nodes,1970,"  Thanks to technological advances leading to near-continuous time observations, emerging multivariate point process data offer new opportunities for causal discovery. However, a key obstacle in achieving this goal is that many relevant processes may not be observed in practice. Naive estimation approaches that ignore these hidden variables can generate misleading results because of the unadjusted confounding. To plug this gap, we propose a deconfounding procedure to estimate high-dimensional point process networks with only a subset of the nodes being observed. Our method allows flexible connections between the observed and unobserved processes. It also allows the number of unobserved processes to be unknown and potentially larger than the number of observed nodes. Theoretical analyses and numerical studies highlight the advantages of the proposed method in identifying causal interactions among the observed processes. ",https://doi.org/10.3390/e23121622,2109.10947v1,Yes,potent(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Inverse scattering for the biharmonic wave equation with a random   potential,1970,"  We consider the inverse random potential scattering problem for the two- and three-dimensional biharmonic wave equation in lossy media. The potential is assumed to be a microlocally isotropic Gaussian rough field. The main contributions of the work are twofold. First, the unique continuation principle is proved for the fourth order biharmonic wave equation with rough potentials and the well-posedness of the direct scattering problem is established in the distribution sense. Second, the correlation strength of the random potential is shown to be uniquely determined by the high frequency limit of the second moment of the scattered field averaged over the frequency band. Moreover, we demonstrate that the expectation in the data can be removed and the data of a single realization is sufficient for the uniqueness of the inverse problem with probability one when the medium is lossless. ",Kein DOI-Link verfügbar,2210.05900v1,Yes,potent(4)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Stochastic Adversarial Networks for Multi-Domain Text Classification,1970,"  Adversarial training has been instrumental in advancing multi-domain text classification (MDTC). Traditionally, MDTC methods employ a shared-private paradigm, with a shared feature extractor for domain-invariant knowledge and individual private feature extractors for domain-specific knowledge. Despite achieving state-of-the-art results, these methods grapple with the escalating model parameters due to the continuous addition of new domains. To address this challenge, we introduce the Stochastic Adversarial Network (SAN), which innovatively models the parameters of the domain-specific feature extractor as a multivariate Gaussian distribution, as opposed to a traditional weight vector. This design allows for the generation of numerous domain-specific feature extractors without a substantial increase in model parameters, maintaining the model's size on par with that of a single domain-specific extractor. Furthermore, our approach integrates domain label smoothing and robust pseudo-label regularization to fortify the stability of adversarial training and to refine feature discriminability, respectively. The performance of our SAN, evaluated on two leading MDTC benchmarks, demonstrates its competitive edge against the current state-of-the-art methodologies. The code is available at https://github.com/wangxu0820/SAN. ",Kein DOI-Link verfügbar,2406.00044v1,Yes,"innovative(1), innovatively(1)"
0000-0002-3381-7274,Xu Wang,Sorbonne University,Nuclear excitation cross section of $^{229}$Th via inelastic electron   scattering,1970,"  Nuclear excitation cross section of $^{229}$Th from the ground state to the low-lying isomeric state via inelastic electron scattering is calculated, on the level of Dirac distorted wave Born approximation. With electron energies below 100 eV, inelastic scattering is very efficient in the isomeric excitation, yielding excitation cross sections on the order of 10$^{-27}$ to 10$^{-26}$ cm$^2$. Systematic analyses are presented on elements affecting the excitation cross section, including the ion-core potential, the relativistic effect, the knowledge of the reduced nuclear transition probabilities, etc. ",Kein DOI-Link verfügbar,2207.08330v1,Yes,potent(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Alpha decay in intense laser fields: Calculations using realistic   nuclear potentials,1970,"  We calculate the effect of intense laser fields on nuclear alpha decay processes, using realistic and quantitative nuclear potentials. We show that alpha decay rates can indeed be modified by strong laser fields to some finite extent. We also predict that alpha decays with lower decay energies are relatively easier to be modified than those with higher decay energies, due to longer tunneling paths for the laser field to act on. Furthermore, we predict that modifications to angle-resolved penetrability are easier to achieve than modifications to angle-integrated penetrability. ",https://doi.org/10.1103/PhysRevC.99.044610,1810.07331v1,Yes,potent(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Nuclear fission in intense laser fields,1970,"  Rapid-advancing intense laser technologies enable the possibility of a direct laser-nucleus coupling. In this paper the effect of intense laser fields on a series of nuclear fission processes, including proton decay, alpha decay, and cluster decay, is theoretically studied with the help of nuclear double folding potentials. The results show that the half-lives of these decay processes can be modified by non-negligible amounts, for example on the order of 0.01 or 0.1 percents in intense laser fields available in the forthcoming years. In addition to numerical results, an approximate analytical formula is derived to connect the laser-induced modification to the decay half-life and the decay energy. ",https://doi.org/10.1103/PhysRevC.102.064629,2008.03498v1,Yes,potent(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,On the mechanical beta relaxation in glass and its relation to the   double-peak phenomenon in impulse excited vibration at high temperatures,1970,"  A viscoelastic model is established to reveal the relation between alpha-beta relaxation of glass and the double-peak phenomenon in the experiments of impulse excited vibration. In the modelling, the normal mode analysis (NMA) of potential energy landscape (PEL) picture is employed to describe mechanical alpha and beta relaxations in a glassy material. The model indicates that a small beta relaxation can lead to an apparent double-peak phenomenon resulted from the free vibration of a glass beam when the frequency of beta relaxation peak is close to the natural frequency of specimen. The theoretical prediction is validated by the acoustic spectrum of a fluorosilicate glass beam excited by a mid-span impulse. Furthermore, the experimental results indicate a negative temperature-dependence of the frequency of beta relaxation in the fluorosilicate glass S-FSL5 which can be explained based on the physical picture of fragmented oxide-network patches in liquid-like regions. ",https://doi.org/10.1016/j.jnoncrysol.2020.119939,2001.09344v1,Yes,potent(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Inverse random potential scattering for elastic waves,1970,"  This paper is concerned with the inverse elastic scattering problem for a random potential in three dimensions. Interpreted as a distribution, the potential is assumed to be a microlocally isotropic Gaussian random field whose covariance operator is a classical pseudo-differential operator. Given the potential, the direct scattering problem is shown to be well-posed in the distribution sense by studying the equivalent Lippmann--Schwinger integral equation. For the inverse scattering problem, we demonstrate that the microlocal strength of the random potential can be uniquely determined with probability one by a single realization of the high frequency limit of the averaged compressional or shear backscattered far-field pattern of the scattered wave. The analysis employs the integral operator theory, the Born approximation in the high frequency regime, the microlocal analysis for the Fourier integral operators, and the ergodicity of the wave field. ",Kein DOI-Link verfügbar,2102.07062v1,Yes,potent(4)
0000-0002-3381-7274,Xu Wang,Sorbonne University,"Towards Process-Oriented, Modular, and Versatile Question Generation   that Meets Educational Needs",1970,"  NLP-powered automatic question generation (QG) techniques carry great pedagogical potential of saving educators' time and benefiting student learning. Yet, QG systems have not been widely adopted in classrooms to date. In this work, we aim to pinpoint key impediments and investigate how to improve the usability of automatic QG techniques for educational purposes by understanding how instructors construct questions and identifying touch points to enhance the underlying NLP models. We perform an in-depth need finding study with 11 instructors across 7 different universities, and summarize their thought processes and needs when creating questions. While instructors show great interests in using NLP systems to support question design, none of them has used such tools in practice. They resort to multiple sources of information, ranging from domain knowledge to students' misconceptions, all of which missing from today's QG systems. We argue that building effective human-NLP collaborative QG systems that emphasize instructor control and explainability is imperative for real-world adoption. We call for QG systems to provide process-oriented support, use modular design, and handle diverse sources of input. ",Kein DOI-Link verfügbar,2205.00355v1,Yes,potent(1)
0000-0002-3381-7274,Xu Wang,Sorbonne University,An inverse potential problem for the stochastic diffusion equation with   a multiplicative white noise,1970,"  This work concerns the direct and inverse potential problems for the stochastic diffusion equation driven by a multiplicative time-dependent white noise. The direct problem is to examine the well-posedness of the stochastic diffusion equation for a given potential, while the inverse problem is to determine the potential from the expectation of the solution at a fixed observation point inside the spatial domain. The direct problem is shown to admit a unique and positive mild solution if the initial value is nonnegative. Moreover, an explicit formula is deduced to reconstruct the square of the potential, which leads to the uniqueness of the inverse problem for nonnegative potential functions. Two regularization methods are utilized to overcome the instability of the numerical differentiation in the reconstruction formula. Numerical results show that the methods are effective to reconstruct both smooth and nonsmooth potential functions. ",Kein DOI-Link verfügbar,2302.03333v1,Yes,potent(6)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Inverse random potential scattering for the polyharmonic wave equation   using far-field patterns,1970,"  This paper addresses the inverse scattering problem of a random potential associated with the polyharmonic wave equation in two and three dimensions. The random potential is represented as a centered complex-valued generalized microlocally isotropic Gaussian random field, where its covariance and relation operators are characterized as conventional pseudo-differential operators. Regarding the direct scattering problem, the well-posedness is established in the distribution sense for sufficiently large wavenumbers through analysis of the corresponding Lippmann--Schwinger integral equation. Furthermore, in the context of the inverse scattering problem, the uniqueness is attained in recovering the microlocal strengths of both the covariance and relation operators of the random potential. Notably, this is accomplished with only a single realization of the backscattering far-field patterns averaged over the high-frequency band. ",Kein DOI-Link verfügbar,2407.15681v1,Yes,potent(3)
0000-0002-3381-7274,Xu Wang,Sorbonne University,Semi-Instruct: Bridging Natural-Instruct and Self-Instruct for Code   Large Language Models,1970,"  Instruction tuning plays a pivotal role in Code Large Language Models (Code LLMs) for the task of program synthesis. Presently, two dominant paradigms for collecting tuning data are natural-instruct (human-written) and self-instruct (automatically generated). Natural-instruct includes diverse and correct codes but lacks instruction-code pairs, and exists improper code formats like nested single-line codes. In contrast, self-instruct automatically generates proper paired data. However, it suffers from low diversity due to generating duplicates and cannot ensure the correctness of codes. To bridge the both paradigms, we propose \textbf{Semi-Instruct}. It first converts diverse but improper codes from natural-instruct into proper instruction-code pairs through a method similar to self-instruct. To verify the correctness of generated codes, we design a novel way to construct test cases by generating cases' inputs and executing correct codes from natural-instruct to get outputs. Finally, diverse and correct instruction-code pairs are retained for instruction tuning. Experiments show that semi-instruct is significantly better than natural-instruct and self-instruct. Furthermore, the performance steadily improves as data scale increases. ",Kein DOI-Link verfügbar,2403.00338v1,Yes,pivotal(1)
0000-0003-2320-6178,Swan Dubois,Sorbonne University,"Self-Stabilization, Byzantine Containment, and Maximizable Metrics:   Necessary Conditions",1970,"  Self-stabilization is a versatile approach to fault-tolerance since it permits a distributed system to recover from any transient fault that arbitrarily corrupts the contents of all memories in the system. Byzantine tolerance is an attractive feature of distributed systems that permits to cope with arbitrary malicious behaviors. We consider the well known problem of constructing a maximum metric tree in this context. Combining these two properties leads to some impossibility results. In this paper, we provide two necessary conditions to construct maximum metric tree in presence of transients and (permanent) Byzantine faults. ",Kein DOI-Link verfügbar,1103.3515v1,Yes,versatile(1)
0000-0003-2320-6178,Swan Dubois,Sorbonne University,The Impact of Topology on Byzantine Containment in Stabilization,1970,"  Self-stabilization is an versatile approach to fault-tolerance since it permits a distributed system to recover from any transient fault that arbitrarily corrupts the contents of all memories in the system. Byzantine tolerance is an attractive feature of distributed system that permits to cope with arbitrary malicious behaviors. We consider the well known problem of constructing a maximum metric tree in this context. Combining these two properties prove difficult: we demonstrate that it is impossible to contain the impact of Byzantine nodes in a self-stabilizing context for maximum metric tree construction (strict stabilization). We propose a weaker containment scheme called topology-aware strict stabilization, and present a protocol for computing maximum metric trees that is optimal for this scheme with respect to impossibility result. ",Kein DOI-Link verfügbar,1005.1195v1,Yes,versatile(1)
0000-0003-2320-6178,Swan Dubois,Sorbonne University,Bounding the Impact of Unbounded Attacks in Stabilization,1970,"  Self-stabilization is a versatile approach to fault-tolerance since it permits a distributed system to recover from any transient fault that arbitrarily corrupts the contents of all memories in the system. Byzantine tolerance is an attractive feature of distributed systems that permits to cope with arbitrary malicious behaviors. Combining these two properties proved difficult: it is impossible to contain the spatial impact of Byzantine nodes in a self-stabilizing context for global tasks such as tree orientation and tree construction. We present and illustrate a new concept of Byzantine containment in stabilization. Our property, called Strong Stabilization enables to contain the impact of Byzantine nodes if they actually perform too many Byzantine actions. We derive impossibility results for strong stabilization and present strongly stabilizing protocols for tree orientation and tree construction that are optimal with respect to the number of Byzantine nodes that can be tolerated in a self-stabilizing context. ",Kein DOI-Link verfügbar,1005.3367v2,Yes,versatile(1)
0000-0003-2320-6178,Swan Dubois,Sorbonne University,On Byzantine Containment Properties of the $min+1$ Protocol,1970,  Self-stabilization is a versatile approach to fault-tolerance since it permits a distributed system to recover from any transient fault that arbitrarily corrupts the contents of all memories in the system. Byzantine tolerance is an attractive feature of distributed systems that permits to cope with arbitrary malicious behaviors. We consider the well known problem of constructing a breadth-first spanning tree in this context. Combining these two properties proves difficult: we demonstrate that it is impossible to contain the impact of Byzantine nodes in a strictly or strongly stabilizing manner. We then adopt the weaker scheme of topology-aware strict stabilization and we present a similar weakening of strong stabilization. We prove that the classical $min+1$ protocol has optimal Byzantine containment properties with respect to these criteria. ,Kein DOI-Link verfügbar,1005.5223v1,Yes,versatile(1)
0000-0003-2320-6178,Swan Dubois,Sorbonne University,Maximum Metric Spanning Tree made Byzantine Tolerant,1970,"  Self-stabilization is a versatile approach to fault-tolerance since it permits a distributed system to recover from any transient fault that arbitrarily corrupts the contents of all memories in the system. Byzantine tolerance is an attractive feature of distributed systems that permits to cope with arbitrary malicious behaviors. This paper focus on systems that are both self-stabilizing and Byzantine tolerant. We consider the well known problem of constructing a maximum metric tree in this context. Combining these two properties is known to induce many impossibility results. In this paper, we provide first two impossibility results about the construction of maximum metric tree in presence of transients and (permanent) Byzantine faults. Then, we provide a new self-stabilizing protocol that provides optimal containment of an arbitrary number of Byzantine faults. ",Kein DOI-Link verfügbar,1104.5368v1,Yes,versatile(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Entanglement and Wigner function negativity of multimode non-Gaussian   states,1970,"  Non-Gaussian operations are essential to exploit the quantum advantages in optical continuous variable quantum information protocols. We focus on mode-selective photon addition and subtraction as experimentally promising processes to create multimode non-Gaussian states. Our approach is based on correlation functions, as is common in quantum statistical mechanics and condensed matter physics, mixed with quantum optics tools. We formulate an analytical expression of the Wigner function after subtraction or addition of a single photon, for arbitrarily many modes. It is used to demonstrate entanglement properties specific to non-Gaussian states, and also leads to a practical and elegant condition for Wigner function negativity. Finally, we analyse the potential of photon addition and subtraction for an experimentally generated multimode Gaussian state. ",https://doi.org/10.1103/PhysRevLett.119.183601,1707.02285v2,Yes,potent(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Statistical signatures of multimode single-photon added and subtracted   states of light,1970,"  The addition or subtraction of a photon from a Gaussian state of light is a versatile and experimentally feasible procedure to create non-Gaussian states. In multimode setups, these states manifest a wide range of phenomena when the photon is added or subtracted in a mode-tunable way. In this contribution, we derive the truncated correlations, which are multimode generalisations of cumulants, between quadratures in different modes as statistical signatures of these states. These correlations are then used to obtain the full multimode Wigner function, the properties of which are subsequently studied. In particular we investigate the effect of impurity in the subtraction or addition process, and evaluate its impact on the negativity of the Wigner function. Finally, we elaborate on the generation of inherent entanglement through subtraction or addition of a photon from a pure squeezed vacuum. ",https://doi.org/10.1103/PhysRevA.96.053835,1708.08412v2,Yes,versatile(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Quantum improvement of time transfer between remote clocks,1970,"  Exchanging light pulses to perform accurate space-time positioning is a paradigmatic issue of physics. It is ultimately limited by the quantum nature of light, which introduces fluctuations in the optical measurements and leads to the so-called Standard Quantum Limit (SQL). We propose a new scheme combining homodyne detection and mode-locked femtosecond lasers that lead to a new SQL in time transfer, potentially reaching the yoctosecond range (10^-21-10^-24 s). We prove that no other measurement strategy can lead to better sensitivity with shot noise limited light. We then demonstrate that this already very low SQL can be overcome using appropriately multimode squeezed light. Benefitting from the large number of photons used in the experiment and from the optimal choice of both the detection strategy and of the quantum resource, the proposed scheme represents a significant potential improvement in space-time positioning. ",https://doi.org/10.1103/PhysRevLett.101.123601,0804.1203v1,Yes,potent(2)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,How subtraction of a single photon affects many quantum modes,1970,"  The subtraction of a single photon from a multimode quantum field is analyzed as the conditional evolution of an open quantum system. We develop a theory describing different subtraction schemes in a unified approach and we introduce the concept of subtraction modes intrinsic to the process. The matching between the subtraction modes and the modes of the field defines different possible scenarios for the photon subtraction. In particular, our framework identifies: the conditions of pure photon subtraction, the quantum states of the field modes conditioned to the photon subtraction, the mode with the highest fidelity with a single-photon state when the subtraction is performed on multimode squeezed light. We use our theory to analyze the photon subtraction from a highly multimode quantum resource - a train of quantum squeezed or correlated optical pulses. Performing the photon subtraction optimally on various multimode light field has the potential to implement a number of quantum information protocols in a multiplexed and scalable way. ",Kein DOI-Link verfügbar,1510.04217v1,Yes,potent(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Quantum improved measurement of time transfer,1970,"  Accurate time transfer has become a crucial issue for future space experiments which require increasing resolution over large distances. In 2008, a scheme combining homodyne detection and mode-locked femtosecond lasers was proposed that leads to a potential timing precision reaching the yoctosecond range; with a multimode quantum frequency comb as the input field, the sub-shot noise measurement for the time transfer can further improve the timing precision. Based on this scheme and applying the multimode squeezing frequency comb that was measured to have a phase quadrature quantum noise reduction of 1.5 dB at the analyzing frequency 2MHz, the measurable timing fluctuation was reduced from a shot-noise limited value of 8.9E-23s to 7.5E-23s. To our knowledge, this is the first experimental demonstration of the time transfer measurement that achieves a precision beyond the standard quantum limit (SQL). ",https://doi.org/10.1103/PhysRevA.98.053821,1802.08384v1,Yes,potent(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Multimode Single-Pass Spatio-temporal Squeezing,1970,  We present a single-pass source of broadband multimode squeezed light with potential application in quantum information and quantum metrology. The source is based on a type I parametric down-conversion (PDC) process inside a bulk nonlinear crystal in a non-collinear configuration. The generated squeezed light exhibits a spatiotemporal multimode behavior that is probed using a homodyne measurement with a local oscillator shaped both spatially and temporally. Finally we follow a covariance matrix based approach to reveal the distribution of the squeezing among several independent temporal and spatial modes. This unambiguously validates the multimode feature of our source. ,https://doi.org/10.1364/OE.386528,2001.03972v1,Yes,potent(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Direct generation of a multi-transverse mode non-classical state of   light,1970,"  Quantum computation and communication protocols require quantum resources which are in the continuous variable regime squeezed and/or quadrature entangled optical modes. To perform more and more complex and robust protocols, one needs sources that can produce in a controlled way highly multimode quantum states of light. One possibility is to mix different single mode quantum resources. Another is to directly use a multimode device, either in the spatial or in the frequency domain. We present here the first experimental demonstration of a device capable of producing simultanuously several squeezed transverse modes of the same frequency and which is potentially scalable. We show that this device, which is an Optical Parametric Oscillator using a self-imaging cavity, produces a multimode quantum resource made of three squeezed transverse modes. ",https://doi.org/10.1364/OE.19.004405,1101.4498v1,Yes,potent(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Full characterization of a highly multimode entangled state embedded in   an optical frequency comb using pulse shaping,1970,"  We present a detailed analysis of the multimode quantum state embedded in an optical frequency comb generated by a Synchronously Pumped Optical Parametric Oscillator (SPOPO). The full covariance matrix of the state is obtained with homodyne detection where the local oscillator is spectrally controlled with pulse shaping techniques. The resulting matrix reveals genuine multipartite entanglement. Additionally, the beam is comprised of several independent eigenmodes that correspond to specific pulse shapes. The experimental data is confirmed with numerical simulations. Finally, the potential to create continuous-variable cluster states from the quantum comb is analyzed. Multiple cluster states are shown to be simultaneously embedded in the SPOPO state, and these states can be revealed by a suitable basis change applied to the measured covariance matrix. ",https://doi.org/10.1103/PhysRevA.89.053828,1401.4867v1,Yes,potent(1)
0000-0001-9778-9025,Claude Fabre,Sorbonne University,Frequency multiplexed entanglement for continuous-variable quantum key   distribution,1970,"  Quantum key distribution with continuous variables already uses advantageous high-speed single-mode homodyne detection with low electronic noise at room temperature. Together with continuous-variable information encoding to nonclassical states, the distance for secure key transmission through lossy channels can approach 300 km in current optical fibers. Such protocols tolerate higher channel noise and also limited data processing efficiency compared to coherent-state protocols. The secret key rate can be further increased by increasing the system clock rates, and, further, by a suitable frequency-mode-multiplexing of optical transmission channels. However, the multiplexed modes couple together in the source or any other part of the protocol. Therefore, multiplexed communication will experience crosstalk and the gain can be minuscule. Advantageously, homodyne detectors allow solving this crosstalk problem by proper data processing. It is a potential advantage over protocols with single-photon detectors, which do not enable similar data processing techniques. We demonstrate the positive outcome of this methodology on the experimentally characterized frequency-multiplexed entangled source of femtosecond optical pulses with natural crosstalk between eight entangled pairs of modes. As the main result, we predict almost 15-fold higher secret key rate. This experimental test and analysis of frequency-multiplexed entanglement source opens the way for the field implementation of high-capacity quantum key distribution with continuous variables. ",Kein DOI-Link verfügbar,2110.14506v1,Yes,potent(1)
0000-0003-1079-6041,Nicolas Rodriguez,Sorbonne University,SBML Qualitative Models: a model representation format and   infrastructure to foster interactions between qualitative modelling   formalisms and tools,1970,"  Background: Qualitative frameworks, especially those based on the logical discrete formalism, are increasingly used to model regulatory and signalling networks. A major advantage of these frameworks is that they do not require precise quantitative data, and that they are well-suited for studies of large networks. While numerous groups have developed specific computational tools that provide original methods to analyse qualitative models, a standard format to exchange qualitative models has been missing.   Results: We present the System Biology Markup Language (SBML) Qualitative Models Package (""qual""), an extension of the SBML Level 3 standard designed for computer representation of qualitative models of biological networks. We demonstrate the interoperability of models via SBML qual through the analysis of a specific signalling network by three independent software tools. Furthermore, the cooperative development of the SBML qual format paved the way for the development of LogicalModel, an open-source model library, which will facilitate the adoption of the format as well as the collaborative development of algorithms to analyze qualitative models.   Conclusion: SBML qual allows the exchange of qualitative models among a number of complementary software tools. SBML qual has the potential to promote collaborative work on the development of novel computational approaches, as well as on the specification and the analysis of comprehensive qualitative models of regulatory and signalling networks. ",Kein DOI-Link verfügbar,1309.1910v1,Yes,potent(1)
0000-0002-9579-6077,Pavel Tumarkin,Durham University,Essential hyperbolic Coxeter polytopes,1970,"  We introduce a notion of essential hyperbolic Coxeter polytope as a polytope which fits some minimality conditions. The problem of classification of hyperbolic reflection groups can be easily reduced to classification of essential Coxeter polytopes. We determine a potentially large combinatorial class of polytopes containing, in particular, all the compact hyperbolic Coxeter polytopes of dimension at least 6 which are known to be essential, and prove that this class contains finitely many polytopes only. We also construct an effective algorithm of classifying polytopes from this class, realize it in four-dimensional case, and formulate a conjecture on finiteness of the number of essential polytopes. ",https://doi.org/10.1007/s11856-013-0046-3,0906.4111v3,Yes,potent(1)
0000-0002-8767-1602,Yuqian Wang,Durham University,Localized excitation on the Jacobi elliptic periodic background for the   (n+1)-dimensional generalized Kadomtsev-Petviashvili equation,1970,"  In this paper, the linear spectral problem, which associated with the (n+1)-dimensional generalized Kadomtsev-Petviashvili (gKP) equation, with the Jacobi elliptic function as the external potential is investigated based on the Lame function, from which some novel local nonlinear wave solutions on the Jacobi elliptic function have been obtained by Darboux transformation, and the corresponding dynamics have also been discussed. At the same time, the degenerate solutions of the nonlinear wave solutions on the Jacobi function background for the gKP equation are constructed by taking the modulus of the Jacobi function to be 0 and 1. The findings indicate that there can be various types of nonlinear wave solutions with different ranges of spectral parameters, including soliton and breather waves. These results will be useful for elucidating and predicting nonlinear phenomena in related physical fields, such as physical ocean and Bose-Einstein condensates. ",Kein DOI-Link verfügbar,2407.19805v1,Yes,potent(1)
0000-0002-6122-7052,Richard Wilson,Durham University,An Hopf algebra for counting simple cycles,1970,"  Simple cycles, also known as self-avoiding polygons, are cycles on graphs which are not allowed to visit any vertex more than once. We present an exact formula for enumerating the simple cycles of any length on any directed graph involving a sum over its induced subgraphs. This result stems from an Hopf algebra, which we construct explicitly, and which provides further means of counting simple cycles. Finally, we obtain a more general theorem asserting that any Lie idempotent can be used to enumerate simple cycles. ",https://doi.org/10.1016/j.disc.2017.10.002,1607.00902v2,Yes,potent(1)
0000-0002-6122-7052,Richard Wilson,Durham University,Demonstrating 24-hour continuous vertical monitoring of atmospheric   optical turbulence,1970,"  We report what is believed to be the first example of fully continuous, 24-hour vertical monitoring of atmospheric optical turbulence. This is achieved using a novel instrument, the 24-hour Shack-Hartmann Image Motion Monitor (24hSHIMM). Optical turbulence is a fundamental limitation for applications such as free-space optical communications, where it limits the achievable bandwidth, and ground-based optical astronomy, restricting the observational precision. Knowledge of the turbulence enables us to select the best sites, design optical instrumentation and optimise the operation of ground-based optical systems. The 24hSHIMM estimates the vertical optical turbulence coherence length, time, angle and Rytov variance from the measurement of a four-layer vertical turbulence profile and a wind speed profile retrieved from meteorological forecasts. To illustrate our advance we show the values of these parameters recorded during a 35-hour, continuous demonstration of the instrument. Due to its portability and ability to work in stronger turbulence, the 24hSHIMM can also operate in urban locations, providing the field with a truly continuous, versatile turbulence monitor for all but the most demanding of applications. ",https://doi.org/10.1364/OE.479544,2301.07612v2,Yes,versatile(1)
0000-0001-7884-9432,Joachim Winther Pedersen,"IT University of Copenhagen, University of Copenhagen",From Text to Life: On the Reciprocal Relationship between Artificial   Life and Large Language Models,1970,"  Large Language Models (LLMs) have taken the field of AI by storm, but their adoption in the field of Artificial Life (ALife) has been, so far, relatively reserved. In this work we investigate the potential synergies between LLMs and ALife, drawing on a large body of research in the two fields. We explore the potential of LLMs as tools for ALife research, for example, as operators for evolutionary computation or the generation of open-ended environments. Reciprocally, principles of ALife, such as self-organization, collective intelligence and evolvability can provide an opportunity for shaping the development and functionalities of LLMs, leading to more adaptive and responsive models. By investigating this dynamic interplay, the paper aims to inspire innovative crossover approaches for both ALife and LLM research. Along the way, we examine the extent to which LLMs appear to increasingly exhibit properties such as emergence or collective intelligence, expanding beyond their original goal of generating text, and potentially redefining our perception of lifelike intelligence in artificial systems. ",Kein DOI-Link verfügbar,2407.09502v1,Yes,"innovative(1), potent(3)"
0000-0001-7303-914X,Uffe Gråe Jørgensen,"University of Copenhagen, University of Copenhagen, Niels Bohr Institute",Near-Earth supernova activity during the past 35 Myr,1970,"  Here we combine observations of open clusters (OCs) with single- and binary population synthesis models and a Galactic potential to reconstruct the SN activity of these OCs during the past 35 Myr. We find that several OCs potentially hosting SN progenitors have passed within 100 pc of the Sun during the past 35 Myr. In particular we find that ASCC 19, NGC 1981, and NGC 1976 are likely to have hosted one or more SNe while passing within 200 pc of the solar system in the period 17 - 12 Myr BP which might have affected Earths' geology and climate. Besides the stellar history of the individual OCs we also compute 1) a spatial and temporal 2D-probability density map showing the most likely position and time of SN from our sample of OCs within 1 kpc during the past 35 Myr, 2) the time series of the SN rate per volume and 3) the relative SN rate compared with today and corrected for OC evaporation of older generations. The SN rate today from core collapse is estimated to be 37.8$\pm$6.1$\rm kpc^{-3} Myr^{-1}$. During the past 35 Myr we find a peak SN rate around 10 Myr before present (BP) where the rate was 40% higher relative to the past 1 Myr. Finally we discuss possible effects of binary stellar evolution in relation to the history of SN production in the solar neighbourhood and the detected $\rm ^{60}Fe$ signal in terrestrial geological samples induced between $\sim$2.2 - 2.8Myr BP. ",Kein DOI-Link verfügbar,1708.08248v1,Yes,potent(2)
0000-0001-7303-914X,Uffe Gråe Jørgensen,"University of Copenhagen, University of Copenhagen, Niels Bohr Institute",Exploring the deep atmospheres of HD 209458b and WASP-43b using a   non-gray general circulation model,1970,"  Simulations with a 3D general circulation model (GCM) suggest that one potential driver behind the observed radius inflation in hot Jupiters may be the downward advection of energy from the highly irradiated photosphere into the deeper layers. Here, we compare dynamical heat transport within the non-inflated hot Jupiter WASP-43b and the canonical inflated hot Jupiter HD 209458b, with similar effective temperatures. We investigate to what extent the radiatively driven heating and cooling in the photosphere (at pressures smaller than 1 bar) influence the deeper temperature profile (at pressures between 1 to 700 bar). Our simulations with the new non-gray 3D radiation-hydrodynamical model expeRT/MITgcm show that the deep temperature profile of WASP-43b is associated with a relatively cold adiabat. The deep layers of HD 209458b, however, do not converge and remain nearly unchanged regardless of whether a cold or a hot initial state is used. Furthermore, we show that different flow structures in the deep atmospheric layers arise. There, we find that WASP-43b exhibits a deep equatorial jet, driven by the relatively fast tidally locked rotation of this planet (0.81 days), as compared to HD 209458b (3.47 days). However, by comparing simulations with different rotation periods, we find that the resulting flow structures only marginally influence the temperature evolution in the deep atmosphere, which is almost completely dominated by radiative heating and cooling. Furthermore, we find that the evolution of deeper layers can influence the 3D temperature structure in the photosphere of WASP-43b. Thus, dayside emission spectra of WASP-43b may shed more light onto the dynamical processes occurring at greater depths. ",Kein DOI-Link verfügbar,2202.09183v4,Yes,potent(1)
0000-0001-7303-914X,Uffe Gråe Jørgensen,"University of Copenhagen, University of Copenhagen, Niels Bohr Institute",No evidence for radius inflation in hot Jupiters from vertical advection   of heat,1970,"  Understanding the radiative-dynamical coupling between upper photosphere and deeper atmosphere is a key in understanding the abnormal large radii of hot Jupiters. One needs very long integration times of 3D general circulation models (GCMs) with self consistent radiative transfer to achieve a better understanding of the feedback process between dynamics and radiation. We here present the longest 3D non-gray GCM study (86000 d) of an ultra hot Jupiter (WASP-76 b) published to this date that reached a final converged state. Furthermore, we present a method that can be used to accelerate the path towards temperature convergence in the deep atmospheric layers. We find that the final converged temperature profile is cold in the deep atmospheric layers, lacking any sign of vertical transport of potential temperature by large scale atmospheric motions. We thus conclude that the coupling between radiation and dynamics alone is not sufficient to explain the abnormal large radii of inflated hot gas giants. ",https://doi.org/10.1051/0004-6361/202244797,2210.01466v1,Yes,potent(1)
0000-0001-7303-914X,Uffe Gråe Jørgensen,"University of Copenhagen, University of Copenhagen, Niels Bohr Institute",Evidence of Radius Inflation in Radiative GCM Models of WASP-76b due to   the Advection of Potential Temperature,1970,"  Understanding the discrepancy between the radii of observed hot Jupiters and standard 'radiative-convective' models remains a hotly debated topic in the exoplanet community. One mechanism which has been proposed to bridge this gap, and which has recently come under scrutiny, is the vertical advection of potential temperature from the irradiated outer atmosphere deep into the interior, heating the deep, unirradiated, atmosphere, warming the internal adiabat, and resulting in radius inflation. Specifically, a recent study which explored the atmosphere of WASP-76b using a 3D, non-grey, GCM suggested that their models lacked radius inflation, and hence any vertical enthalpy advection. Here we perform additional analysis of these, and related, models, focusing on an explicit analysis of vertical enthalpy transport and the resulting heating of the deep atmosphere compared with 1D models. Our results indicate that, after any evolution linked with initialisation, all the WASP-76b models considered here exhibit significant vertical enthalpy transport, heating the deep atmosphere significantly when compared with standard 1D models. Furthermore, comparison of a long time-scale (and hence near steady-state) model with a Jupiter-like internal-structure model suggests not only strong radius-inflation, but also that the model radius, $1.98 \mathrm{R_{J}}$, may be comparable with observations ($1.83\pm0.06 \mathrm{R_{J}}$). We thus conclude that the vertical advection of potential temperature alone is enough to explain the radius inflation of WASP-76b, and potentially other irradiated gas giants, albeit with the proviso that the exact strength of the vertical advection remains sensitive to model parameters, such as the inclusion of deep atmospheric drag. ",https://doi.org/10.1093/mnras/stad1905,2306.12352v1,Yes,potent(3)
0000-0003-1293-5498,Michael Kretschmer,University of Zurich,Evaluating Galaxy Dynamical Masses From Kinematics and Jeans Equilibrium   in Simulations,1970,"  We provide prescriptions to evaluate the dynamical mass ($M_{\rm dyn}$) of galaxies from kinematic measurements of stars or gas using analytic considerations and the VELA suite of cosmological zoom-in simulations at $z=1-5$. We find that Jeans or hydrostatic equilibrium is approximately valid for galaxies of stellar masses above $M_\star \!\sim\! 10^{9.5}M_\odot$ out to $5$ effective radii ($R_e$). When both measurements of the rotation velocity $v_\phi$ and of the radial velocity dispersion $\sigma_r$ are available, the dynamical mass $M_{\rm dyn} \!\simeq\! G^{-1} V_c^2 r$ can be evaluated from the Jeans equation $V_c^2= v_\phi^2 + \alpha \sigma_r^2$ assuming cylindrical symmetry and a constant, isotropic $\sigma_r$. For spheroids, $\alpha$ is inversely proportional to the S\'ersic index $n$ and $\alpha \simeq 2.5$ within $R_e$ for the simulated galaxies. The prediction for a self-gravitating exponential disc, $\alpha = 3.36(r/R_e)$, is invalid in the simulations, where the dominant spheroid causes a weaker gradient from $\alpha \!\simeq\! 1$ at $R_e$ to 4 at $5R_e$. The correction in $\alpha$ for the stars due to the gradient in $\sigma_r(r)$ is roughly balanced by the effect of the aspherical potential, while the effect of anisotropy is negligible. When only the effective projected velocity dispersion $\sigma_l$ is available, the dynamical mass can be evaluated as $M_{\rm dyn} = K G^{-1} R_e \sigma_l^2$, where the virial factor $K$ is derived from $\alpha$ given the inclination and $v_\phi/\sigma_r$. We find that the standard value $K=5$ is approximately valid only when averaged over inclinations and for compact and thick discs, as it ranges from 4.5 to above 10 between edge-on and face-on projections. ",https://doi.org/10.1093/mnras/stab833,2010.04629v3,Yes,potent(1)
0000-0002-1762-9687,M. Michael Denner,University of Zurich,Efficient Learning of a One-dimensional Density Functional Theory,1970,"  Density functional theory underlies the most successful and widely used numerical methods for electronic structure prediction of solids. However, it has the fundamental shortcoming that the universal density functional is unknown. In addition, the computational result---energy and charge density distribution of the ground state---is useful for electronic properties of solids mostly when reduced to a band structure interpretation based on the Kohn-Sham approach. Here, we demonstrate how machine learning algorithms can help to free density functional theory from these limitations. We study a theory of spinless fermions on a one-dimensional lattice. The density functional is implicitly represented by a neural network, which predicts, besides the ground-state energy and density distribution, density-density correlation functions. At no point do we require a band structure interpretation. The training data, obtained via exact diagonalization, feeds into a learning scheme inspired by active learning, which minimizes the computational costs for data generation. We show that the network results are of high quantitative accuracy and, despite learning on random potentials, capture both symmetry-breaking and topological phase transitions correctly. ",https://doi.org/10.1103/PhysRevResearch.2.033388,2005.03014v2,Yes,potent(1)
0000-0002-1762-9687,M. Michael Denner,University of Zurich,Nature of unconventional pairing in the kagome superconductors   AV$_3$Sb$_5$,1970,"  The recent discovery of AV$_3$Sb$_5$ (A=K,Rb,Cs) has uncovered an intriguing arena for exotic Fermi surface instabilities in a kagome metal. Among them, superconductivity is found in the vicinity of multiple van Hove singularities, exhibiting indications of unconventional pairing. We show that the sublattice interference mechanism is central to understanding the formation of superconductivity in a kagome metal. Starting from an appropriately chosen minimal tight-binding model with multiple with multiple van Hove singularities close to the Fermi level for AV$_3$Sb$_5$, we provide a random phase approximation analysis of superconducting instabilities. Non-local Coulomb repulsion, the sublattice profile of the van Hove bands, and the bare interaction strength turn out to be the crucial parameters to determine the preferred pairing symmetry. Implications for potentially topological surface states are discussed, along with a proposal for additional measurements to pin down the nature of superconductivity in AV$_3$Sb$_5$. ",https://doi.org/10.1103/PhysRevLett.127.177001,2104.05671v2,Yes,potent(1)
0000-0002-1762-9687,M. Michael Denner,University of Zurich,Giant Strain Response of Charge Modulation and Singularity in a Kagome   Superconductor,1970,"  Tunable quantum materials hold great potential for applications. Of special interest are materials in which small lattice strain induces giant electronic responses. The kagome compounds AV3Sb5 (A = K, Rb, Cs) provide a testbed for such singular electronic states. In this study, through angle-resolved photoemission spectroscopy, we provide comprehensive spectroscopic measurements of the giant responses induced by compressive and tensile strains on the charge-density-wave (CDW) order parameter and high-order van Hove singularity (HO-VHS) in CsV3Sb5. We observe a tripling of the CDW gap magnitudes with ~1% strain, accompanied by the changes of both energy and mass of the saddle-point fermions. Our results reveal an anticorrelation between the unconventional CDW order parameter and the mass of a HO-VHS, and highlight the role of the latter in the superconducting pairing. The giant electronic responses uncover a rich strain tunability of the versatile kagome system in studying quantum interplays under lattice perturbations. ",Kein DOI-Link verfügbar,2402.16089v1,Yes,"versatile(1), potent(1)"
0000-0002-1762-9687,M. Michael Denner,University of Zurich,Van-Hove annihilation and nematic instability on a Kagome lattice,1970,"  Novel states of matter arise in quantum materials due to strong interactions among electrons. A nematic phase breaks the point group symmetry of the crystal lattice and is known to emerge in correlated materials. Here we report the observation of an intra-unit-cell nematic order and signatures of Pomeranchuk instability in the Kagome metal ScV6Sn6. Using scanning tunneling microscopy and spectroscopy, we reveal a stripe-like nematic order breaking the crystal rotational symmetry within the Kagome lattice itself. Moreover, we identify a set of van Hove singularities adhering to the Kagome layer electrons, which appear along one direction of the Brillouin zone while being annihilated along other high-symmetry directions, revealing a rotational symmetry breaking. Via detailed spectroscopic maps, we further observe an elliptical deformation of Fermi surface, which provides direct evidence for an electronically mediated nematic order. Our work not only bridges the gap between electronic nematicity and Kagome physics, but also sheds light on the potential mechanism for realizing symmetry-broken phases in correlated electron systems. ",https://doi.org/10.1038/s41563-024-01914-z,2406.13702v2,Yes,potent(1)
0000-0003-2761-6852,Thomas Keller,University of Zurich,Class 2 quotients of solvable linear groups,1970,"  Let $G$ be a finite group, and let $V$ be a completely reducible faithful $G$-module. By a result of Glauberman it has been known for a long time that if $G$ is nilpotent of class 2, then $|G| < |V|$. In this paper we generalize this result as follows. Assuming $G$ to be solvable, we show that the order of the maximal class 2 quotient of $G$ is strictly bounded above by $|V|$. ",Kein DOI-Link verfügbar,1710.01860v1,Yes,potent(1)
0000-0002-4403-4953,Zifan Jiang,University of Zurich,Privacy-Preserving Eye-tracking Using Deep Learning,1970,"  The expanding usage of complex machine learning methods like deep learning has led to an explosion in human activity recognition, particularly applied to health. In particular, as part of a larger body sensor network system, face and full-body analysis is becoming increasingly common for evaluating health status. However, complex models which handle private and sometimes protected data, raise concerns about the potential leak of identifiable data. In this work, we focus on the case of a deep network model trained on images of individual faces. Full-face video recordings taken from 493 individuals undergoing an eye-tracking based evaluation of neurological function were used. Outputs, gradients, intermediate layer outputs, loss, and labels were used as inputs for a deep network with an added support vector machine emission layer to recognize membership in the training data. The inference attack method and associated mathematical analysis indicate that there is a low likelihood of unintended memorization of facial features in the deep learning model. In this study, it is showed that the named model preserves the integrity of training data with reasonable confidence. The same process can be implemented in similar conditions for different models. ",Kein DOI-Link verfügbar,2106.09621v2,Yes,potent(1)
0000-0002-4403-4953,Zifan Jiang,University of Zurich,SignCLIP: Connecting Text and Sign Language by Contrastive Learning,1970,"  We present SignCLIP, which re-purposes CLIP (Contrastive Language-Image Pretraining) to project spoken language text and sign language videos, two classes of natural languages of distinct modalities, into the same space. SignCLIP is an efficient method of learning useful visual representations for sign language processing from large-scale, multilingual video-text pairs, without directly optimizing for a specific task or sign language which is often of limited size.   We pretrain SignCLIP on Spreadthesign, a prominent sign language dictionary consisting of ~500 thousand video clips in up to 44 sign languages, and evaluate it with various downstream datasets. SignCLIP discerns in-domain signing with notable text-to-video/video-to-text retrieval accuracy. It also performs competitively for out-of-domain downstream tasks such as isolated sign language recognition upon essential few-shot prompting or fine-tuning.   We analyze the latent space formed by the spoken language text and sign language poses, which provides additional linguistic insights. Our code and models are openly available. ",Kein DOI-Link verfügbar,2407.01264v1,Yes,notable(1)
0000-0003-3342-9906,Dixeena Lopez,University of Zurich,Prospects for detecting and localizing short-duration transient   gravitational waves from glitching neutron stars without electromagnetic   counterparts,1970,"  Neutron stars are known to show accelerated spin-up of their rotational frequency called a glitch. Highly magnetized rotating neutron stars (pulsars) are frequently observed by radio telescopes (and in other frequencies), where the glitch is observed as irregular arrival times of pulses which are otherwise very regular. A glitch in an isolated neutron star can excite the fundamental (f)-mode oscillations which can lead to gravitational wave generation. Electromagnetic observations of pulsars (and hence pulsar glitches) require the pulsar to be oriented so that the jet is pointed toward the detector, but this is not a requirement for gravitational wave emission which is more isotropic and not jetlike. Hence, gravitational wave observations have the potential to uncover nearby neutron stars where the jet is not pointed towards the Earth. In this work, we study the prospects of finding glitching neutron stars using a generic all-sky search for short-duration gravitational wave transients. The analysis covers the high-frequency range from $1-4$ kHz of LIGO-Virgo detectors for signals up to a few seconds. We set upper limits for the third observing run of the LIGO-Virgo detectors and present the prospects for upcoming observing runs of LIGO, Virgo, KAGRA, and LIGO India. We find the detectable glitch size will be around $10^{-5}$ Hz for the fifth observing run for pulsars with spin frequencies and distances comparable to the Vela pulsar. We also present the prospects of localizing the direction in the sky of these sources with gravitational waves alone, which can facilitate electromagnetic follow-up. We find that for the five detector configuration, the localization capability for a glitch size of $10^{-5}$ Hz is around $132\,\mathrm{deg}^{2}$ at $1\sigma$ confidence for $50\%$ of events with distance and spin frequency as that of Vela. ",https://doi.org/10.1103/PhysRevD.106.103037,2206.14515v3,Yes,potent(1)
0000-0003-0362-7848,David McKay,University of St Andrews,Metastable Bose-Einstein Condensation in a Strongly Correlated Optical   Lattice,1970,"  We experimentally and theoretically study the peak fraction of a Bose-Einstein condensate loaded into a cubic optical lattice as the lattice potential depth and entropy per particle are varied. This system is well-described by the superfluid regime of the Bose-Hubbard model, which allows for comparison with mean-field theories and exact quantum Monte Carlo (QMC) simulations. Despite correcting for systematic discrepancies between condensate fraction and peak fraction, we discover that the experiment consistently shows the presence of a condensate at temperatures higher than the critical temperature predicted by QMC simulations. This metastability suggests that turning on the lattice potential is non-adiabatic. To confirm this behavior, we compute the timescales for relaxation in this system, and find that equilibration times are comparable with the known heating rates. The similarity of these timescales implies that turning on the lattice potential adiabatically may be impossible. Our results point to the urgent need for a better theoretical and experimental understanding of the timescales for relaxation and adiabaticity in strongly interacting quantum gases, and the importance of model-independent probes of thermometry in optical lattices. ",https://doi.org/10.1103/PhysRevA.91.023625,1411.5593v2,Yes,potent(3)
0000-0001-5299-3292,Olexandr Konovalov,University of St Andrews,Don't mention it: An approach to assess challenges to using software   mentions for citation and discoverability research,1970,"  Datasets collecting software mentions from scholarly publications can potentially be used for research into the software that has been used in the published research, as well as into the practice of software citation. Recently, new software mention datasets with different characteristics have been published. We present an approach to assess the usability of such datasets for research on research software. Our approach includes sampling and data preparation, manual annotation for quality and mention characteristics, and annotation analysis. We applied it to two software mention datasets for evaluation based on qualitative observation. Doing this, we were able to find challenges to working with the selected datasets to do research. Main issues refer to the structure of the dataset, the quality of the extracted mentions (54% and 23% of mentions respectively are not to software), and software accessibility. While one dataset does not provide links to mentioned software at all, the other does so in a way that can impede quantitative research endeavors: (1) Links may come from different sources and each point to different software for the same mention. (2) The quality of the automatically retrieved links is generally poor (in our sample, 65.4% link the wrong software). (3) Links exist only for a small subset (in our sample, 20.5%) of mentions, which may lead to skewed or disproportionate samples. However, the greatest challenge and underlying issue in working with software mention datasets is the still suboptimal practice of software citation: Software should not be mentioned, it should be cited following the software citation principles. ",Kein DOI-Link verfügbar,2402.14602v1,Yes,"potent(1), scholarly(1)"
0000-0002-1933-0670,James Palmer,University of St Andrews,Record Maximum Oscillation Frequency in C-face Epitaxial Graphene   Transistors,1970,"  The maximum oscillation frequency (fmax) quantifies the practical upper bound for useful circuit operation. We report here an fmax of 70 GHz in transistors using epitaxial graphene grown on the C-face of SiC. This is a significant improvement over Si-face epitaxial graphene used in the prior high frequency transistor studies, exemplifying the superior electronics potential of C-face epitaxial graphene. Careful transistor design using a high {\kappa} dielectric T-gate and self-aligned contacts, further contributed to the record-breaking fmax. ",https://doi.org/10.1021/nl303587r,1302.3907v1,Yes,potent(1)
0000-0001-9114-3522,Arvydas Ruseckas,University of St Andrews,Triple Halide Wide Bandgap Perovskites for Efficient Indoor   Photovoltaics,1970,"  Indoor photovoltaics are receiving tremendous attention due to the continuous development of the Internet of Things (IoT). Here we report a triple anion (TA) perovskite CH3NH3PbI2.6(BrCl)0.2 with a tailored bandgap suitable for maximizing indoor light harvesting compared to methyl ammonium lead iodide CH3NH3PbI3. The best-performing TA perovskite indoor-photovoltaic device achieved a steady-state power conversion efficiency (PCE) of 25.1% with an output power density of ~ 75 microW/cm2 under 1000 lux indoor illumination   (0.3 mW/cm2 irradiance). This PCE is almost 40% higher than that of equivalent CH3NH3PbI3-based devices (PCE of 17.9%). Longer carrier lifetime, reduced density of trap states and improved crystalline quality were achieved by the triple anion alloying method. The decisive role of chlorine (Cl) in the better performance of TA-based indoor photovoltaic devices was further investigated by successively reducing the Cl content and correlating it with the corresponding photovoltaic device performance. Replacing the commonly used hole transporting layer of Spiro-MeOTAD with undoped P3HT was found to significantly reduce the current-voltage hysteresis under indoor lighting conditions. A graphene-coated textile fiber-based temperature sensor was successfully powered by the triple anion perovskite indoor photovoltaic devices. The results from the present study demonstrate a novel route to maximize the PCE of halide perovskite indoor photovoltaic devices and their potential for application in the IoT industry. ",Kein DOI-Link verfügbar,2301.13772v1,Yes,potent(1)
0000-0002-1552-2149,Emily Finer,University of St Andrews,Science Fiction Media Representations of Exoplanets: Portrayals of   Changing Astronomical Discoveries,1970,"  Interest in science fiction's (SF's) potential science communication use is hindered by concerns about SF misrepresenting science. This study addresses these concerns by asking how SF media reflects scientific findings in exoplanet science. A database of SF exoplanets was analysed using a Bayesian network to find interconnected interactions between planetary characterisation features and literary data. Results reveal SF exoplanets designed after the discovery of real exoplanets are less Earth-like, providing statistical evidence that SF incorporates rapidly-evolving science. Understanding SF's portrayal of science is crucial for its potential use in science communication. ",https://doi.org/10.22323/2.23010204,2405.00684v1,Yes,potent(2)
0000-0003-2519-0720,Matteo Rossi,"Politecnico di MILANO, Politecnico di Milano",Co-Simulation of Human-Robot Collaboration: from Temporal Logic to 3D   Simulation,1970,"  Human-Robot Collaboration (HRC) is rapidly replacing the traditional application of robotics in the manufacturing industry. Robots and human operators no longer have to perform their tasks in segregated areas and are capable of working in close vicinity and performing hybrid tasks -- performed partially by humans and by robots.   We have presented a methodology in an earlier work [16] to promote and facilitate formally modeling HRC systems, which are notoriously safety-critical. Relying on temporal logic modeling capabilities and automated model checking tools, we built a framework to formally model HRC systems and verify the physical safety of human operator against ISO 10218-2 [10] standard. In order to make our proposed formal verification framework more appealing to safety engineers, whom are usually not very fond of formal modeling and verification techniques, we decided to couple our model checking approach with a 3D simulator that demonstrates the potential hazardous situations to the safety engineers in a more transparent way. This paper reports our co-simulation approach, using Morse simulator [4] and Zot model checker [14]. ",https://doi.org/10.4204/EPTCS.319.1,2007.11737v1,Yes,potent(1)
0000-0003-1791-6640,Enrico Masoero,"Politecnico di MILANO, Politecnico di Milano",MASKE: A kinetic simulator of coupled chemo-mechanical processes driving   microstructural evolution,1970,"  The microstructure of materials evolves through chemical reactions and mechanical stress, often strongly coupled in phenomena such as pressure solution or crystallization pressure. This article presents MASKE: a simulator to address the challenge of modelling coupled chemo-mechanical processes in microstructures. MASKE represents solid phases as agglomerations of particles whose off-lattice displacements generate mechanical stress through interaction potentials. Particle precipitation and dissolution are sampled using Kinetic Monte Carlo, with original reaction rate equations derived from Transition State Theory and featuring contributions from mechanical interactions. Molecules in solution around the solid are modelled implicitly, through concentrations that change during microstructural evolution and define the saturation indexes for user-defined chemical reactions. The structure and implementation of the software are explained first. Then, two examples on a nanocrystal of calcium hydroxide address its chemical equilibrium and its mechanical response under a range of imposed strain rates, involving stress-driven dissolution and recrystallization. These examples highlight MASKE's distinctive ability to simulate strongly coupled chemo-mechanical processes. MASKE is available, open-source, on GitHub. ",Kein DOI-Link verfügbar,2311.06533v2,Yes,potent(1)
0000-0003-1791-6640,Enrico Masoero,"Politecnico di MILANO, Politecnico di Milano",Functional Renormalisation Group for Brownian Motion I: The Effective   Equations of Motion,1970,"  We use the functional Renormalisation Group (fRG) to describe the in and out of equilibrium dynamics of stochastic processes, governed by an overdamped Langevin equation. Exploiting the connection between Langevin dynamics and supersymmetric quantum mechanics in imaginary time, we write down renormalisation flow equations for the effective action, approximated in terms of the Local Potential Approximation and Wavefunction Renormalisation. We derive \textit{effective equations of motion} (EEOM) from the effective action (EA) $\Gamma$ for the average position $\left\langle x\right\rangle$, variance $\langle \left(x- \langle x \rangle\right)^2\rangle$ and covariance. The fRG flow equations outlined here provide a concrete way to compute the EA and thus solve the derived EEOM. The obtained effective potential should determine directly the exact equilibrium statistics, name the position, the variance, as well as all higher order cumulants of the equilibrium Boltzmann distribution. This first paper of a two part series is mostly concerned with setting up the necessary formalism while in part two we will numerically solve the equations derived her and assess their validity both in and out of equilibrium. ",Kein DOI-Link verfügbar,2008.00472v3,Yes,potent(2)
0000-0003-1791-6640,Enrico Masoero,"Politecnico di MILANO, Politecnico di Milano",Coarse-graining in time with the Functional Renormalisation Group:   Relaxation in Brownian Motion,1970,"  We apply the functional Renormalisation Group (fRG) to study relaxation in a stochastic process governed by an overdamped Langevin equation with one degree of freedom, exploiting the connection with supersymmetric quantum mechanics in imaginary time. After reviewing the functional integral formulation of the system and its underlying symmetries, including the resulting Ward-Takahashi identities for arbitrary initial conditions, we compute the effective action $\Gamma$ from the fRG, approximated in terms of the leading and subleading terms in the gradient expansion: the Local Potential Approximation and Wavefunction Renormalisation respectively. This is achieved by coarse-graining the thermal fluctuations in time resulting in e.g. an effective potential incorporating fluctuations at all timescales. We then use the resulting effective equations of motion to describe the decay of the covariance, and the relaxation of the average position and variance towards their equilibrium values at different temperatures. We use as examples a simple polynomial potential, an unequal Lennard-Jones type potential and a more complex potential with multiple trapping wells and barriers. We find that these are all handled well, with the accuracy of the approximations improving as the relaxation's spectral representation shifts to lower eigenvalues, in line with expectations about the validity of the gradient expansion. The spectral representation's range also correlates with temperature, leading to the conclusion that the gradient expansion works better for higher temperatures than lower ones. This work demonstrates the ability of the fRG to expedite the computation of statistical objects in otherwise long-timescale simulations, acting as a first step to more complicated systems. ",https://doi.org/10.1103/PhysRevE.106.054109,2102.04899v3,Yes,potent(5)
0000-0002-7014-972X,Mattia Corti,"Politecnico di MILANO, Politecnico di Milano",A discontinuous Galerkin method for the three-dimensional heterodimer   model with application to prion-like proteins' dynamics,1970,"  Neurocognitive disorders, such as Alzheimer's and Parkinson's, have a wide social impact. These proteinopathies involve misfolded proteins accumulating into neurotoxic aggregates. Mathematical and computational models describing the prion-like dynamics offer an analytical basis to study the diseases' evolution and a computational framework for exploring potential therapies. This work focuses on the heterodimer model in a three-dimensional setting, a reactive-diffusive system of nonlinear partial differential equations describing the evolution of both healthy and misfolded proteins. We investigate traveling wave solutions and diffusion-driven instabilities as a mechanism of neurotoxic pattern formation. For the considered mathematical model, we propose a space discretization, relying on the Discontinuous Galerkin method on polytopal/polyhedral grids, allowing high-order accuracy and flexible handling of the complicated brain's geometry. Further, we present a priori error estimates for the semi-discrete formulation and we perform convergence tests to verify the theoretical results. Finally, we conduct simulations using realistic data on a three-dimensional brain mesh reconstructed from medical images. ",Kein DOI-Link verfügbar,2407.16065v1,Yes,potent(1)
0000-0002-7014-972X,Mattia Corti,"Politecnico di MILANO, Politecnico di Milano",A high-order discontinuous Galerkin method for the numerical modeling of   epileptic seizures,1970,"  Epilepsy is a clinical neurological disorder characterized by recurrent and spontaneous seizures consisting of abnormal high-frequency electrical activity in the brain. In this condition, the transmembrane potential dynamics are characterized by rapid and sharp wavefronts traveling along the heterogeneous and anisotropic conduction pathways of the brain. This work employs the monodomain model, coupled with specific neuronal ionic models characterizing ion concentration dynamics, to mathematically describe brain tissue electrophysiology in grey and white matter at the organ scale. This multiscale model is discretized in space with the high-order discontinuous Galerkin method on polygonal and polyhedral grids (PolyDG) and advanced in time with a Crank-Nicolson scheme. This ensures, on the one hand, efficient and accurate simulations of the high-frequency electrical activity that is responsible for epileptic seizure and, on the other hand, keeps reasonably low the computational costs by a suitable combination of high-order approximations and agglomerated polytopal meshes. We numerically investigate synthetic test cases on a two-dimensional heterogeneous squared domain discretized with a polygonal grid, and on a two-dimensional brainstem in a sagittal plane with an agglomerated polygonal grid that takes full advantage of the flexibility of the PolyDG approximation of the semidiscrete formulation. Finally, we provide a theoretical analysis of stability and an a-priori convergence analysis for a simplified mathematical problem. ",Kein DOI-Link verfügbar,2401.14310v1,Yes,potent(1)
0000-0002-6527-0779,Emilia Ambrosini,"Politecnico di MILANO, Politecnico di Milano",Sensorless model-based tension control for a cable-driven exosuit,1970,"  Cable-driven exosuits have the potential to support individuals with motor disabilities across the continuum of care. When supporting a limb with a cable, force sensors are often used to measure tension. However, force sensors add cost, complexity, and distal components. This paper presents a design and control approach to remove the force sensor from an upper limb cable-driven exosuit. A mechanical design for the exosuit was developed to maximize passive transparency. Then, a data-driven friction identification was conducted on a mannequin test bench to design a model-based tension controller. Seventeen healthy participants raised and lowered their right arms to evaluate tension tracking, movement quality, and muscular effort. Questionnaires on discomfort, physical exertion, and fatigue were collected. The proposed strategy allowed tracking the desired assistive torque with an RMSE of 0.71 Nm (18%) at 50% gravity support. During the raising phase, the EMG signals of the anterior deltoid, trapezius, and pectoralis major were reduced on average compared to the no-suit condition by 30%, 38%, and 38%, respectively. The posterior deltoid activity was increased by 32% during lowering. Position tracking was not significantly altered, whereas movement smoothness significantly decreased. This work demonstrates the feasibility and effectiveness of removing the force sensor from a cable-driven exosuit. A significant increase in discomfort in the lower neck and right shoulder indicated that the ergonomics of the suit could be improved. Overall this work paves the way towards simpler and more affordable exosuits. ",Kein DOI-Link verfügbar,2406.18412v1,Yes,potent(1)
0000-0002-1285-2724,Alberto Corigliano,"Politecnico di MILANO, Politecnico di Milano",Fully convolutional networks for structural health monitoring through   multivariate time series classification,1970,"  We propose a novel approach to Structural Health Monitoring (SHM), aiming at the automatic identification of damage-sensitive features from data acquired through pervasive sensor systems. Damage detection and localization are formulated as classification problems, and tackled through Fully Convolutional Networks (FCNs). A supervised training of the proposed network architecture is performed on data extracted from numerical simulations of a physics-based model (playing the role of digital twin of the structure to be monitored) accounting for different damage scenarios. By relying on this simplified model of the structure, several load conditions are considered during the training phase of the FCN, whose architecture has been designed to deal with time series of different length. The training of the neural network is done before the monitoring system starts operating, thus enabling a real time damage classification. The numerical performances of the proposed strategy are assessed on a numerical benchmark case consisting of an eight-story shear building subjected to two load types, one of which modeling random vibrations due to low-energy seismicity. Measurement noise has been added to the responses of the structure to mimic the outputs of a real monitoring system. Extremely good classification capacities are shown: among the nine possible alternatives (represented by the healthy state and by a damage at any floor), damage is correctly classified in up to 95% of cases, thus showing the strong potential of the proposed approach in view of the application to real-life cases. ",Kein DOI-Link verfügbar,2002.07032v1,Yes,potent(1)
0000-0002-1285-2724,Alberto Corigliano,"Politecnico di MILANO, Politecnico di Milano",Mastering truss structure optimization with tree search,1970,"  This study investigates the combined use of generative grammar rules and Monte Carlo Tree Search (MCTS) for optimizing truss structures. Our approach accommodates intermediate construction stages characteristic of progressive construction settings. We demonstrate the significant robustness and computational efficiency of our approach compared to alternative reinforcement learning frameworks from previous research activities, such as Q-learning or deep Q-learning. These advantages stem from the ability of MCTS to strategically navigate large state spaces, leveraging the upper confidence bound for trees formula to effectively balance exploitation-exploration trade-offs. We also emphasize the importance of early decision nodes in the search tree, reflecting design choices crucial for identifying the global optimum. Additionally, we show how MCTS dynamically adapts to complex and extensive state spaces without significantly affecting solution quality. While the focus of this paper is on truss optimization, our findings suggest MCTS as a powerful tool for addressing other increasingly complex engineering applications. ",Kein DOI-Link verfügbar,2406.06145v2,Yes,strategically(1)
0000-0002-1285-2724,Alberto Corigliano,"Politecnico di MILANO, Politecnico di Milano",Graded metasurface for enhanced sensing and energy harvesting,1970,"  In elastic wave systems, combining the powerful concepts of resonance and spatial grading within structured surface arrays enable resonant metasurfaces to exhibit broadband wave focusing, mode conversion from surface (Rayleigh) waves to bulk (shear) waves, and spatial frequency selection. Devices built around these concepts allow for precise control of surface waves, often with structures that are subwavelength, and utilise rainbow trapping that separates the signal spatially by frequency. Rainbow trapping yields large amplifications of displacement at the resonator positions where each frequency component accumulates. We investigate whether this amplification, and the associated control, can be used to create energy harvesting devices; the potential advantages and disadvantages of using graded resonant devices as energy harvesters is considered. We concentrate upon elastic plate models for which the A0 mode dominates, and take advantage of the large displacement amplitudes in graded resonant arrays of rods, to design innovative metasurfaces that focus waves for enhanced piezoelectric sensing and energy harvesting. Numerical simulation allows us to identify the advantages of such graded metasurface devices and quantify its efficiency, we also develop accurate models of the phenomena and extend our analysis to that of an elastic half-space and Rayleigh surface waves. ",Kein DOI-Link verfügbar,1907.09297v1,Yes,"innovative(1), potent(1)"
0000-0001-7103-2788,Maurizio Ferrari Dacrema,"Politecnico di MILANO, Politecnico di Milano",Are We Really Making Much Progress? A Worrying Analysis of Recent Neural   Recommendation Approaches,1970,"  Deep learning techniques have become the method of choice for researchers working on algorithmic aspects of recommender systems. With the strongly increased interest in machine learning in general, it has, as a result, become difficult to keep track of what represents the state-of-the-art at the moment, e.g., for top-n recommendation tasks. At the same time, several recent publications point out problems in today's research practice in applied machine learning, e.g., in terms of the reproducibility of the results or the choice of the baselines when proposing new models. In this work, we report the results of a systematic analysis of algorithmic proposals for top-n recommendation tasks. Specifically, we considered 18 algorithms that were presented at top-level research conferences in the last years. Only 7 of them could be reproduced with reasonable effort. For these methods, it however turned out that 6 of them can often be outperformed with comparably simple heuristic methods, e.g., based on nearest-neighbor or graph-based techniques. The remaining one clearly outperformed the baselines but did not consistently outperform a well-tuned non-neural linear ranking method. Overall, our work sheds light on a number of potential problems in today's machine learning scholarship and calls for improved scientific practices in this area. Source code of our experiments and full results are available at: https://github.com/MaurizioFD/RecSys2019_DeepLearning_Evaluation. ",https://doi.org/10.1145/3298689.3347058,1907.06902v3,Yes,potent(1)
0000-0001-7103-2788,Maurizio Ferrari Dacrema,"Politecnico di MILANO, Politecnico di Milano",Critically Examining the Claimed Value of Convolutions over User-Item   Embedding Maps for Recommender Systems,1970,"  In recent years, algorithm research in the area of recommender systems has shifted from matrix factorization techniques and their latent factor models to neural approaches. However, given the proven power of latent factor models, some newer neural approaches incorporate them within more complex network architectures. One specific idea, recently put forward by several researchers, is to consider potential correlations between the latent factors, i.e., embeddings, by applying convolutions over the user-item interaction map. However, contrary to what is claimed in these articles, such interaction maps do not share the properties of images where Convolutional Neural Networks (CNNs) are particularly useful. In this work, we show through analytical considerations and empirical evaluations that the claimed gains reported in the literature cannot be attributed to the ability of CNNs to model embedding correlations, as argued in the original papers. Moreover, additional performance evaluations show that all of the examined recent CNN-based models are outperformed by existing non-neural machine learning techniques or traditional nearest-neighbor approaches. On a more general level, our work points to major methodological issues in recommender systems research. ",https://doi.org/10.1145/3340531.3411901,2007.11893v2,Yes,potent(1)
0000-0001-7103-2788,Maurizio Ferrari Dacrema,"Politecnico di MILANO, Politecnico di Milano",Feature Selection for Recommender Systems with Quantum Computing,1970,"  The promise of quantum computing to open new unexplored possibilities in several scientific fields has been long discussed, but until recently the lack of a functional quantum computer has confined this discussion mostly to theoretical algorithmic papers. It was only in the last few years that small but functional quantum computers have become available to the broader research community. One paradigm in particular, quantum annealing, can be used to sample optimal solutions for a number of NP-hard optimization problems represented with classical operations research tools, providing an easy access to the potential of this emerging technology. One of the tasks that most naturally fits in this mathematical formulation is feature selection. In this paper, we investigate how to design a hybrid feature selection algorithm for recommender systems that leverages the domain knowledge and behavior hidden in the user interactions data. We represent the feature selection as an optimization problem and solve it on a real quantum computer, provided by D-Wave. The results indicate that the proposed approach is effective in selecting a limited set of important features and that quantum computers are becoming powerful enough to enter the wider realm of applied science. ",https://doi.org/10.3390/e23080970,2110.05089v1,Yes,potent(1)
0000-0001-7103-2788,Maurizio Ferrari Dacrema,"Politecnico di MILANO, Politecnico di Milano",Adaptive Learning for Quantum Linear Regression,1970,"  The recent availability of quantum annealers as cloud-based services has enabled new ways to handle machine learning problems, and several relevant algorithms have been adapted to run on these devices. In a recent work, linear regression was formulated as a quadratic binary optimization problem that can be solved via quantum annealing. Although this approach promises a computational time advantage for large datasets, the quality of the solution is limited by the necessary use of a precision vector, used to approximate the real-numbered regression coefficients in the quantum formulation. In this work, we focus on the practical challenge of improving the precision vector encoding: instead of setting an array of generic values equal for all coefficients, we allow each one to be expressed by its specific precision, which is tuned with a simple adaptive algorithm. This approach is evaluated on synthetic datasets of increasing size, and linear regression is solved using the D-Wave Advantage quantum annealer, as well as classical solvers. To the best of our knowledge, this is the largest dataset ever evaluated for linear regression on a quantum annealer. The results show that our formulation is able to deliver improved solution quality in all instances, and could better exploit the potential of current quantum devices. ",Kein DOI-Link verfügbar,2408.02833v1,Yes,potent(1)
0000-0002-6155-2031,Jacopo Maria De Ponti,"Politecnico di MILANO, Politecnico di Milano",Analytical solutions for Bloch waves in resonant phononic crystals: Deep   subwavelength energy splitting and mode steering between topologically   protected interfacial and edge states,1970,"  We derive analytical solutions based on singular Green's functions, which enable efficient computations of scattering simulations or Floquet-Bloch dispersion relations for waves propagating through an elastic plate, whose surface is patterned by periodic arrays of elastic beams. Our methodology is versatile and allows us to solve a range of problems regarding arrangements of multiple beams per primitive cell, over Bragg to deep-subwavelength scales; we cross-verify against finite element numerical simulations to gain further confidence in our approach, which relies upon the hypothesis of Euler-Bernoulli beam theory considerably simplifying continuity conditions such that each beam can be replaced by point forces and moments applied to the neutral plane of the plate. The representations of Green's functions by Fourier series or Fourier transforms readily follows, yielding rapid and accurate analytical schemes. The accuracy and flexibility of our solutions are demonstrated by engineering topologically non-trivial states, from primitive cells with broken spatial symmetries, following the phononic analogue of the Quantum Valley Hall Effect (QVHE). Topologically protected states are produced and coexist along: interfaces between adjoining chiral-mirrored bulk media and edges between one such chiral bulk and the surrounding bare elastic plate, allowing topological circuits to be designed with robust waveguiding; these topologically non-trivial states exist within near flexural resonances of the constituent beams of the phononic crystal, and hence can be tuned into a deep-subwavelength regime. ",Kein DOI-Link verfügbar,2207.13118v2,Yes,versatile(1)
0000-0001-9543-0422,Valeria Russo,"Politecnico di Milano, Politecnico di Milano IT","Structural, Electronic, and Vibrational Properties of 2D Graphdiyne-Like   Carbon Nanonetwork Synthesized on Au(111): Implications for the Engineering   of sp-sp2 Carbon Nanostructures",1970,"  Graphdiyne, atomically-thin 2D carbon nanostructure based on sp-sp2 hybridization, is an appealing system potentially showing outstanding mechanical and optoelectronic properties. Surface-catalyzed coupling of halogenated sp-carbon-based molecular precursors represents a promising bottom-up strategy to fabricate extended 2D carbon systems with engineered structure on metallic substrates. Here, we investigate the atomic-scale structure and electronic and vibrational properties of an extended graphdiyne-like sp-sp2 carbon nanonetwork grown on Au(111) by means of on-surface synthesis. The formation of such 2D nanonetwork at its different stages as a function of the annealing temperature after the deposition is monitored by scanning tunneling microscopy (STM), Raman spectroscopy and combined with density functional theory (DFT) calculations. High-resolution STM imaging and the high sensitivity of Raman spectroscopy to the bond nature provide a unique strategy to unravel the atomic-scale properties of sp-sp2 carbon nanostructures. We show that hybridization between the 2D carbon nanonetwork and the underlying substrate states strongly affects its electronic and vibrational properties, modifying substantially the density of states and the Raman spectrum compared to the free standing system. This opens the way to the modulation of the electronic properties with significant prospects in future applications as active nanomaterials for catalysis, photoconversion and carbon-based nanoelectronics. ",https://doi.org/10.1021/acsanm.0c02665,2011.12410v1,Yes,potent(1)
0000-0001-9543-0422,Valeria Russo,"Politecnico di Milano, Politecnico di Milano IT",In situ surface-enhanced Raman spectroscopy to investigate polyyne   formation during pulsed laser ablation in liquid,1970,"  The synthesis of polyynes during their formation by pulsed laser ablation in liquid (i.e. acetonitrile) has been analyzed by in-situ surface-enhanced Raman spectroscopy (SERS). A polyethylene pellet, functionalized with silver nanoparticles and placed into the ablation medium, served as SERS active surface. This innovative approach granted the possibility to investigate the dynamics of formation and degradation of polyynes with a time-resolution of a few seconds, starting from the early stages of ablation when the concentration is low. The processes occurring during the synthesis have been studied comparing the in-situ SERS signal of polyynes and byproducts in the solution. The different kinetics of short and long polyynes have been investigated by their in-situ SERS signal, exploring the final distribution of chain lengths. Ex situ UV-Vis and high-performance liquid chromatography confirmed the observations gained from in-situ SERS data and validated this innovative in-situ and in-operando analysis. ",https://doi.org/10.1016/j.carbon.2021.12.060,2110.13734v2,Yes,innovative(2)
0000-0002-1853-1614,Daniele Ielmini,"Politecnico di Milano, Politecnico di Milano - Milano Leonardo",2022 Roadmap on Neuromorphic Computing and Engineering,1970,"  Modern computation based on the von Neumann architecture is today a mature cutting-edge science. In the Von Neumann architecture, processing and memory units are implemented as separate blocks interchanging data intensively and continuously. This data transfer is responsible for a large part of the power consumption. The next generation computer technology is expected to solve problems at the exascale with 1018 calculations each second. Even though these future computers will be incredibly powerful, if they are based on von Neumann type architectures, they will consume between 20 and 30 megawatts of power and will not have intrinsic physically built-in capabilities to learn or deal with complex data as our brain does. These needs can be addressed by neuromorphic computing systems which are inspired by the biological concepts of the human brain. This new generation of computers has the potential to be used for the storage and processing of large amounts of digital information with much lower power consumption than conventional processors. Among their potential future applications, an important niche is moving the control from data centers to edge devices.   The aim of this Roadmap is to present a snapshot of the present state of neuromorphic technology and provide an opinion on the challenges and opportunities that the future holds in the major areas of neuromorphic technology, namely materials, devices, neuromorphic circuits, neuromorphic algorithms, applications, and ethics. The Roadmap is a collection of perspectives where leading researchers in the neuromorphic community provide their own view about the current state and the future challenges. We hope that this Roadmap will be a useful resource to readers outside this field, for those who are just entering the field, and for those who are well established in the neuromorphic community. https://doi.org/10.1088/2634-4386/ac4a83 ",https://doi.org/10.1088/2634-4386/ac4a83,2105.05956v3,Yes,potent(2)
0000-0003-0168-6975,Francesco Romano,"Politecnico di Milano, Politecnico di Milano Dipartimento di Energia",A Predictive Momentum-Based Whole-Body Torque Controller: Theory and   Simulations for the iCub Stepping,1970,"  When balancing, a humanoid robot can be easily subjected to unexpected disturbances like external pushes. In these circumstances, reactive movements as steps become a necessary requirement in order to avoid potentially harmful falling states. In this paper we conceive a Model Predictive Controller which determines a desired set of contact wrenches by predicting the future evolution of the robot, while taking into account constraints switching in case of steps. The control inputs computed by this strategy, namely the desired contact wrenches, are directly obtained on the robot through a modification of the momentum-based whole-body torque controller currently implemented on iCub. The proposed approach is validated through simulations in a stepping scenario, revealing high robustness and reliability when executing a recovery strategy. ",Kein DOI-Link verfügbar,1705.10635v2,Yes,potent(1)
0000-0003-0168-6975,Francesco Romano,"Politecnico di Milano, Politecnico di Milano Dipartimento di Energia",RIPTIDE: a novel recoil-proton track imaging detector for fast neutrons,1970,"  Neutron detectors are an essential tool for the development of many research fields, as nuclear, particle and astroparticle physics as well as radiotherapy and radiation safety. Since neutrons cannot directly ionize, their detection is only possible via nuclear reactions. Consequently, neutron-based experimental techniques are related to the detection of charged particle or electromagnetic radiation originating from neutron-induced reactions. The study of fast neutrons is often based on the neutron-proton elastic scattering reaction. In this case, the ionization induced by the recoil protons in a hydrogenous material constitutes the basic information for the design and development of neutron detectors. Although experimental techniques have continuously improved and refined, so far, proton-recoil track imaging is still weak in laboratory rate environments because of the extremely small detection efficiency. To address this deficiency, we propose a novel recoil-proton track imaging system in which the light deriving from a fast scintillation signal is used to perform a complete reconstruction in space and time of the event. In particular, we report the idea of RIPTIDE (RecoIl Proton Track Imaging DEtector): an innovative system which combines a plastic scintillator coupled to imaging devices, based on CMOS technology, or Micro Channel Plate sensors. The proposed apparatus aims at providing neutron spectrometry capability by stereoscopically imaging the recoil-protons tracks, correlating the spatial information with the time information. ",https://doi.org/10.1088/1748-0221/16/12/C12013,2109.11543v2,Yes,innovative(1)
0000-0003-0168-6975,Francesco Romano,"Politecnico di Milano, Politecnico di Milano Dipartimento di Energia",Design and First Tests of the Trapped Electrons Experiment T-REX,1970,"  Gyrotrons are the only sources used for electron cyclotron resonance heating (ECRH). Past gyrotron experiments have highlighted some instability issues, leading to restricted operating ranges. One major cause is the possible presence of trapped electrons within the gyrotron's magnetron injection gun (MIG) region, resulting in undesired currents and subsequent operational failures. Hereby, we present the design and initial experimental findings of the TRapped Electrons eXperiment (T-REX), a novel and unique basic plasma experiment constructed at the Swiss Plasma Center to investigate the underlying physics of trapped electron clouds, therefore entering the realm of non-neutral plasmas. The primary objective is to explore the formation and evolution of electron clouds in gyrotron MIG designs. Typical MIG conditions are replicated in T-REX in terms of geometries, electric and magnetic fields, and supported by kinetic simulations via the 2D Particle-in-Cell (PIC) code FENNECS. T-REX is characterized by two coaxial electrodes, placed in a vacuum chamber on top of a superconducting magnet, with an applied external radial electric field (0.1 - 2 MV/m), and a nearly axial magnetic field (B<0.4T) inducing azimuthal drifts and confining electrons with energies between 0.1-1 keV. The experimental diagnostics include current, voltage, and Faraday probes, and in future plans imaging via phosphor screen, optical emission spectroscopy, streak camera imaging, and, potentially, electric field distribution via the Stark effect. Moreover, T-REX is also designed to be capable of detecting potential growth of diocotron modes via such diagnostics in combination with a segmented outer electrode. Through T-REX, we aim to gain valuable insights into the trapping and dynamics of electrons within MIG regions, paving the way for improved gyrotron performance and reliability in magnetic fusion reactors. ",Kein DOI-Link verfügbar,2406.19123v1,Yes,potent(2)
0000-0003-0168-6975,Francesco Romano,"Politecnico di Milano, Politecnico di Milano Dipartimento di Energia",Barkour: Benchmarking Animal-level Agility with Quadruped Robots,1970,"  Animals have evolved various agile locomotion strategies, such as sprinting, leaping, and jumping. There is a growing interest in developing legged robots that move like their biological counterparts and show various agile skills to navigate complex environments quickly. Despite the interest, the field lacks systematic benchmarks to measure the performance of control policies and hardware in agility. We introduce the Barkour benchmark, an obstacle course to quantify agility for legged robots. Inspired by dog agility competitions, it consists of diverse obstacles and a time based scoring mechanism. This encourages researchers to develop controllers that not only move fast, but do so in a controllable and versatile way. To set strong baselines, we present two methods for tackling the benchmark. In the first approach, we train specialist locomotion skills using on-policy reinforcement learning methods and combine them with a high-level navigation controller. In the second approach, we distill the specialist skills into a Transformer-based generalist locomotion policy, named Locomotion-Transformer, that can handle various terrains and adjust the robot's gait based on the perceived environment and robot states. Using a custom-built quadruped robot, we demonstrate that our method can complete the course at half the speed of a dog. We hope that our work represents a step towards creating controllers that enable robots to reach animal-level agility. ",Kein DOI-Link verfügbar,2305.14654v1,Yes,versatile(1)
0000-0001-9593-6757,Alexander Leithes,Queen Mary University of London,PYESSENCE - Generalised Coupled Quintessence Linear Perturbation Python   Code - User Guide,1970,  This paper is a guide to the installation and use of the Python package PYESSENCE. PYESSENCE is designed to evolve linear perturbations to Coupled Quintessence models with a arbitrary number of cold dark matter (CDM) fluids and dark energy (DE) scalar fields as dictated by a given model. The equations are sufficiently general to allow for more exotic dark matter with a non-zero equation of state. Several example uses are included in order to demonstrate typical functionality to the potential user. PYESSENCE is released under an open source modified BSD license and is available on Bitbucket. ,Kein DOI-Link verfügbar,1608.00910v3,Yes,potent(1)
0000-0002-9809-8119,Hanxiao Wang,Queen Mary University of London,Diffusion Deepfake,1970,"  Recent progress in generative AI, primarily through diffusion models, presents significant challenges for real-world deepfake detection. The increased realism in image details, diverse content, and widespread accessibility to the general public complicates the identification of these sophisticated deepfakes. Acknowledging the urgency to address the vulnerability of current deepfake detectors to this evolving threat, our paper introduces two extensive deepfake datasets generated by state-of-the-art diffusion models as other datasets are less diverse and low in quality. Our extensive experiments also showed that our dataset is more challenging compared to the other face deepfake datasets. Our strategic dataset creation not only challenge the deepfake detectors but also sets a new benchmark for more evaluation. Our comprehensive evaluation reveals the struggle of existing detection methods, often optimized for specific image domains and manipulations, to effectively adapt to the intricate nature of diffusion deepfakes, limiting their practical utility. To address this critical issue, we investigate the impact of enhancing training data diversity on representative detection methods. This involves expanding the diversity of both manipulation techniques and image domains. Our findings underscore that increasing training data diversity results in improved generalizability. Moreover, we propose a novel momentum difficulty boosting strategy to tackle the additional challenge posed by training data heterogeneity. This strategy dynamically assigns appropriate sample weights based on learning difficulty, enhancing the model's adaptability to both easy and challenging samples. Extensive experiments on both existing and newly proposed benchmarks demonstrate that our model optimization approach surpasses prior alternatives significantly. ",Kein DOI-Link verfügbar,2404.01579v1,Yes,intricate(1)
0000-0002-9809-8119,Hanxiao Wang,Queen Mary University of London,E$^3$-Net: Efficient E(3)-Equivariant Normal Estimation Network,1970,"  Point cloud normal estimation is a fundamental task in 3D geometry processing. While recent learning-based methods achieve notable advancements in normal prediction, they often overlook the critical aspect of equivariance. This results in inefficient learning of symmetric patterns. To address this issue, we propose E3-Net to achieve equivariance for normal estimation. We introduce an efficient random frame method, which significantly reduces the training resources required for this task to just 1/8 of previous work and improves the accuracy. Further, we design a Gaussian-weighted loss function and a receptive-aware inference strategy that effectively utilizes the local properties of point clouds. Our method achieves superior results on both synthetic and real-world datasets, and outperforms current state-of-the-art techniques by a substantial margin. We improve RMSE by 4% on the PCPNet dataset, 2.67% on the SceneNN dataset, and 2.44% on the FamousShape dataset. ",Kein DOI-Link verfügbar,2406.00347v1,Yes,notable(1)
0000-0002-1170-4885,Eder Leao Fernandes,Queen Mary University of London,The Road to BOFUSS: The Basic OpenFlow User-space Software Switch,1970,"  Software switches are pivotal in the Software-Defined Networking (SDN) paradigm, particularly in the early phases of development, deployment and testing. Currently, the most popular one is Open vSwitch (OVS), leveraged in many production-based environments. However, due to its kernel-based nature, OVS is typically complex to modify when additional features or adaptation is required. To this regard, a simpler user-space is key to perform these modifications.   In this article, we present a rich overview of BOFUSS, the basic OpenFlow user-space software switch. BOFUSS has been widely used in the research community for diverse reasons, but it lacked a proper reference document. For this purpose, we describe the switch, its history, architecture, uses cases and evaluation, together with a survey of works that leverage this switch. The main goal is to provide a comprehensive overview of the switch and its characteristics. Although the original BOFUSS is not expected to surpass the high performance of OVS, it is a useful complementary artifact that provides some OpenFlow features missing in OVS and it can be easily modified for extended functionality. Moreover, enhancements provided by the BEBA project brought the performance from BOFUSS close to OVS. In any case, this paper sheds light to researchers looking for the trade-offs between performance and customization of BOFUSS. ",Kein DOI-Link verfügbar,1901.06699v1,Yes,pivotal(1)
0000-0003-1840-9616,Joseph Doyle,Queen Mary University of London,IPA: Inference Pipeline Adaptation to Achieve High Accuracy and   Cost-Efficiency,1970,"  Efficiently optimizing multi-model inference pipelines for fast, accurate, and cost-effective inference is a crucial challenge in machine learning production systems, given their tight end-to-end latency requirements. To simplify the exploration of the vast and intricate trade-off space of latency, accuracy, and cost in inference pipelines, providers frequently opt to consider one of them. However, the challenge lies in reconciling latency, accuracy, and cost trade-offs. To address this challenge and propose a solution to efficiently manage model variants in inference pipelines, we present IPA, an online deep learning Inference Pipeline Adaptation system that efficiently leverages model variants for each deep learning task. Model variants are different versions of pre-trained models for the same deep learning task with variations in resource requirements, latency, and accuracy. IPA dynamically configures batch size, replication, and model variants to optimize accuracy, minimize costs, and meet user-defined latency Service Level Agreements (SLAs) using Integer Programming. It supports multi-objective settings for achieving different trade-offs between accuracy and cost objectives while remaining adaptable to varying workloads and dynamic traffic patterns. Navigating a wider variety of configurations allows \namex{} to achieve better trade-offs between cost and accuracy objectives compared to existing methods. Extensive experiments in a Kubernetes implementation with five real-world inference pipelines demonstrate that IPA improves end-to-end accuracy by up to 21% with a minimal cost increase. The code and data for replications are available at https://github.com/reconfigurable-ml-pipeline/ipa. ",https://doi.org/10.5070/SR34163500,2308.12871v3,Yes,intricate(1)
0000-0003-2477-6623,Wilke van der Schee,Utrecht University,A new dynamical instability in Anti-de-Sitter spacetime,1970,"  We present fully dynamical solutions to Einstein-scalar theory in asymptotically Anti-de-Sitter spacetime with a scalar potential containing particularly rich physics. Depending on one parameter in the potential we find an especially interesting regime, which exhibits a thermodynamically stable, but dynamically unstable black brane, even at zero momentum. We show this using the non-linear dynamics, and give a clear interpretation in terms of the spectrum of linearized perturbations. Our results translate directly to their dual strongly coupled non-conformal field theories. ",https://doi.org/10.1103/PhysRevD.94.061901,1603.07724v2,Yes,potent(2)
0000-0003-2477-6623,Wilke van der Schee,Utrecht University,Non-equilibrium steady state formation in 3+1 dimensions,1970,"  We present the first holographic simulations of non-equilibrium steady state formation in strongly coupled $\mathcal{N}=4$ SYM theory in 3+1 dimensions. We initially join together two thermal baths at different temperatures and chemical potentials and compare the subsequent evolution of the combined system to analytic solutions of the corresponding Riemann problem and to numeric solutions of ideal and viscous hydrodynamics. The time evolution of the energy density that we obtain holographically is consistent with the combination of a shock and a rarefaction wave: A shock wave moves towards the cold bath, and a smooth broadening wave towards the hot bath. Between the two waves emerges a steady state with constant temperature and flow velocity, both of which are accurately described by a shock+rarefaction wave solution of the Riemann problem. In the steady state region, a smooth crossover develops between two regions of different charge density. This is reminiscent of a contact discontinuity in the Riemann problem. We also obtain results for the entanglement entropy of regions crossed by shock and rarefaction waves and find both of them to closely follow the evolution of the energy density. ",https://doi.org/10.21468/SciPostPhys.11.3.047,2103.10435v2,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Yang-Mills Interactions and Gravity in Terms of Clifford Algebra,1970,"  A model of Yang-Mills interactions and gravity in terms of the Clifford algebra Cl(0,6) is presented. The gravity and Yang-Mills actions are formulated as different order terms in a generalized action. The feebleness of gravity as well as the smallness of the cosmological constant and theta terms are discussed at the classical level. The invariance groups, including the de Sitter and the Pati-Salam SU(4) subgroups, consist of gauge transformations from either side of an algebraic spinor. Upon symmetry breaking via the Higgs fields, the remaining symmetries are the Lorentz SO(1,3), color SU(3), electromagnetic U(1)_EM, and an additional U(1). The first generation leptons and quarks are identified with even and odd parts of spinor idempotent projections. There are still several shortcomings with the current model. Further research is needed to fully recover the standard model results. ",https://doi.org/10.1007/s00006-010-0243-7,1008.0122v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Electroweak and Majorana Sector Higgs Bosons and Pseudo-Nambu-Goldstone   Bosons,1970,"  We propose a Clifford algebra based model, which treats both gravity and Yang-Mills interactions as gauge fields. There are two sectors of boson fields as electroweak and Majorana bosons. The electroweak boson sector induces fermion masses via spontaneous symmetry breaking. It is composed of scalar Higgs, pseudoscalar Higgs, and antisymmetric tensor components. The Majorana boson sector contributes to flavor mixing and Majorana masses of right-handed neutrinos. It is comprised of neutrino Higgs and pseudo-Nambu-Goldstone bosons. The LHC 750 GeV diphoton resonance might possibly be identified as a Majorana sector pseudo-Nambu-Goldstone boson, which results from spontaneous symmetry breaking of a flavor-related global U(1) symmetry involving four-fermion condensation of right-handed leptons and quarks. The diphoton decay is loop induced, since tree-level decay is suppressed by large Majorana mass of the right-handed neutrino. There is also a potential dark matter candidate, which is the four-lepton condensation of muon, muon-neutrino, tau, and tau-neutrino. ",Kein DOI-Link verfügbar,1603.04697v4,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",A Minimal Physics-Based Model on the Electrochemical Impedance   Spectroscopy of Solid-State Electrolyte,1970,"  Solid state batteries have emerged as a potential next-generation energy storage device due to safety and energy density advantages. Development of electrolyte is one of the most important topics in solid state batteries. Electrochemical Impedance Spectroscopy (EIS) is a popular measurement technique to obtain the conductivity and diagnose the electrolyte. Current interpretation mainly uses the semicircle part of the curves and discards other information revealed by EIS such as the slope of the curve at low frequency. What is worse, some features on the curve are not fully interpreted. To better understand the transport mechanism and interpret EIS curves, we introduce a continuous model to quantify the ion transport and current flow in the electrolyte. The produced EIS curves from the model are compared with experiment data to show good agreement. ",Kein DOI-Link verfügbar,2110.00551v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Dependency-Guided LSTM-CRF for Named Entity Recognition,1970,"  Dependency tree structures capture long-distance and syntactic relationships between words in a sentence. The syntactic relations (e.g., nominal subject, object) can potentially infer the existence of certain named entities. In addition, the performance of a named entity recognizer could benefit from the long-distance dependencies between the words in dependency trees. In this work, we propose a simple yet effective dependency-guided LSTM-CRF model to encode the complete dependency trees and capture the above properties for the task of named entity recognition (NER). The data statistics show strong correlations between the entity types and dependency relations. We conduct extensive experiments on several standard datasets and demonstrate the effectiveness of the proposed model in improving NER and achieving state-of-the-art performance. Our analysis reveals that the significant improvements mainly result from the dependency relations and long-distance interactions provided by dependency trees. ",Kein DOI-Link verfügbar,1909.10148v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",A Facile Process to Fabricate Phosphorus/Carbon Xerogel Composite as   Anode for Sodium Ion Batteries,1970,"  Sodium ion batteries become popular due to their low cost. Among possible anode materials of sodium ion batteries, phosphorus has great potential owing to its high theoretical capacity. Previous research that yielded high capacity and long duration of phosphorus anode used expensive materials such as black phosphorus (BP) and phosphorene. To take advantage of the low cost of sodium ion batteries, we report a simple and low-cost method to fabricate anode: condensing red phosphorus on carbon xerogel. Even with large particle size (~ 50 $\mu$m) and high mass loading (2 mg cm$^{-2}$), the composite cycled at 100 mA g$^{-1}$ yielded a capacity of 357 mA g$^{-1}$ or 2498 mAh g$^{-1}_P$ based on phosphorus after subtracting the contribution of carbon. The average coulombic efficiency is as high as 99.4%. When cycled at 200 mA g$^{-1}$, it yielded a capacity of 242 mAh g$^{-1}$ or 1723 mAh g$^{-1}_P$, with average degradation rate only 0.06% in 80 cycles. Our research provided an innovative approach to synthesize anodes for sodium ion batteries at extremely low cost, with performance exceeding or comparable to state-of-the-art materials, which will promote their commercialization. ",https://doi.org/10.1149/1945-7111/ac18e0,2009.13056v2,Yes,"innovative(1), potent(1)"
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Mixed Cross Entropy Loss for Neural Machine Translation,1970,"  In neural machine translation, cross entropy (CE) is the standard loss function in two training methods of auto-regressive models, i.e., teacher forcing and scheduled sampling. In this paper, we propose mixed cross entropy loss (mixed CE) as a substitute for CE in both training approaches. In teacher forcing, the model trained with CE regards the translation problem as a one-to-one mapping process, while in mixed CE this process can be relaxed to one-to-many. In scheduled sampling, we show that mixed CE has the potential to encourage the training and testing behaviours to be similar to each other, more effectively mitigating the exposure bias problem. We demonstrate the superiority of mixed CE over CE on several machine translation datasets, WMT'16 Ro-En, WMT'16 Ru-En, and WMT'14 En-De in both teacher forcing and scheduled sampling setups. Furthermore, in WMT'14 En-De, we also find mixed CE consistently outperforms CE on a multi-reference set as well as a challenging paraphrased reference set. We also found the model trained with mixed CE is able to provide a better probability distribution defined over the translation output space. Our code is available at https://github.com/haorannlp/mix. ",Kein DOI-Link verfügbar,2106.15880v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Differentiable Data Augmentation for Contrastive Sentence Representation   Learning,1970,"  Fine-tuning a pre-trained language model via the contrastive learning framework with a large amount of unlabeled sentences or labeled sentence pairs is a common way to obtain high-quality sentence representations. Although the contrastive learning framework has shown its superiority on sentence representation learning over previous methods, the potential of such a framework is under-explored so far due to the simple method it used to construct positive pairs. Motivated by this, we propose a method that makes hard positives from the original training examples. A pivotal ingredient of our approach is the use of prefix that is attached to a pre-trained language model, which allows for differentiable data augmentation during contrastive learning. Our method can be summarized in two steps: supervised prefix-tuning followed by joint contrastive fine-tuning with unlabeled or labeled examples. Our experiments confirm the effectiveness of our data augmentation approach. The proposed method yields significant improvements over existing methods under both semi-supervised and supervised settings. Our experiments under a low labeled data setting also show that our method is more label-efficient than the state-of-the-art contrastive learning methods. ",Kein DOI-Link verfügbar,2210.16536v1,Yes,"pivotal(1), potent(1)"
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Galois Groups of Linear Difference-Differential Equations,1970,"  We study the relation between the Galois group $G$ of a linear difference-differential system and two classes $\mathcal{C}_1$ and $\mathcal{C}_2$ of groups that are the Galois groups of the specializations of the linear difference equation and the linear differential equation in this system respectively. We show that almost all groups in $\mathcal{C}_1\cup \mathcal{C}_2$ are algebraic subgroups of $G$, and there is a nonempty subset of $\mathcal{C}_1$ and a nonempty subset of $\mathcal{C}_2$ such that $G$ is the product of any pair of groups from these two subsets. These results have potential application to the computation of the Galois group of a linear difference-differential system. We also give a criterion for testing linear dependence of elements in a simple difference-differential ring, which generalizes Kolchin's criterion for partial differential fields. ",Kein DOI-Link verfügbar,2211.01977v2,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business","Reduced-order electrochemical models with shape functions for fast,   accurate prediction of lithium-ion batteries under high C rates",1970,"  This paper proposes physical-based, reduced-order electrochemical models that are much faster than the electrochemical pseudo 2D (P2D) model, while providing high accuracy even under the challenging conditions of high C-rate and strong polarization of lithium ion concentration and potential in a battery cell. In particular, an innovative weak form of equations are developed by using shape functions, which reduces the fully coupled electrochemical and transport equations to ordinary differential equations, and provides self-consistent solutions for the evolution of the polynomial coefficients. Results show that the models, named as revised single-particle model (RSPM) and fast-calculating P2D model (FCP2D), give highly reliable prediction of battery operations, including under dynamic driving profiles. They can calculate battery parameters, such as terminal voltage, over-potential, interfacial current density, lithium-ion concentration distribution, and electrolyte potential distribution with a relative error less than 2%. Applicable for moderately high C rates (below 2.5 C), the RSPM is up to more than 33 times faster than the P2D model. The FCP2D is applicable for high C rates (above 2.5 C) and is about 8 times faster than the P2D model. With their high speed and accuracy, these physics-based models can significantly improve the capability and performance of the battery management system and accelerate battery design optimization. ",Kein DOI-Link verfügbar,2305.18133v1,Yes,"innovative(1), potent(3)"
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Tuna: Instruction Tuning using Feedback from Large Language Models,1970,"  Instruction tuning of open-source large language models (LLMs) like LLaMA, using direct outputs from more powerful LLMs such as Instruct-GPT and GPT-4, has proven to be a cost-effective way to align model behaviors with human preferences. However, the instruction-tuned model has only seen one response per instruction, lacking the knowledge of potentially better responses. In this paper, we propose finetuning an instruction-tuned LLM using our novel \textit{probabilistic ranking} and \textit{contextual ranking} approaches to increase the likelihood of generating better responses. Probabilistic ranking enables the instruction-tuned model to inherit the relative rankings of high-quality and low-quality responses from the teacher LLM. On the other hand, learning with contextual ranking allows the model to refine its own response distribution using the contextual understanding ability of stronger LLMs. Furthermore, we apply probabilistic ranking and contextual ranking sequentially to the instruction-tuned LLM. The resulting model, which we call \textbf{Tuna}, consistently improves the performance on Super Natural Instructions (119 test tasks), LMentry (25 test tasks), Vicuna QA, and can even obtain better results than several strong reinforcement learning baselines. Our code and data are available at \url{ https://github.com/microsoft/LMOps}. ",Kein DOI-Link verfügbar,2310.13385v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Profit Maximization over Social Networks,1970,"  Influence maximization is the problem of finding a set of influential users in a social network such that the expected spread of influence under a certain propagation model is maximized. Much of the previous work has neglected the important distinction between social influence and actual product adoption. However, as recognized in the management science literature, an individual who gets influenced by social acquaintances may not necessarily adopt a product (or technology), due, e.g., to monetary concerns. In this work, we distinguish between influence and adoption by explicitly modeling the states of being influenced and of adopting a product. We extend the classical Linear Threshold (LT) model to incorporate prices and valuations, and factor them into users' decision-making process of adopting a product. We show that the expected profit function under our proposed model maintains submodularity under certain conditions, but no longer exhibits monotonicity, unlike the expected influence spread function. To maximize the expected profit under our extended LT model, we employ an unbudgeted greedy framework to propose three profit maximization algorithms. The results of our detailed experimental study on three real-world datasets demonstrate that of the three algorithms, \textsf{PAGE}, which assigns prices dynamically based on the profit potential of each candidate seed, has the best performance both in the expected profit achieved and in running time. ",https://doi.org/10.1109/ICDM.2012.145,1210.4211v2,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Complementary Resistive Switching in Tantalum Oxide-Based Resistive   Memory Devices,1970,  Complementary resistive switches (CRS) are considered as a potential solution for the sneak path problem in large-scale integration of passive crossbar resistive memory arrays. A typical CRS is composed of two bipolar memory cells that are connected anti-serially. Here we report a tantalum-oxide based resistive memory that achieves the complementary switching functionality within a single memory cell. The complementary switching effect is accompanied by switching polarity reversal in different voltage bias regimes. These effects were explained by the redistribution of oxygen vacancies inside the tantalum-oxide layers. The effects of symmetry breaking on bipolar switching and complementary switching were also discussed. ,https://doi.org/10.1063/1.4719198,1204.3515v2,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Exploring Direct Citations between Citing Publications,1970,"  This paper defines and explores the direct citations between citing publications (DCCPs) of a publication. We construct an ego-centered citation network for each paper that contains all of its citing papers and itself, as well as the citation relationships among them. By utilizing a large-scale scholarly dataset from the computer science field in the Microsoft Academic Graph (MAG-CS) dataset, we find that DCCPs exist universally in medium and highly cited papers. For those papers that have DCCPs, DCCPs do occur frequently; highly cited papers tend to contain more DCCPs than others. Meanwhile, the number of DCCPs of papers published in different years does not vary dramatically. The current paper also discusses the relationship between DCCPs and some indirect citation relationships (e.g., co-citation and bibliographic coupling). ",https://doi.org/10.1177/0165551520917654,1811.01120v4,Yes,scholarly(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Backdoor Attacks and Countermeasures in Natural Language Processing   Models: A Comprehensive Security Review,1970,"  Applicating third-party data and models has become a new paradigm for language modeling in NLP, which also introduces some potential security vulnerabilities because attackers can manipulate the training process and data source. In this case, backdoor attacks can induce the model to exhibit expected behaviors through specific triggers and have little inferior influence on primitive tasks. Hence, it could have dire consequences, especially considering that the backdoor attack surfaces are broad.   However, there is still no systematic and comprehensive review to reflect the security challenges, attacker's capabilities, and purposes according to the attack surface. Moreover, there is a shortage of analysis and comparison of the diverse emerging backdoor countermeasures in this context. In this paper, we conduct a timely review of backdoor attacks and countermeasures to sound the red alarm for the NLP security community. According to the affected stage of the machine learning pipeline, the attack surfaces are recognized to be wide and then formalized into three categorizations: attacking pre-trained model with fine-tuning (APMF) or parameter-efficient tuning (APMP), and attacking final model with training (AFMT). Thus, attacks under each categorization are combed. The countermeasures are categorized into two general classes: sample inspection and model inspection. Overall, the research on the defense side is far behind the attack side, and there is no single defense that can prevent all types of backdoor attacks. An attacker can intelligently bypass existing defenses with a more invisible attack. Drawing the insights from the systematic review, we also present crucial areas for future research on the backdoor, such as empirical security evaluations on large language models, and in particular, more efficient and practical countermeasures are solicited. ",Kein DOI-Link verfügbar,2309.06055v4,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Tracking biomedical articles along the translational continuum: a   measure based on biomedical knowledge representation,1970,"  Keeping track of translational research is essential to evaluating the performance of programs on translational medicine. Despite several indicators in previous studies, a consensus measure is still needed to represent the translational features of biomedical research at the article level. In this study, we first trained semantic representations of biomedical entities and documents (i.e., bio entity2vec and bio doc2vec) based on over 30 million PubMed articles. With these vectors, we then developed a new measure called Translational Progression (TP) for tracking biomedical articles along the translational continuum. We validated the effectiveness of TP from two perspectives (Clinical trial phase identification and ACH classification), which showed excellent consistency between TP and other indicators. Meanwhile, TP has several advantages. First, it can track the degree of translation of biomedical research dynamically and in real time. Second, it is straightforward to interpret and operationalize. Third, it doesn%u2019t require labor-intensive MeSH labeling and it is suitable for big scholarly data as well as papers that are not indexed in PubMed. In addition, we examined the translational progressions of biomedical research from three dimensions (including overall distribution, time, and research topic), which revealed three significant findings. The proposed measure in this study could be used by policymakers to monitor biomedical research with high translational potential in real time and make better decisions. It can also be adopted and improved for other domains, such as physics or computer science, to assess the application value of scientific discoveries. ",Kein DOI-Link verfügbar,2211.12819v1,Yes,"potent(1), scholarly(1)"
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Research Explosion: More Effort to Climb onto Shoulders of the Giant,1970,"  Fast-growing scientific publications present challenges to the scientific community. In this paper, we describe their implications to researchers. As references form explicit foundations for researchers to conduct a study, we investigate the evolution in reference patterns based on 60.8 million papers published from 1960 to 2015. The results demonstrate that recent papers contain more references than older ones, especially the well-cited papers compared with other papers. Well-cited papers receive 10 or more citations within 5 years of publication. Their references cover a longer period from classic research to very recent studies. Authors of well-cited papers are also farsighted to discover the reference papers with good potential to receive high citation numbers in near future. We also discover that the number of accumulative publications has a negative impact on next-5-year citation count for most fields except Chemistry, Materials science, Environmental science, Biology, and Engineering. Our findings suggest that researchers are expected to devote more effort to producing impactful research. Based on all these findings, we strongly advise against judging researchers simply based on the number of their publications. On the other hand, authors and reviewers should ensure that published papers contain adequate contributions. The code for our analysis is on GitHub: https://github.com/ECNU-Text-Computing/Research-Explosion. ",Kein DOI-Link verfügbar,2307.06506v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Unraveling Feature Extraction Mechanisms in Neural Networks,1970,"  The underlying mechanism of neural networks in capturing precise knowledge has been the subject of consistent research efforts. In this work, we propose a theoretical approach based on Neural Tangent Kernels (NTKs) to investigate such mechanisms. Specifically, considering the infinite network width, we hypothesize the learning dynamics of target models may intuitively unravel the features they acquire from training data, deepening our insights into their internal mechanisms. We apply our approach to several fundamental models and reveal how these models leverage statistical features during gradient descent and how they are integrated into final decisions. We also discovered that the choice of activation function can affect feature extraction. For instance, the use of the \textit{ReLU} activation function could potentially introduce a bias in features, providing a plausible explanation for its replacement with alternative functions in recent pre-trained language models. Additionally, we find that while self-attention and CNN models may exhibit limitations in learning n-grams, multiplication-based models seem to excel in this area. We verify these theoretical findings through experiments and find that they can be applied to analyze language modeling tasks, which can be regarded as a special variant of classification. Our contributions offer insights into the roles and capacities of fundamental components within large language models, thereby aiding the broader understanding of these complex systems. ",Kein DOI-Link verfügbar,2310.16350v2,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",Generative Models are Self-Watermarked: Declaring Model Authentication   through Re-Generation,1970,"  As machine- and AI-generated content proliferates, protecting the intellectual property of generative models has become imperative, yet verifying data ownership poses formidable challenges, particularly in cases of unauthorized reuse of generated data. The challenge of verifying data ownership is further amplified by using Machine Learning as a Service (MLaaS), which often functions as a black-box system.   Our work is dedicated to detecting data reuse from even an individual sample. Traditionally, watermarking has been leveraged to detect AI-generated content. However, unlike watermarking techniques that embed additional information as triggers into models or generated content, potentially compromising output quality, our approach identifies latent fingerprints inherently present within the outputs through re-generation. We propose an explainable verification procedure that attributes data ownership through re-generation, and further amplifies these fingerprints in the generative models through iterative data re-generation. This methodology is theoretically grounded and demonstrates viability and robustness using recent advanced text and image generative models. Our methodology is significant as it goes beyond protecting the intellectual property of APIs and addresses important issues such as the spread of misinformation and academic misconduct. It provides a useful tool to ensure the integrity of sources and authorship, expanding its application in different scenarios where authenticity and ownership verification are essential. ",Kein DOI-Link verfügbar,2402.16889v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",3D Vertical Dual-Layer Oxide Memristive Devices for Neuromorphic   Computing,1970,"  Dual-layer resistive switching devices with horizontal W electrodes, vertical Pd electrodes and WOx switching layer formed at the sidewall of the horizontal electrodes have been fabricated and characterized. The devices exhibit well-characterized analog switching characteristics and small mismatch in electrical characteristics for devices formed at the two layers. The three-dimensional (3D) vertical device structure allows higher storage density and larger connectivity for neuromorphic computing applications. We show the vertical devices exhibit potentiation and depression characteristics similar to planar devices, and can be programmed independently with no crosstalk between the layers. ",Kein DOI-Link verfügbar,1404.1158v1,Yes,potent(1)
0000-0002-0783-5436,Wei Lu,"Aalto University, Aalto University School of Business",A Full Adagrad algorithm with O(Nd) operations,1970,"  A novel approach is given to overcome the computational challenges of the full-matrix Adaptive Gradient algorithm (Full AdaGrad) in stochastic optimization. By developing a recursive method that estimates the inverse of the square root of the covariance of the gradient, alongside a streaming variant for parameter updates, the study offers efficient and practical algorithms for large-scale applications. This innovative strategy significantly reduces the complexity and resource demands typically associated with full-matrix methods, enabling more effective optimization processes. Moreover, the convergence rates of the proposed estimators and their asymptotic efficiency are given. Their effectiveness is demonstrated through numerical studies. ",Kein DOI-Link verfügbar,2405.01908v1,Yes,innovative(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Prototype tests for a highly granular scintillator-based hadronic   calorimeter,1970,"  Within the CALICE collaboration, several concepts for the hadronic calorimeter of a future lepton collider detector are studied. After having demonstrated the capabilities of the measurement methods in ""physics prototypes"", the focus now lies on improving their implementation in ""technological prototypes"", that are scalable to the full linear collider detector. The Analogue Hadronic Calorimeter (AHCAL) concept is a sampling calorimeter of tungsten or steel absorber plates and plastic scintillator tiles read out by silicon photomultipliers (SiPMs) as active components. The front-end electronics is fully integrated into the active layers of the calorimeter and is designed for minimal power consumption (i.e. power pulsing). The versatile electronics enables the prototype to be equipped with different types of scintillator tiles and SiPMs. In recent beam tests, a prototype with $\sim$3700 channels, equipped with several types of scintillator tiles and SiPMs, was exposed to electron, muon and hadron beams. The experience of these beam tests resulted in an optimal detector design with surface-mounted SiPMs suitable for the automated mass assembly. The proceeding will cover topics including the testbeam measurements with the AHCAL technological prototype, the improved detector design and the ongoing development of a large prototype for hadronic showers. ",Kein DOI-Link verfügbar,1710.03622v1,Yes,versatile(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",In-context Learning with Transformer Is Really Equivalent to a   Contrastive Learning Pattern,1970,"  Pre-trained large language models based on Transformers have demonstrated amazing in-context learning (ICL) abilities. Given several demonstration examples, the models can implement new tasks without any parameter updates. However, it is still an open question to understand the mechanism of ICL. In this paper, we interpret the inference process of ICL as a gradient descent process in a contrastive learning pattern. Firstly, leveraging kernel methods, we establish the relationship between gradient descent and self-attention mechanism under generally used softmax attention setting instead of linear attention setting. Then, we analyze the corresponding gradient descent process of ICL from the perspective of contrastive learning without negative samples and discuss possible improvements of this contrastive learning pattern, based on which the self-attention layer can be further modified. Finally, we design experiments to support our opinions. To the best of our knowledge, our work is the first to provide the understanding of ICL from the perspective of contrastive learning and has the potential to facilitate future model design by referring to related works on contrastive learning. ",Kein DOI-Link verfügbar,2310.13220v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Improved Learning Rates for Stochastic Optimization: Two Theoretical   Viewpoints,1970,"  Generalization performance of stochastic optimization stands a central place in learning theory. In this paper, we investigate the excess risk performance and towards improved learning rates for two popular approaches of stochastic optimization: empirical risk minimization (ERM) and stochastic gradient descent (SGD). Although there exists plentiful generalization analysis of ERM and SGD for supervised learning, current theoretical understandings of ERM and SGD either have stronger assumptions in convex learning, e.g., strong convexity, or show slow rates and less studied in nonconvex learning. Motivated by these problems, we aim to provide improved rates under milder assumptions in convex learning and derive faster rates in nonconvex learning. It is notable that our analysis span two popular theoretical viewpoints: \emph{stability} and \emph{uniform convergence}. Specifically, in stability regime, we present high probability learning rates of order $\mathcal{O} (1/n)$ w.r.t. the sample size $n$ for ERM and SGD with milder assumptions in convex learning and similar high probability rates of order $\mathcal{O} (1/n)$ in nonconvex learning, rather than in expectation. Furthermore, this type of learning rate is improved to faster order $\mathcal{O} (1/n^2)$ in uniform convergence regime. To our best knowledge, for ERM and SGD, the learning rates presented in this paper are all state-of-the-art. ",Kein DOI-Link verfügbar,2107.08686v2,Yes,notable(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Neural Architecture Optimization with Graph VAE,1970,"  Due to their high computational efficiency on a continuous space, gradient optimization methods have shown great potential in the neural architecture search (NAS) domain. The mapping of network representation from the discrete space to a latent space is the key to discovering novel architectures, however, existing gradient-based methods fail to fully characterize the networks. In this paper, we propose an efficient NAS approach to optimize network architectures in a continuous space, where the latent space is built upon variational autoencoder (VAE) and graph neural networks (GNN). The framework jointly learns four components: the encoder, the performance predictor, the complexity predictor and the decoder in an end-to-end manner. The encoder and the decoder belong to a graph VAE, mapping architectures between continuous representations and network architectures. The predictors are two regression models, fitting the performance and computational complexity, respectively. Those predictors ensure the discovered architectures characterize both excellent performance and high computational efficiency. Extensive experiments demonstrate our framework not only generates appropriate continuous representations but also discovers powerful neural architectures. ",Kein DOI-Link verfügbar,2006.10310v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Koopa: Learning Non-stationary Time Series Dynamics with Koopman   Predictors,1970,"  Real-world time series are characterized by intrinsic non-stationarity that poses a principal challenge for deep forecasting models. While previous models suffer from complicated series variations induced by changing temporal distribution, we tackle non-stationary time series with modern Koopman theory that fundamentally considers the underlying time-variant dynamics. Inspired by Koopman theory of portraying complex dynamical systems, we disentangle time-variant and time-invariant components from intricate non-stationary series by Fourier Filter and design Koopman Predictor to advance respective dynamics forward. Technically, we propose Koopa as a novel Koopman forecaster composed of stackable blocks that learn hierarchical dynamics. Koopa seeks measurement functions for Koopman embedding and utilizes Koopman operators as linear portraits of implicit transition. To cope with time-variant dynamics that exhibits strong locality, Koopa calculates context-aware operators in the temporal neighborhood and is able to utilize incoming ground truth to scale up forecast horizon. Besides, by integrating Koopman Predictors into deep residual structure, we ravel out the binding reconstruction loss in previous Koopman forecasters and achieve end-to-end forecasting objective optimization. Compared with the state-of-the-art model, Koopa achieves competitive performance while saving 77.3% training time and 76.0% memory. ",Kein DOI-Link verfügbar,2305.18803v2,Yes,intricate(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Large deviations principle for stationary solutions of stochastic   differential equations with multiplicative noise,1970,"  We study the large deviations principle (LDP) for stationary solutions of a class of stochastic differential equations (SDE) in infinite time intervals by the weak convergence approach, and then establish the LDP for the invariant measures of the SDE by the contraction principle. We further point out the equivalence of the rate function of the LDP for invariant measures induced by the LDP for stationary solutions and the rate function defined by quasi-potential. This fact gives another view of the quasi-potential introduced by Freidlin and Wentzell. ",Kein DOI-Link verfügbar,2206.02356v1,Yes,potent(2)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",A Quantitative Metric for Privacy Leakage in Federated Learning,1970,"  In the federated learning system, parameter gradients are shared among participants and the central modulator, while the original data never leave their protected source domain. However, the gradient itself might carry enough information for precise inference of the original data. By reporting their parameter gradients to the central server, client datasets are exposed to inference attacks from adversaries. In this paper, we propose a quantitative metric based on mutual information for clients to evaluate the potential risk of information leakage in their gradients. Mutual information has received increasing attention in the machine learning and data mining community over the past few years. However, existing mutual information estimation methods cannot handle high-dimensional variables. In this paper, we propose a novel method to approximate the mutual information between the high-dimensional gradients and batched input data. Experimental results show that the proposed metric reliably reflect the extent of information leakage in federated learning. In addition, using the proposed metric, we investigate the influential factors of risk level. It is proven that, the risk of information leakage is related to the status of the task model, as well as the inherent data distribution. ",Kein DOI-Link verfügbar,2102.13472v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Predicting Fatigue Crack Growth via Path Slicing and Re-Weighting,1970,"  Predicting potential risks associated with the fatigue of key structural components is crucial in engineering design. However, fatigue often involves entangled complexities of material microstructures and service conditions, making diagnosis and prognosis of fatigue damage challenging. We report a statistical learning framework to predict the growth of fatigue cracks and the life-to-failure of the components under loading conditions with uncertainties. Digital libraries of fatigue crack patterns and the remaining life are constructed by high-fidelity physical simulations. Dimensionality reduction and neural network architectures are then used to learn the history dependence and nonlinearity of fatigue crack growth. Path-slicing and re-weighting techniques are introduced to handle the statistical noises and rare events. The predicted fatigue crack patterns are self-updated and self-corrected by the evolving crack patterns. The end-to-end approach is validated by representative examples with fatigue cracks in plates, which showcase the digital-twin scenario in real-time structural health monitoring and fatigue life prediction for maintenance management decision-making. ",https://doi.org/10.1016/j.taml.2023.100477,2309.06708v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",YOLIC: An Efficient Method for Object Localization and Classification on   Edge Devices,1970,"  In the realm of Tiny AI, we introduce ``You Only Look at Interested Cells"" (YOLIC), an efficient method for object localization and classification on edge devices. Through seamlessly blending the strengths of semantic segmentation and object detection, YOLIC offers superior computational efficiency and precision. By adopting Cells of Interest for classification instead of individual pixels, YOLIC encapsulates relevant information, reduces computational load, and enables rough object shape inference. Importantly, the need for bounding box regression is obviated, as YOLIC capitalizes on the predetermined cell configuration that provides information about potential object location, size, and shape. To tackle the issue of single-label classification limitations, a multi-label classification approach is applied to each cell for effectively recognizing overlapping or closely situated objects. This paper presents extensive experiments on multiple datasets to demonstrate that YOLIC achieves detection performance comparable to the state-of-the-art YOLO algorithms while surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU. All resources related to this study, including datasets, cell designer, image annotation tool, and source code, have been made publicly available on our project website at https://kai3316.github.io/yolic.github.io ",https://doi.org/10.1016/j.imavis.2024.105095,2307.06689v3,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Refining Latent Representations: A Generative SSL Approach for   Heterogeneous Graph Learning,1970,"  Self-Supervised Learning (SSL) has shown significant potential and has garnered increasing interest in graph learning. However, particularly for generative SSL methods, its potential in Heterogeneous Graph Learning (HGL) remains relatively underexplored. Generative SSL utilizes an encoder to map the input graph into a latent representation and a decoder to recover the input graph from the latent representation. Previous HGL SSL methods generally design complex strategies to capture graph heterogeneity, which heavily rely on contrastive view construction strategies that are often non-trivial. Yet, refining the latent representation in generative SSL can effectively improve graph learning results. In this study, we propose HGVAE, a generative SSL method specially designed for HGL. Instead of focusing on designing complex strategies to capture heterogeneity, HGVAE centers on refining the latent representation. Specifically, HGVAE innovatively develops a contrastive task based on the latent representation. To ensure the hardness of negative samples, we develop a progressive negative sample generation (PNSG) mechanism that leverages the ability of Variational Inference (VI) to generate high-quality negative samples. As a pioneer in applying generative SSL for HGL, HGVAE refines the latent representation, thereby compelling the model to learn high-quality representations. Compared with various state-of-the-art (SOTA) baselines, HGVAE achieves impressive results, thus validating its superiority. ",Kein DOI-Link verfügbar,2310.11102v4,Yes,"innovative(1), potent(2), innovatively(1)"
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",VIGraph: Generative Self-supervised Learning for Class-Imbalanced Node   Classification,1970,"  Class imbalance in graph data presents significant challenges for node classification. While existing methods, such as SMOTE-based approaches, partially mitigate this issue, they still exhibit limitations in constructing imbalanced graphs. Generative self-supervised learning (SSL) methods, exemplified by graph autoencoders (GAEs), offer a promising solution by directly generating minority nodes from the data itself, yet their potential remains underexplored. In this paper, we delve into the shortcomings of SMOTE-based approaches in the construction of imbalanced graphs. Furthermore, we introduce VIGraph, a simple yet effective generative SSL approach that relies on the Variational GAE as the fundamental model. VIGraph strictly adheres to the concept of imbalance when constructing imbalanced graphs and innovatively leverages the variational inference (VI) ability of Variational GAE to generate nodes for minority classes. VIGraph introduces comprehensive training strategies, including cross-view contrastive learning at the decoding phase to capture semantic knowledge, adjacency matrix reconstruction to preserve graph structure, and alignment strategy to ensure stable training. VIGraph can generate high-quality nodes directly usable for classification, eliminating the need to integrate the generated nodes back to the graph as well as additional retraining found in SMOTE-based methods. We conduct extensive experiments, results from which demonstrate the superiority and generality of our approach. ",Kein DOI-Link verfügbar,2311.01191v2,Yes,"innovative(1), potent(1), innovatively(1)"
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",A well-balanced oscillation-free discontinuous Galerkin method for   shallow water equations,1970,"  In this paper, we develop a well-balanced oscillation-free discontinuous Galerkin (OFDG) method for solving the shallow water equations with a non-flat bottom topography. One notable feature of the constructed scheme is the well-balanced property, which preserves exactly the hydrostatic equilibrium solutions up to machine error. Another feature is the non-oscillatory property, which is very important in the numerical simulation when there exist some shock discontinuities. To control the spurious oscillations, we construct an OFDG method with an extra damping term to the existing well-balanced DG schemes proposed in [Y. Xing and C.-W. Shu, CICP, 1(2006), 100-134.]. With a careful construction of the damping term, the proposed method achieves both the well-balanced property and non-oscillatory property simultaneously without compromising any order of accuracy. We also present a detailed procedure for the construction and a theoretical analysis for the preservation of the well-balancedness property. Extensive numerical experiments including one- and two-dimensional space demonstrate that the proposed methods possess the desired properties without sacrificing any order of accuracy. ",Kein DOI-Link verfügbar,2109.02193v1,Yes,notable(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Guide Local Feature Matching by Overlap Estimation,1970,"  Local image feature matching under large appearance, viewpoint, and distance changes is challenging yet important. Conventional methods detect and match tentative local features across the whole images, with heuristic consistency checks to guarantee reliable matches. In this paper, we introduce a novel Overlap Estimation method conditioned on image pairs with TRansformer, named OETR, to constrain local feature matching in the commonly visible region. OETR performs overlap estimation in a two-step process of feature correlation and then overlap regression. As a preprocessing module, OETR can be plugged into any existing local feature detection and matching pipeline, to mitigate potential view angle or scale variance. Intensive experiments show that OETR can boost state-of-the-art local feature matching performance substantially, especially for image pairs with small shared regions. The code will be publicly available at https://github.com/AbyssGaze/OETR. ",Kein DOI-Link verfügbar,2202.09050v2,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Prediction of interesting ferromagnetism in Janus semiconducting   Cr$_2$AsP monolayer,1970,"  Two-dimensional (2D) half-metallic materials that have sparked intense interest in advanced spintronic applications are essential to the developing next-generation nanospintronic devices. Here we have adopted a first-principles calculation method to predict the magnetic properties of intrinsic, Se-doped, and biaxial strain tuning Cr$_2$AsP monolayer. The Janus Cr$_2$AsP monolayer is proved to be an intrinsic ferromagnetic (FM) semiconductor with a exchange splitting bandgap of 0.15 eV at the PBE+U level. Concentration-dependent Se doping such as Cr$_2$As$_{1-x}$Se$_x$P (x = 0.25, 0.50, 0.75) can regulate Cr$_2$AsP from FM semiconductor to FM half-metallicity. Specifically, the spin-up channel crosses the Fermi level, while the spin-down channel has a bandgap. More interestingly, the wide half-metallic bandgaps and spin bandgaps make them have important implications for the preparation of spintronic devices. At last, we also explore the effect of biaxial strain from -14% to 10% on the magnetism of the Cr$_2$AsP monolayer. There appears a transition from FM to antiferromagnetic (AFM) at a compressive strain of -10.7%, originating from the competition between the indirect FM superexchange interaction and the direct AFM interaction between the nearest-neighbor Cr atoms. Additionally, when the compressive strain to -2% or the tensile strain to 6%, the semiconducting Cr$_2$AsP becomes a half-metallic material. These charming properties render the Janus Cr$_2$AsP monolayer with great potential for applications in spintronic devices. ",Kein DOI-Link verfügbar,2210.13727v2,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Large Out-of-Plane Piezoelectric Effect in Janus Ferromagnetic   Semiconductor Monolayer of CrOFBr,1970,"  The exploitation of piezoelectric ferromagnetism (PFM) in two-dimensional (2D) materials with large out-of-plane piezoelectric response is motivated not only by technological applications but also scientific interest. In this study, the CrONM monolayer family (N=F, Cl; M=Br, Cl) was investigated using first-principles calculations, revealing that the Janus CrOFBr monolayer exhibits intrinsic ferromagnetic semiconductor behavior along with a significant out-of-plane piezoelectric effect. The calculated out-of-plane piezoelectric strain coefficients d$_{31}$ and d$_{32}$ are up to 1.21 and 0.63 pm/V, respectively. These values are greater than those of the majority of 2D materials. Furthermore, our findings demonstrate that applying tensile strain can enhance the out-of-plane piezoelectric response, leading to a respective 27% and 67% augmentation in the piezoelectric strain coefficients d$_{31}$ and d$_{32}$ compared to the unstrained configurations. This discovery holds great potential for propelling the field of nanoelectronics forward and facilitating the development of multifunctional semiconductor spintronic applications. Finally, by comparing d$_{31}$ and d$_{32}$ of the CrONM monolayer family (N=F, Cl; M=Br, Cl), we find that the magnitudes of d$_{31}$ and d$_{32}$ are correlated with the electronegativity difference between the M and N atoms. These findings provide valuable insights for the design of 2D piezoelectric materials with enhanced vertical piezoelectric responses. ",Kein DOI-Link verfügbar,2406.06265v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Two Dimensional Ferromagnetic Semiconductor: Monolayer CrGeS$_3$,1970,"  Recently, two-dimensional ferromagnetic semiconductors have been an important class of materials for many potential applications in spintronic devices. Based on density functional theory, we systematically explore the magnetic and electronic properties of CrGeS$_3$ with the monolayer structures. The comparison of total energy between different magnetic states ensures the ferromagnetic ground state of monolayer CrGeS$_3$. It is also shown that ferromagnetic and semiconducting properties are exhibited in monolayer CrGeS$_3$ with the magnetic moment of 3 $\mu_{B}$ for each Cr atom, donated mainly by the intense $dp$$\sigma$-hybridization of Cr $e_g$-S $p$. There are the bandgap of 0.70 eV of spin-up state in the monolayer structure when 0.77 eV in spin-down state. The global gap is 0.34 eV (2.21 eV by using HSE06 functional), which originates from bonding $dp\sigma$ hybridized states of Cr $e_g$-S $p$ and unoccupied Cr $t_{2g}$-Ge $p$ hybridization. Besides, we estimate that the monolayer CrGeS$_3$ possesses the Curie temperature of 161 K by mean-field theory. ",https://doi.org/10.1088/1361-648X/ab4395,1906.07396v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Can Large Language Models Empower Molecular Property Prediction?,1970,"  Molecular property prediction has gained significant attention due to its transformative potential in multiple scientific disciplines. Conventionally, a molecule graph can be represented either as a graph-structured data or a SMILES text. Recently, the rapid development of Large Language Models (LLMs) has revolutionized the field of NLP. Although it is natural to utilize LLMs to assist in understanding molecules represented by SMILES, the exploration of how LLMs will impact molecular property prediction is still in its early stage. In this work, we advance towards this objective through two perspectives: zero/few-shot molecular classification, and using the new explanations generated by LLMs as representations of molecules. To be specific, we first prompt LLMs to do in-context molecular classification and evaluate their performance. After that, we employ LLMs to generate semantically enriched explanations for the original SMILES and then leverage that to fine-tune a small-scale LM model for multiple downstream tasks. The experimental results highlight the superiority of text explanations as molecular representations across multiple benchmark datasets, and confirm the immense potential of LLMs in molecular property prediction tasks. Codes are available at \url{https://github.com/ChnQ/LLM4Mol}. ",Kein DOI-Link verfügbar,2307.07443v1,Yes,potent(2)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Preserving Node Distinctness in Graph Autoencoders via Similarity   Distillation,1970,"  Graph autoencoders (GAEs), as a kind of generative self-supervised learning approach, have shown great potential in recent years. GAEs typically rely on distance-based criteria, such as mean-square-error (MSE), to reconstruct the input graph. However, relying solely on a single reconstruction criterion may lead to a loss of distinctiveness in the reconstructed graph, causing nodes to collapse into similar representations and resulting in sub-optimal performance. To address this issue, we have developed a simple yet effective strategy to preserve the necessary distinctness in the reconstructed graph. Inspired by the knowledge distillation technique, we found that the dual encoder-decoder architecture of GAEs can be viewed as a teacher-student relationship. Therefore, we propose transferring the knowledge of distinctness from the raw graph to the reconstructed graph, achieved through a simple KL constraint. Specifically, we compute pairwise node similarity scores in the raw graph and reconstructed graph. During the training process, the KL constraint is optimized alongside the reconstruction criterion. We conducted extensive experiments across three types of graph tasks, demonstrating the effectiveness and generality of our strategy. This indicates that the proposed approach can be employed as a plug-and-play method to avoid vague reconstructions and enhance overall performance. ",Kein DOI-Link verfügbar,2406.17517v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",High Curie Temperature Ferromagnetic Semiconductor: Bimetal Transition   Iodide V$_2$Cr$_2$I$_9$,1970,"  Bimetal transition iodides in two-dimensional scale provide an interesting idea to combine a set of single-transition-metal ferromagnetic semiconductors together. Motivated by structural engineering on bilayer CrI$_3$ to tune its magnetism and works that realize ideal properties by stacking van der Waals transitional metal dichalcogenides in a certain order. Here we stack monolayer VI$_3$ onto monolayer CrI$_3$ with a middle-layer I atoms discarded to construct monolayer V$_2$Cr$_2$I$_9$. Based on this crystal model, the stable and metastable phases are determined among 7 possible phases by first-principles calculations. It is illustrated that both the two phases have Curie temperature $\sim$ 6 (4) times higher than monolayer CrI$_3$ and VI$_3$. The reason can be partly attributed to their large magnetic anisotropy energy (the maximum value reaches 412.9 $\mu$eV/atom). More importantly, the Curie temperature shows an electric field and strain dependent character and can even surpass room temperature under a moderate strain range. At last, we believe that the bimetal transition iodide V$_2$Cr$_2$I$_9$ monolayer would support potential opportunities for spintronic devices. ",Kein DOI-Link verfügbar,2012.04270v2,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",ConDefects: A New Dataset to Address the Data Leakage Concern for   LLM-based Fault Localization and Program Repair,1970,"  With the growing interest on Large Language Models (LLMs) for fault localization and program repair, ensuring the integrity and generalizability of the LLM-based methods becomes paramount. The code in existing widely-adopted benchmarks for these tasks was written before the the bloom of LLMs and may be included in the training data of existing popular LLMs, thereby suffering from the threat of data leakage, leading to misleadingly optimistic performance metrics. To address this issue, we introduce ""ConDefects"", a novel dataset of real faults meticulously curated to eliminate such overlap. ConDefects contains 1,254 Java faulty programs and 1,625 Python faulty programs. All these programs are sourced from the online competition platform AtCoder and were produced between October 2021 and September 2023. We pair each fault with fault locations and the corresponding repaired code versions, making it tailored for in fault localization and program repair related research. We also provide interfaces for selecting subsets based on different time windows and coding task difficulties. While inspired by LLM-based tasks, ConDefects can be adopted for benchmarking ALL types of fault localization and program repair methods. The dataset is publicly available, and a demo video can be found at https://www.youtube.com/watch?v=22j15Hj5ONk. ",Kein DOI-Link verfügbar,2310.16253v1,Yes,"meticulous(1), meticulously(1)"
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Interactive $360^{\circ}$ Video Streaming Using FoV-Adaptive Coding with   Temporal Prediction,1970,"  For $360^{\circ}$ video streaming, FoV-adaptive coding that allocates more bits for the predicted user's field of view (FoV) is an effective way to maximize the rendered video quality under the limited bandwidth. We develop a low-latency FoV-adaptive coding and streaming system for interactive applications that is robust to bandwidth variations and FoV prediction errors. To minimize the end-to-end delay and yet maximize the coding efficiency, we propose a frame-level FoV-adaptive inter-coding structure. In each frame, regions that are in or near the predicted FoV are coded using temporal and spatial prediction, while a small rotating region is coded with spatial prediction only. This rotating intra region periodically refreshes the entire frame, thereby providing robustness to both FoV prediction errors and frame losses due to transmission errors. The system adapts the sizes and rates of different regions for each video segment to maximize the rendered video quality under the predicted bandwidth constraint. Integrating such frame-level FoV adaptation with temporal prediction is challenging due to the temporal variations of the FoV. We propose novel ways for modeling the influence of FoV dynamics on the quality-rate performance of temporal predictive coding.We further develop LSTM-based machine learning models to predict the user's FoV and network bandwidth.The proposed system is compared with three benchmark systems, using real-world network bandwidth traces and FoV traces, and is shown to significantly improve the rendered video quality, while achieving very low end-to-end delay and low frame-freeze probability. ",Kein DOI-Link verfügbar,2403.11155v1,Yes,fresh(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Moving Forward in Formation: A Decentralized Hierarchical Learning   Approach to Multi-Agent Moving Together,1970,"  Multi-agent path finding in formation has many potential real-world applications like mobile warehouse robots. However, previous multi-agent path finding (MAPF) methods hardly take formation into consideration. Furthermore, they are usually centralized planners and require the whole state of the environment. Other decentralized partially observable approaches to MAPF are reinforcement learning (RL) methods. However, these RL methods encounter difficulties when learning path finding and formation problem at the same time. In this paper, we propose a novel decentralized partially observable RL algorithm that uses a hierarchical structure to decompose the multi objective task into unrelated ones. It also calculates a theoretical weight that makes every task reward has equal influence on the final RL value function. Additionally, we introduce a communication method that helps agents cooperate with each other. Experiments in simulation show that our method outperforms other end-to-end RL methods and our method can naturally scale to large world sizes where centralized planner struggles. We also deploy and validate our method in a real world scenario. ",Kein DOI-Link verfügbar,2011.02373v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business","Electronic, magnetic, and optical properties of Mn-doped GaSb: a   first-principles study",1970,"  Half-metallic ferromagnets can produce fully spin-polarized conduction electrons and can be applied to fabricate spintronic devices. Thus, in this study, the electronic structure, magnetic properties, and optical properties of GaSb, which has exhibited half-metallicity, doped with Mn, a 3d transition metal, are calculated using the generalized gradient approximation and Heyd-Scuseria-Ernzerhof (HSE) functional. Ga$_{1-x}$Mn$_x$Sb ($x = 0.25, 0.5, 0.75$) materials exhibit ferromagnetic half-metallic properties and a high Curie temperature, indicating that this series can applied in spintronic devices. Meanwhile, they absorb strongly in the infrared band, suggesting that Ga$_{1-x}$Mn$_{x}$Sb also has potential applications in infrared photoelectric devices. ",https://doi.org/10.1016/j.physb.2019.08.001,1903.10818v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",High Thermoelectric Performance of Au@Sb2Te3 Heterostructure Derived   from the Potential Barriers,1970,"  The correlated couple of electrical and thermal property is the challenge to realize a substantial leap in thermoelectric materials.Synthesis of semiconductor and metal composites is a significant and versatile design strategy to optimize the thermoelectric performance driven by tailored interface between nanoinclusions and matrix.In this study, we present the simultaneous increase of electrical conductivity and Seebeck coefficient, and reduction of thermal conductivity in Sb2Te3-Au system.The enhanced electrical conductivity lies in the incorporated Au nanostructures contributing to injecting carriers to Sb2Te3 matrix.The appropriate barriers originated from the Au-Sb2Te3 interface, which filter low energy carriers, results in enhancement of Seebeck coefficient.The increased boundaries and nanodomains block the transport of phonons, subsequently reducing the thermal conductivity.As a consequence, combination of these effects promote double of ZT value in 1% Au@ Sb2Te3 composites with respect to the pristine Sb2Te3. ",Kein DOI-Link verfügbar,1805.08519v1,Yes,versatile(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Recent Advances in Diversified Recommendation,1970,"  With the rapid development of recommender systems, accuracy is no longer the only golden criterion for evaluating whether the recommendation results are satisfying or not. In recent years, diversity has gained tremendous attention in recommender systems research, which has been recognized to be an important factor for improving user satisfaction. On the one hand, diversified recommendation helps increase the chance of answering ephemeral user needs. On the other hand, diversifying recommendation results can help the business improve product visibility and explore potential user interests. In this paper, we are going to review the recent advances in diversified recommendation. Specifically, we first review the various definitions of diversity and generate a taxonomy to shed light on how diversity have been modeled or measured in recommender systems. After that, we summarize the major optimization approaches to diversified recommendation from a taxonomic view. Last but not the least, we project into the future and point out trending research directions on this topic. ",Kein DOI-Link verfügbar,1905.06589v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business","Large magnetic anisotropy energy and robust half-metallic ferromagnetism   in 2D MnXSe$_4$ (X = As, Sb)",1970,"  In recent years, intrinsic two-dimensional (2D) magnetism aroused great interest because of its potential application in spintronic devices. However, low Curie temperature (\emph{T}$_c$) and magnetic anisotropy energy (MAE) limit its application prospects. Here, using first-principles calculations based on density-functional theory (DFT), we predicted a series of stable MnXSe$_4$ (X=As, Sb) single-layer. The MAE of single-layer MnAsSe$_4$ and MnSbSe$_4$ was 648.76 and 808.95 ${\mu}$eV per Mn atom, respectively. Monte Carlo (MC) simulations suggested the \emph{T}$_c$ of single-layer MnAsSe$_4$ and MnSbSe$_4$ was 174 and 250 K, respectively. The energy band calculation with hybrid Heyd-Scuseria-Ernzerhof (HSE06) function indicated the MnXSe$_4$ (X = As, Sb) were ferromagnetic (FM) half-metallic. Also it had 100\% spin-polarization ratio at the Fermi level. For MnAsSe$_4$ and MnSbSe$_4$, the spin-gap were 1.59 and 1.48 eV, respectively. These excellent magnetic properties render MnXSe$_4$ (X = As, Sb) promising candidate materials for 2D spintronic applications. ",https://doi.org/10.1002/andp.202000365,2007.05951v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Universal Segmentation at Arbitrary Granularity with Language   Instruction,1970,"  This paper aims to achieve universal segmentation of arbitrary semantic level. Despite significant progress in recent years, specialist segmentation approaches are limited to specific tasks and data distribution. Retraining a new model for adaptation to new scenarios or settings takes expensive computation and time cost, which raises the demand for versatile and universal segmentation model that can cater to various granularity. Although some attempts have been made for unifying different segmentation tasks or generalization to various scenarios, limitations in the definition of paradigms and input-output spaces make it difficult for them to achieve accurate understanding of content at arbitrary granularity. To this end, we present UniLSeg, a universal segmentation model that can perform segmentation at any semantic level with the guidance of language instructions. For training UniLSeg, we reorganize a group of tasks from original diverse distributions into a unified data format, where images with texts describing segmentation targets as input and corresponding masks are output. Combined with a automatic annotation engine for utilizing numerous unlabeled data, UniLSeg achieves excellent performance on various tasks and settings, surpassing both specialist and unified segmentation models. ",Kein DOI-Link verfügbar,2312.01623v3,Yes,versatile(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",LVIS Challenge Track Technical Report 1st Place Solution: Distribution   Balanced and Boundary Refinement for Large Vocabulary Instance Segmentation,1970,"  This report introduces the technical details of the team FuXi-Fresher for LVIS Challenge 2021. Our method focuses on the problem in following two aspects: the long-tail distribution and the segmentation quality of mask and boundary. Based on the advanced HTC instance segmentation algorithm, we connect transformer backbone(Swin-L) through composite connections inspired by CBNetv2 to enhance the baseline results. To alleviate the problem of long-tail distribution, we design a Distribution Balanced method which includes dataset balanced and loss function balaced modules. Further, we use a Mask and Boundary Refinement method composed with mask scoring and refine-mask algorithms to improve the segmentation quality. In addition, we are pleasantly surprised to find that early stopping combined with EMA method can achieve a great improvement. Finally, by using multi-scale testing and increasing the upper limit of the number of objects detected per image, we achieved more than 45.4% boundary AP on the val set of LVIS Challenge 2021. On the test data of LVIS Challenge 2021, we rank 1st and achieve 48.1% AP. Notably, our APr 47.5% is very closed to the APf 48.0%. ",Kein DOI-Link verfügbar,2111.02668v2,Yes,fresh(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business","First-principles calculations on the mechanical, electronic, magnetic   and optical properties of two-dimensional Janus Cr$_2$TeX (X= P, As, Sb)   monolayers",1970,"  Janus materials possess extraordinary physical, chemical, and mechanical properties caused by symmetry breaking. Here, the mechanic properties, electronic structure, magnetic properties, and optical properties of Janus Cr$_2$TeX (X= P, As, Sb) monolayers are systematically investigated by the density functional theory. Janus Cr$_2$TeP, Cr$_2$TeAs, and Cr$_2$TeSb are intrinsic ferromagnetic (FM) half-metals with wide spin gaps and half-metallic gaps. Monte Carlo simulations based on the Heisenberg model estimate the Curie temperature (\emph{T}$_c$) of these monolayers are about 583, 608, and 597 K, respectively. Additionally, it is found that Cr$_2$TeX (X= P, As, Sb) monolayers still exhibit FM half-metallic properties under biaxial strain from -6% to 6%. At last, the Cr$_2$TeP monolayer has a higher absorption coefficient than the Cr$_2$TeAs and Cr$_2$TeSb monolayers in the visible region. The results predict that Janus Cr$_2$TeX (X= P, As, Sb) monolayers with novel properties have good potential for applications in future nanodevices. ",Kein DOI-Link verfügbar,2210.09686v4,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",FG-Depth: Flow-Guided Unsupervised Monocular Depth Estimation,1970,"  The great potential of unsupervised monocular depth estimation has been demonstrated by many works due to low annotation cost and impressive accuracy comparable to supervised methods. To further improve the performance, recent works mainly focus on designing more complex network structures and exploiting extra supervised information, e.g., semantic segmentation. These methods optimize the models by exploiting the reconstructed relationship between the target and reference images in varying degrees. However, previous methods prove that this image reconstruction optimization is prone to get trapped in local minima. In this paper, our core idea is to guide the optimization with prior knowledge from pretrained Flow-Net. And we show that the bottleneck of unsupervised monocular depth estimation can be broken with our simple but effective framework named FG-Depth. In particular, we propose (i) a flow distillation loss to replace the typical photometric loss that limits the capacity of the model and (ii) a prior flow based mask to remove invalid pixels that bring the noise in training loss. Extensive experiments demonstrate the effectiveness of each component, and our approach achieves state-of-the-art results on both KITTI and NYU-Depth-v2 datasets. ",Kein DOI-Link verfügbar,2301.08414v2,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",iTransformer: Inverted Transformers Are Effective for Time Series   Forecasting,1970,"  The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer. ",Kein DOI-Link verfügbar,2310.06625v4,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Sparse MeZO: Less Parameters for Better Performance in Zeroth-Order LLM   Fine-Tuning,1970,"  While fine-tuning large language models (LLMs) for specific tasks often yields impressive results, it comes at the cost of memory inefficiency due to back-propagation in gradient-based training. Memory-efficient Zeroth-order (MeZO) optimizers, recently proposed to address this issue, only require forward passes during training, making them more memory-friendly. However, the quality of gradient estimates in zeroth order optimization often depends on the data dimensionality, potentially explaining why MeZO still exhibits significant performance drops compared to standard fine-tuning across various tasks. Inspired by the success of Parameter-Efficient Fine-Tuning (PEFT), this paper introduces Sparse MeZO, a novel memory-efficient zeroth-order optimization approach that applies ZO only to a carefully chosen subset of parameters. We propose a simple yet effective parameter selection scheme that yields significant performance gains with Sparse-MeZO. Additionally, we develop a memory-optimized implementation for sparse masking, ensuring the algorithm requires only inference-level memory consumption, allowing Sparse-MeZO to fine-tune LLaMA-30b on a single A100 GPU. Experimental results illustrate that Sparse-MeZO consistently improves both performance and convergence speed over MeZO without any overhead. For example, it achieves a 9\% absolute accuracy improvement and 3.5x speedup over MeZO on the RTE task. ",Kein DOI-Link verfügbar,2402.15751v1,Yes,potent(1)
0000-0002-0228-8971,Yong Liu,"Aalto University, Aalto University School of Business",Analytic Expression for Exact Ground State Energy Based on an Operator   Method for a Class of Anharmonic Potentials,1970,"  A general procedure based on shift operators is formulated to deal with anharmonic potentials. It is possible to extract the ground state energy analytically using our method provided certain consistency relations are satisfied. Analytic expressions for the exact ground state energy have also been derived specifically for a large class of the one-dimensional oscillator with cubic-quartic anharmonic terms. Our analytical results can be used to check the accuracy of existing numerical methods, for instance the method of state-dependent diagonalization. Our results also agree with the asymptotic behavior in the divergent pertubative expansion of quartic harmonic oscillator. ",https://doi.org/10.1103/PhysRevA.62.052107,quant-ph/0007031v1,Yes,potent(1)
0000-0002-2341-9301,Fabio Colella,Aalto University,Human Strategic Steering Improves Performance of Interactive   Optimization,1970,"  A central concern in an interactive intelligent system is optimization of its actions, to be maximally helpful to its human user. In recommender systems for instance, the action is to choose what to recommend, and the optimization task is to recommend items the user prefers. The optimization is done based on earlier user's feedback (e.g. ""likes"" and ""dislikes""), and the algorithms assume the feedback to be faithful. That is, when the user clicks ""like,"" they actually prefer the item. We argue that this fundamental assumption can be extensively violated by human users, who are not passive feedback sources. Instead, they are in control, actively steering the system towards their goal. To verify this hypothesis, that humans steer and are able to improve performance by steering, we designed a function optimization task where a human and an optimization algorithm collaborate to find the maximum of a 1-dimensional function. At each iteration, the optimization algorithm queries the user for the value of a hidden function $f$ at a point $x$, and the user, who sees the hidden function, provides an answer about $f(x)$. Our study on 21 participants shows that users who understand how the optimization works, strategically provide biased answers (answers not equal to $f(x)$), which results in the algorithm finding the optimum significantly faster. Our work highlights that next-generation intelligent systems will need user models capable of helping users who steer systems to pursue their goals. ",https://doi.org/10.1145/3340631.3394883,2005.01291v1,Yes,strategically(1)
0000-0002-5502-5905,Akseli Mäkinen,Aalto University,Three-waveform bidirectional pumping of single electrons with a silicon   quantum dot,1970,"  Semiconductor-based quantum dot single-electron pumps are currently the most promising candidates for the direct realization of the emerging quantum standard of the ampere in the International System of Units. Here, we discuss a silicon quantum dot single-electron pump with radio frequency control over the transparencies of entrance and exit barriers as well as the dot potential. We show that our driving protocol leads to robust bidirectional pumping: one can conveniently reverse the direction of the quantized current by changing only the phase shift of one driving waveform with respect to the others. We also study the improvement in the robustness of the current quantization owing to the introduction of three control voltages in comparison with the two-waveform driving. We anticipate that this pumping technique may be used in the future to perform error counting experiments by pumping the electrons into and out of a reservoir island monitored by a charge sensor. ",Kein DOI-Link verfügbar,1603.01225v2,Yes,potent(1)
0000-0002-5502-5905,Akseli Mäkinen,Aalto University,Optimized heat transfer at exceptional points in quantum circuits,1970,"  Superconducting quantum circuits are potential candidates to realize a large-scale quantum computer. The envisioned large density of integrated components, however, requires a proper thermal management and control of dissipation. To this end, it is advantageous to utilize tunable dissipation channels and to exploit the optimized heat flow at exceptional points (EPs). Here, we experimentally realize an EP in a superconducting microwave circuit consisting of two resonators. The EP is a singularity point of the Hamiltonian, and corresponds to the most efficient heat transfer between the resonators without oscillation of energy. We observe a crossover from underdamped to overdamped coupling via the EP by utilizing photon-assisted tunneling as an \emph{in situ} tunable dissipative element in one of the resonators. The methods studied here can be applied to different circuits to obtain fast dissipation, for example, for initializing qubits to their ground states. In addition, these results pave the way towards thorough investigation of parity--time ($\mathcal{PT}$) symmetric systems and the spontaneous symmetry breaking in superconducting microwave circuits operating at the level of single energy quanta. ",https://doi.org/10.1103/PhysRevB.100.134505,1812.02683v1,Yes,potent(1)
0009-0006-0871-4982,Yuting Jiang,Aalto University,Neural Network-Assisted End-to-End Design for Dispersive Full-Parameter   Control of Meta-Optics,1970,"  Flexible control light field across multiple parameters is the cornerstone of versatile and miniaturized optical devices. Metasurfaces, comprising subwavelength scatterers, offer a potent platform for executing such precise manipulations. However, the inherent mutual constraints between parameters of metasurfaces make it challenging for traditional approaches to achieve full-parameter control across multiple wavelengths. Here, we propose a universal end-to-end inverse design framework to directly optimize the geometric parameter layout of meta-optics based on the target functionality of full-parameter control across multiple wavelengths. This framework employs a differentiable forward simulator integrating a neural network-based dispersive full-parameter Jones matrix and Fourier propagation to facilitate gradient-based optimization. Its superiority over sequential forward designs in dual-polarization channel color holography with higher quality and tri-polarization three-dimensional color holography with higher multiplexed capacity is showcased. To highlight the universality, we further present polarized spectral multi-information processing with six arbitrary polarizations and three wavelengths. This versatile, differentiable, system-level design framework is poised to expedite the advancement of meta-optics in integrated multi-information display, imaging, and communication, extending to multi-modal sensing applications. ",Kein DOI-Link verfügbar,2407.00559v1,Yes,"versatile(2), potent(1)"
0000-0002-0942-9941,Mikhail Sidorenko,Aalto University,Long-Range Over-a-Meter NFC Antenna Design and Impedance Matching,1970,"  NFC and RFID technologies have seen significant advancements, with expanding applications necessitating the design of novel antenna structures that enhance range capabilities. This paper presents a study on the design and impedance matching of long-range NFC coils, focusing on optimizing antenna performance over distances exceeding one meter. Through numerical analyses of various coil geometries, including single-turn and multi-wire configurations, we explore the effects of coil size, wire separation, and current distribution on magnetic field generation. Additionally, an adaptive impedance matching approach is proposed to maintain efficient power transfer, significantly improving field strength and system performance. The proposed designs demonstrate superior interrogation distances compared to existing configurations, highlighting the potential for enhanced long-range NFC applications. ",Kein DOI-Link verfügbar,2408.14168v1,Yes,potent(1)
0000-0003-2443-7242,Klemens Winkler,University of Vienna,Nonequilibrium entanglement between levitated masses under optimal   control,1970,"  We present a protocol that maximizes unconditional entanglement generation between two masses interacting directly through $1/r^{n}$ potential. The protocol combines optimal quantum control of continuously measured masses with their non-equilibrium dynamics, driven by a time-dependent interaction strength. Applied to a pair of optically trapped sub-micron particles coupled via electrostatic interaction, our protocol enables unconditional entanglement generation at the fundamental limit of the conditional state and with an order of magnitude smaller interaction between the masses compared to the existing steady-state approaches. ",Kein DOI-Link verfügbar,2408.06251v1,Yes,potent(1)
0000-0002-3900-1532,Peter Schiansky,"University Of Vienna, University of Vienna",Demonstration of quantum-digital payments,1970,"  Digital payments have replaced physical banknotes in many aspects of our daily lives. Similarly to banknotes, they should be easy to use, unique, tamper-resistant and untraceable, but additionally withstand digital attackers and data breaches. Current technology substitutes customers' sensitive data by randomized tokens, and secures the payment's uniqueness with a cryptographic function, called a cryptogram. However, computationally powerful attacks violate the security of these functions. Quantum technology comes with the potential to protect even against infinite computational power. Here, we show how quantum light can secure daily digital payments by generating inherently unforgeable quantum cryptograms. We implement the scheme over an urban optical fiber link, and show its robustness to noise and loss-dependent attacks. Unlike previously proposed protocols, our solution does not depend on long-term quantum storage or trusted agents and authenticated channels. It is practical with near-term technology and may herald an era of quantum-enabled security. ",https://doi.org/10.1038/s41467-023-39519-w,2305.14504v2,Yes,potent(1)
0000-0003-1371-147X,Andrea Arnold,"University of Vienna, University of Vienna  AT",Sparse model identification and prediction of microglial cells during   ischemic stroke,1970,"  Dynamics between key neuroinflammatory components, detrimental M1 and beneficial M2 microglial cells, are not fully understood post-ischemic stroke. To discover, model, and predict these dynamics, we use a method based on sparse identification of nonlinear dynamics (SINDy). The resulting data-driven dynamical system involves constant and linear terms but does not include nonlinear interactions between cells. Results show M2 microglial cell dominance of four days. Forward predictions capture potential long-term dynamics of microglial cells and suggest a persistent inflammatory response. ",Kein DOI-Link verfügbar,2404.10915v1,Yes,potent(1)
0000-0003-1371-147X,Andrea Arnold,"University of Vienna, University of Vienna  AT",Identification of Tissue Optical Properties During Thermal Laser-Tissue   Interactions: An Ensemble Kalman Filter-Based Approach,1970,"  In this paper, we propose a computational framework to estimate the physical properties that govern the thermal response of laser-irradiated tissue. We focus in particular on two quantities, the absorption and scattering coefficients, which describe the optical absorption of light in the tissue and whose knowledge is vital to correctly plan medical laser treatments. To perform the estimation, we utilize an implementation of the Ensemble Kalman Filter (EnKF), a type of Bayesian filtering algorithm for data assimilation. Unlike prior approaches, in this work we estimate the tissue optical properties based on observations of the tissue thermal response to laser irradiation. This method has the potential for straightforward implementation in a clinical setup, as it would only require a simple thermal sensor, e.g., a miniaturized infrared camera. Because the optical properties of tissue can undergo shifts during laser exposure, we employ a variant of EnKF capable of tracking time-varying parameters. Through simulated experimental studies, we demonstrate the ability of the proposed technique to identify the tissue optical properties and track their dynamic changes during laser exposure, while simultaneously tracking changes in the tissue temperature at locations beneath the surface. We further demonstrate the framework's capability in estimating additional unknown tissue properties (i.e., the volumetric heat capacity and thermal conductivity) along with the optical properties of interest. ",https://doi.org/10.1002/cnm.3574,2107.10340v2,Yes,potent(1)
0000-0003-1371-147X,Andrea Arnold,"University of Vienna, University of Vienna  AT",A data-informed mathematical model of microglial cell dynamics during   ischemic stroke in the middle cerebral artery,1970,"  Neuroinflammation immediately follows the onset of ischemic stroke in the middle cerebral artery. During this process, microglial cells are activated in and recruited to the penumbra. Microglial cells can be activated into two different phenotypes: M1, which can worsen brain injury; or M2, which can aid in long-term recovery. In this study, we contribute a summary of experimental data on microglial cell counts in the penumbra following ischemic stroke induced by middle cerebral artery occlusion (MCAO) in mice and compile available data sets into a single set suitable for time series analysis. Further, we formulate a mathematical model of microglial cells in the penumbra during ischemic stroke due to MCAO. Through use of global sensitivity analysis and Markov Chain Monte Carlo (MCMC)-based parameter estimation, we analyze the effects of the model parameters on the number of M1 and M2 cells in the penumbra and fit identifiable parameters to the compiled experimental data set. We utilize results from MCMC parameter estimation to ascertain uncertainty bounds and forward predictions for the number of M1 and M2 microglial cells over time. Results demonstrate the significance of parameters related to M1 and M2 activation on the number of M1 and M2 microglial cells. Simulations further suggest that potential outliers in the observed data may be omitted and forecast predictions suggest a lingering inflammatory response. ",Kein DOI-Link verfügbar,2403.15284v1,Yes,potent(1)
0000-0003-1094-5641,Florian Gruber,"Medical University of Vienna, University of Vienna",Comparing topological charge definitions using topology fixing actions,1970,"  We investigate both the hyperbolic action and the determinant ratio action designed to fix the topological charge on the lattice. We show to what extent topology is fixed depending on the parameters of these actions, keeping the physical situation fixed. At the same time the agreement between different definitions of topological charge - the field theoretic and the index definition - is directly correlated to the degree topology is fixed. Moreover, it turns out that the two definitions agree very well. We also study finite volume effects arising in the static potential and related quantities due to topology fixing. ",https://doi.org/10.1140/epja/i2010-10915-1,0905.2849v2,Yes,potent(1)
0000-0002-9142-8090,Jörn Manz,Freie Universitaet Berlin,Maximum tunneling velocities in symmetric double well potentials,1970,"  We consider coherent tunneling of one-dimensional model systems in non-cyclic or cyclic symmetric double well potentials. Generic potentials are constructed which allow for analytical estimates of the quantum dynamics in the non-relativistic deep tunneling regime, in terms of the tunneling distance, barrier height and mass (or moment of inertia). For cyclic systems, the results may be scaled to agree well with periodic potentials for which semi-analytical results in terms of Mathieu functions exist. Starting from a wavepacket which is initially localized in one of the potential wells, the subsequent periodic tunneling is associated with tunneling velocities. These velocities (or angular velocities) are evaluated as the ratio of the flux densities versus the probability densities. The maximum velocities are found under the top of the barrier where they scale as the square root of the ratio of barrier height and mass (or moment of inertia), independent of the tunneling distance. They are applied exemplarily to several prototypical molecular models of non-cyclic and cyclic tunneling, including ammonia inversion, Cope rearrangment of semibullvalene, torsions of molecular fragments, and rotational tunneling in strong laser fields. Typical maximum velocities and angular velocities are in the order of a few km/s and from 10 to 100 THz for our non-cyclic and cyclic systems, respectively, much faster than time-averaged velocities. Even for the more extreme case of an electron tunneling through a barrier of height of one Hartree, the velocity is only about one percent of the speed of light. Estimates of the corresponding time scales for passing through {the narrow domain just} below the potential barrier are in the domain from 2 to 40 fs, much shorter than the tunneling times. ",https://doi.org/10.1016/j.chemphys.2014.04.004,1404.2244v2,Yes,potent(5)
